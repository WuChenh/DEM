{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DEM documentation!","text":""},{"location":"#dual-extraction-modeling-dem-a-multi-modal-deep-learning-architecture-for-phenotypic-prediction-and-functional-gene-mining-of-complex-traits","title":"Dual-Extraction Modeling (DEM): A multi-modal deep-learning architecture for phenotypic prediction and functional gene mining of complex traits","text":"<p>DOI: 10.1016/j.xplc.2024.101002</p> <p>A multi-modal deep-learning architecture designed to extract representative features from diverse omics datasets. DEM enables robust and precise prediction of qualitative and quantitative traits phenotypes.</p> <p></p>"},{"location":"#key-features","title":"Key Features:","text":"<ul> <li>Multi-Modal Deep Learning: DEM efficiently integrates heterogeneous omics data for both classification and regression tasks.</li> <li>High Accuracy &amp; Generalizability: Benchmarking experiments demonstrate DEM\u2019s superior accuracy, robustness, and flexibility across various complex trait predictions.</li> <li>Explainability: DEM excels at identifying pleiotropic genes, such as those influencing flowering time and rosette leaf number, with impressive Explainability.</li> <li>User-Friendly Software: The repository includes easy-to-use tools for seamless application of DEM.</li> </ul> <p>The DEM is implemented in the Python package <code>biodem</code>, which comprises 4 modules:</p> <ul> <li>Data preprocessing</li> <li>Dual-extraction modeling</li> <li>Phenotypic prediction</li> <li>Functional gene mining</li> </ul> <p></p>"},{"location":"installation/","title":"Installation","text":"<p>Conda / Mamba is recommended for installation.</p>"},{"location":"installation/#create-a-conda-environment","title":"Create a conda environment:","text":"<pre><code>mamba create -n dem python=3.11\nmamba activate dem\n\n# Install PyTorch with CUDA support\nmamba install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia\n</code></pre>"},{"location":"installation/#install-biodem-from-pypi","title":"Install biodem from PyPI","text":"<pre><code>pip install biodem\n</code></pre> <p>We also provide a lightweight tool <code>pregv</code> for VCF &amp; GFF file processing and SNP encoding, which is implemented in Rust. Click to download or install with: Install pregv<pre><code># mamba activate dem\ndem-install-pregv\n</code></pre> You may need to deactivate and reactivate the conda environment after installation.</p> <p>Test your installation with: Test pregv installation<pre><code>pregv --help\n</code></pre></p>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#input-and-output-data-formats","title":"Input and output data formats","text":"<p>These are data formats that users can prepare for <code>biodem</code> modules.</p>"},{"location":"quick_start/#tabular-data","title":"Tabular data","text":"<p>The Modules <code>DEMDataset</code> can read tabular data in the CSV and Parquet formats.</p> <ul> <li>The first column contains sample IDs that will be read as string type.</li> <li>The first column's name must be <code>\"ID\"</code>.</li> </ul> <p>Please see the detailed requirements in the Modules &gt; Utilities &gt; Preprocessing Data &gt; <code>DEMDataset</code>.</p> <p>Example:</p> ID gene_1 gene_2 id_1 0.87 0.03 id_2 0.34 0.65"},{"location":"quick_start/#vcf","title":"VCF","text":"<p>Genotype data in Variant Call Format (VCF).</p>"},{"location":"quick_start/#gff","title":"GFF","text":"<p>Genomic annotations in Generic Feature Format Version 3 (GFF3).</p>"},{"location":"quick_start/#running-tests","title":"Running tests","text":"<p>A simple usage example is provided in <code>./tests/test_biodem.py</code>. Please refer to the script and \"Modules\" documentation for more details.</p> Run tests<pre><code>git clone https://github.com/cma2015/DEM.git\ncd DEM/tests\n# Activate your conda environment\n# mamba activate dem\npython test_biodem.py\n</code></pre>"},{"location":"reference/biodem.cli_dem/","title":"biodem.cli_dem","text":""},{"location":"reference/biodem.cli_dem/#biodem.cli_dem","title":"<code>biodem.cli_dem</code>","text":""},{"location":"reference/biodem.cli_dem/#biodem.cli_dem.cli_install_pregv","title":"<code>cli_install_pregv()</code>","text":"<p>Install pregv.</p> Source code in <code>src\\biodem\\cli_dem.py</code> <pre><code>def cli_install_pregv():\n    r\"\"\"Install pregv.\n    \"\"\"\n    # parser = argparse.ArgumentParser(description=\"Install compiled pregv tool.\")\n    # Find the path where packages installed, like \"$HOME/miniforge3/envs/env_name/lib/python3.12/site-packages\"\n    site_packages_path = site.getsitepackages()[0]\n    # Define the path where the compiled tool will be copied to\n    dst_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(site_packages_path))), \"bin\")\n    print(f\"Destination directory: {dst_dir}\")\n    if not os.path.exists(dst_dir):\n        raise SystemExit(f\"Destination directory does not exist.\")\n\n    # Get the current platform and architecture\n    _platform = platform.system()\n    _architecture = platform.machine()\n    bname = \"pregv\"\n\n    # Check the architecture\n    if _architecture != 'x86_64':\n        raise SystemExit(f\"Unsupported architecture: {_architecture}, please compile {bname} manually.\")\n\n    # Check the platform and copy the tool\n    if _platform == 'Linux':\n        dst_path = os.path.join(dst_dir, bname)\n        source_path = os.path.join(site_packages_path, \"bin\", \"linux\", bname)\n        shutil.copyfile(source_path, dst_path)\n        os.chmod(dst_path, 0o755)\n        print(f\"{bname} has been installed successfully: {dst_path}\")\n    elif _platform == 'Windows':\n        dst_path = os.path.join(dst_dir, bname + \".exe\")\n        source_path = os.path.join(site_packages_path, \"bin\", \"windows\", bname + \".exe\")\n        shutil.copyfile(source_path, dst_path)\n        os.chmod(dst_path, 0o755)\n        print(f\"{bname} has been installed successfully: {dst_path}\")\n    else:\n        raise SystemExit(f\"Unsupported platform: {_platform}, please compile {bname} manually.\")\n</code></pre>"},{"location":"reference/biodem.cli_dem/#biodem.cli_dem.cli_rm_ckpt","title":"<code>cli_rm_ckpt()</code>","text":"<p>Remove inferior models' checkpoints.</p> Source code in <code>src\\biodem\\cli_dem.py</code> <pre><code>def cli_rm_ckpt():\n    r\"\"\"Remove inferior models' checkpoints.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Remove inferior models' checkpoints.\")\n    parser.add_argument(\"-p\", \"--path\", type=str, help=\"Path to a directory containing fitting logs.\", required=True)\n    parser.add_argument(\"-r\", \"--report\", type=str, help=\"Path to a directory to save the report.\", required=False, default=None)\n    args = parser.parse_args()\n    ckpt_collector = CollectFitLog(args.path)\n    ckpt_collector.remove_inferior_models()\n    if args.report is not None:\n        ckpt_collector.get_df_csv(args.report, overwrite_collected_log=True)\n</code></pre>"},{"location":"reference/biodem.dem.model/","title":"biodem.dem.model","text":""},{"location":"reference/biodem.dem.model/#biodem.dem.model","title":"<code>biodem.dem.model</code>","text":"<p>The implementation of the DEM model.</p>"},{"location":"reference/biodem.dem.model/#biodem.dem.model.DEM","title":"<code>DEM</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src\\biodem\\dem\\model.py</code> <pre><code>class DEM(nn.Module):\n    def __init__(\n            self,\n            omics_dim: list[int],\n            n_heads: int,\n            n_encoders: int,\n            hidden_dim: int,\n            output_dim: int,\n            dropout: float,\n        ):\n        r\"\"\"The DEM model.\n        \"\"\"\n        super().__init__()\n        self.omics_dim = omics_dim\n        self.extract_conc = ExtractConcOmics(n_heads, n_encoders, sum(omics_dim), hidden_dim, output_dim, dropout)\n        self.extract_each_omics = nn.ModuleList([\n            Extract1Omics(n_heads, n_encoders, omics_dim[i], hidden_dim, output_dim, dropout)\n            for i in range(len(omics_dim))\n        ])\n\n        integrated_input_dim = const.hparam_candidates.linear_dims_conc_omics[0][0] + const.hparam_candidates.linear_dims_single_omics[0][0] * len(omics_dim)\n\n        self.integrate_extractions = IntegrateExtractions(n_heads, n_encoders, integrated_input_dim, hidden_dim, output_dim, dropout)\n\n        # Initialize learnable weights for every omics in y_pred_each_omics\n        self.weights_each_omics = nn.ParameterList([\n            nn.Parameter(torch.ones(1) / len(omics_dim))\n            for _ in range(len(omics_dim))\n        ])\n        # Initialize learnable weights for y_pred_conc\n        self.weight_conc = nn.Parameter(torch.ones(1))\n        # Initialize learnable weights for y_pred_integrated\n        self.weight_integrated = nn.Parameter(torch.ones(1))\n\n    def forward(self, x: list[torch.Tensor]):\n        # Extract concatenated omics\n        y_pred_conc, h_conc = self.extract_conc(torch.cat(x, dim=1))\n\n        # Extract each omics\n        y_pred_each_omics = []\n        h_each_omics = []\n        for i in range(len(self.omics_dim)):\n            y_pred_omics_i, h_xomics_i = self.extract_each_omics[i](x[i])\n            y_pred_each_omics.append(y_pred_omics_i)\n            h_each_omics.append(h_xomics_i)\n        # Calc weighted y_pred_each_omics\n        y_pred_each_omics = [self.weights_each_omics[i] * y_pred_each_omics[i] for i in range(len(y_pred_each_omics))]\n        # Sum weighted y_pred_each_omics\n        y_pred_each_omics = torch.sum(torch.stack(y_pred_each_omics), dim=0)\n\n        h_conc_each_omics = torch.cat(h_each_omics, dim=1)\n\n        h_integrated = torch.cat([h_conc, h_conc_each_omics], dim=1)\n\n        y_pred_integrated = self.integrate_extractions(h_integrated)\n\n        y_pred = self.weight_conc * y_pred_conc + self.weight_integrated * y_pred_integrated + y_pred_each_omics\n\n        return y_pred\n</code></pre>"},{"location":"reference/biodem.dem.model/#biodem.dem.model.DEM.__init__","title":"<code>__init__(omics_dim, n_heads, n_encoders, hidden_dim, output_dim, dropout)</code>","text":"<p>The DEM model.</p> Source code in <code>src\\biodem\\dem\\model.py</code> <pre><code>def __init__(\n        self,\n        omics_dim: list[int],\n        n_heads: int,\n        n_encoders: int,\n        hidden_dim: int,\n        output_dim: int,\n        dropout: float,\n    ):\n    r\"\"\"The DEM model.\n    \"\"\"\n    super().__init__()\n    self.omics_dim = omics_dim\n    self.extract_conc = ExtractConcOmics(n_heads, n_encoders, sum(omics_dim), hidden_dim, output_dim, dropout)\n    self.extract_each_omics = nn.ModuleList([\n        Extract1Omics(n_heads, n_encoders, omics_dim[i], hidden_dim, output_dim, dropout)\n        for i in range(len(omics_dim))\n    ])\n\n    integrated_input_dim = const.hparam_candidates.linear_dims_conc_omics[0][0] + const.hparam_candidates.linear_dims_single_omics[0][0] * len(omics_dim)\n\n    self.integrate_extractions = IntegrateExtractions(n_heads, n_encoders, integrated_input_dim, hidden_dim, output_dim, dropout)\n\n    # Initialize learnable weights for every omics in y_pred_each_omics\n    self.weights_each_omics = nn.ParameterList([\n        nn.Parameter(torch.ones(1) / len(omics_dim))\n        for _ in range(len(omics_dim))\n    ])\n    # Initialize learnable weights for y_pred_conc\n    self.weight_conc = nn.Parameter(torch.ones(1))\n    # Initialize learnable weights for y_pred_integrated\n    self.weight_integrated = nn.Parameter(torch.ones(1))\n</code></pre>"},{"location":"reference/biodem.dem.model/#biodem.dem.model.DEMLTN","title":"<code>DEMLTN</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>src\\biodem\\dem\\model.py</code> <pre><code>class DEMLTN(ltn.LightningModule):\n    def __init__(\n            self,\n            omics_dim: list[int],\n            n_heads: int,\n            n_encoders: int,\n            hidden_dim: int,\n            output_dim: int,\n            dropout: float,\n            learning_rate: float,\n            is_regression: bool,\n        ):\n        r\"\"\"DEM model in lightning.\n\n        Args:\n            omics_dim: list of input dimensions for each omics data.\n            n_heads: number of heads in the multi-head attention.\n            n_encoders: number of encoders.\n            hidden_dim: dimension of the feedforward network.\n            output_dim: number of output classes. If it is 1, the model will be a regression model. Otherwise, it should be at least 3 (3 for binary classification) for classification tasks.\n            dropout: dropout rate.\n            learning_rate: learning rate for the optimizer.\n            is_regression: whether the task is a regression task or not.\n\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.output_dim = output_dim\n        self.learning_rate = learning_rate\n        self.is_regression = is_regression\n\n        self._define_metrics(output_dim, is_regression)\n\n        self.DEM_model = DEM(\n            omics_dim=omics_dim,\n            n_heads=n_heads,\n            n_encoders=n_encoders,\n            hidden_dim=hidden_dim,\n            output_dim=output_dim,\n            dropout=dropout,\n        )\n\n    def forward(self, x_omics: list[torch.Tensor]):\n        return self.DEM_model(x_omics)\n\n    def training_step(self, batch, batch_idx):\n        x = batch[const.dkey.litdata_omics]\n        y = batch[const.dkey.litdata_label]\n        y_pred = self.forward(x)\n        loss = self._loss(const.title_train, y, y_pred)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x = batch[const.dkey.litdata_omics]\n        y = batch[const.dkey.litdata_label]\n        y_pred = self.forward(x)\n        loss = self._loss(const.title_val, y, y_pred)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x = batch[const.dkey.litdata_omics]\n        y = batch[const.dkey.litdata_label]\n        y_pred = self.forward(x)\n        loss = self._loss(const.title_test, y, y_pred)\n        return loss\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        x = batch[const.dkey.litdata_omics]\n        y_pred = self.forward(x)\n\n        y = batch[const.dkey.litdata_label]\n        loss = self._loss(const.title_predict, y, y_pred)\n        return y_pred, loss\n\n    def configure_optimizers(self):\n        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5, 2)\n        return {'optimizer': optimizer,\n                'lr_scheduler': {\n                    'scheduler': scheduler,\n                    'interval': 'step',\n                    'frequency': 1,\n                    'monitor': const.title_val_loss,\n                    }\n                }\n\n    def _define_metrics(self, output_dim: int, regression: bool):\n        r\"\"\"Define the loss function and the metrics.\n        \"\"\"\n        if output_dim == 1:\n            self.loss_fn = nn.MSELoss()\n            self.mae = MeanAbsoluteError()\n            self.r2 = R2Score()\n            self.pcc = PearsonCorrCoef()\n        else:\n            if regression:\n                # Multi-label regression\n                self.loss_fn = nn.MSELoss(reduction='none')\n                # self.mae = MeanAbsoluteError()\n                # self.r2 = R2Score()\n                # self.pcc = PearsonCorrCoef()\n            else:\n                self.loss_fn = nn.CrossEntropyLoss()\n                self.mcc = MatthewsCorrCoef(task='multiclass', num_classes=output_dim)\n                self.recall_micro = MulticlassRecall(average=\"micro\", num_classes=output_dim)\n                self.recall_macro = MulticlassRecall(average=\"macro\", num_classes=output_dim)\n                self.recall_weighted = MulticlassRecall(average=\"weighted\", num_classes=output_dim)\n                #\n                self.precision_micro = MulticlassPrecision(average=\"micro\", num_classes=output_dim)\n                self.precision_macro = MulticlassPrecision(average=\"macro\", num_classes=output_dim)\n                self.precision_weighted = MulticlassPrecision(average=\"weighted\", num_classes=output_dim)\n                #\n                self.f1_micro = MulticlassF1Score(average=\"micro\", num_classes=output_dim)\n                self.f1_macro = MulticlassF1Score(average=\"macro\", num_classes=output_dim)\n                self.f1_weighted = MulticlassF1Score(average=\"weighted\", num_classes=output_dim)\n                #\n                self.accuracy_micro = MulticlassAccuracy(average=\"micro\", num_classes=output_dim)\n                self.accuracy_macro = MulticlassAccuracy(average=\"macro\", num_classes=output_dim)\n                self.accuracy_weighted = MulticlassAccuracy(average=\"weighted\", num_classes=output_dim)\n                #\n                self.auroc_macro = MulticlassAUROC(average=\"macro\", num_classes=output_dim)\n                self.auroc_weighted = MulticlassAUROC(average=\"weighted\", num_classes=output_dim)\n\n    def _loss(self, which_step: str, y: torch.Tensor, y_pred: torch.Tensor):\n        #!!!!!!!!!!!!!!!!!!!!!!!!!\n        if self.output_dim &gt; 1 and not self.is_regression:\n            y = y.argmax(dim=-1)\n\n        if self.output_dim == 1:\n            loss = self.loss_fn(y_pred, y)\n            if which_step == const.title_predict:\n                return loss\n            self.log(f\"{which_step}_loss\", loss, sync_dist=True)\n            self.log(f\"{which_step}_mae\", self.mae(y_pred, y), sync_dist=True)\n            if y.shape[0] &lt; 2:\n                return loss\n            self.log(f\"{which_step}_pcc\", self.pcc(y_pred, y), sync_dist=True)\n            self.log(f\"{which_step}_r2\", self.r2(y_pred, y), sync_dist=True)\n        else:\n            if self.is_regression:\n                loss = self.loss_fn(y_pred, y).mean(dim=0).sum()\n                if which_step == const.title_predict:\n                    return loss\n                self.log(f\"{which_step}_loss\", loss, sync_dist=True)\n                # self.log(f\"{which_step}_mae\", self.mae(y_pred, y), sync_dist=True)\n                # if y.shape[0] &lt; 2:\n                #     return loss\n                # self.log(f\"{which_step}_pcc\", self.pcc(y_pred, y), sync_dist=True)\n                # self.log(f\"{which_step}_r2\", self.r2(y_pred, y), sync_dist=True)\n            else:\n                loss = self.loss_fn(y_pred, y)\n                if which_step == const.title_predict:\n                    return loss\n                self.log(f\"{which_step}_loss\", loss, sync_dist=True)\n\n                self.log(f\"{which_step}_mcc\", self.mcc(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_f1_micro\", self.f1_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_f1_macro\", self.f1_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_f1_weighted\", self.f1_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_recall_micro\", self.recall_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_recall_macro\", self.recall_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_recall_weighted\", self.recall_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_precision_micro\", self.precision_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_precision_macro\", self.precision_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_precision_weighted\", self.precision_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_accuracy_micro\", self.accuracy_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_accuracy_macro\", self.accuracy_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_accuracy_weighted\", self.accuracy_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_auroc_macro\", self.auroc_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_auroc_weighted\", self.auroc_weighted(y_pred, y), sync_dist=True)\n\n        return loss\n</code></pre>"},{"location":"reference/biodem.dem.model/#biodem.dem.model.DEMLTN.__init__","title":"<code>__init__(omics_dim, n_heads, n_encoders, hidden_dim, output_dim, dropout, learning_rate, is_regression)</code>","text":"<p>DEM model in lightning.</p> <p>Parameters:</p> Name Type Description Default <code>omics_dim</code> <code>list[int]</code> <p>list of input dimensions for each omics data.</p> required <code>n_heads</code> <code>int</code> <p>number of heads in the multi-head attention.</p> required <code>n_encoders</code> <code>int</code> <p>number of encoders.</p> required <code>hidden_dim</code> <code>int</code> <p>dimension of the feedforward network.</p> required <code>output_dim</code> <code>int</code> <p>number of output classes. If it is 1, the model will be a regression model. Otherwise, it should be at least 3 (3 for binary classification) for classification tasks.</p> required <code>dropout</code> <code>float</code> <p>dropout rate.</p> required <code>learning_rate</code> <code>float</code> <p>learning rate for the optimizer.</p> required <code>is_regression</code> <code>bool</code> <p>whether the task is a regression task or not.</p> required Source code in <code>src\\biodem\\dem\\model.py</code> <pre><code>def __init__(\n        self,\n        omics_dim: list[int],\n        n_heads: int,\n        n_encoders: int,\n        hidden_dim: int,\n        output_dim: int,\n        dropout: float,\n        learning_rate: float,\n        is_regression: bool,\n    ):\n    r\"\"\"DEM model in lightning.\n\n    Args:\n        omics_dim: list of input dimensions for each omics data.\n        n_heads: number of heads in the multi-head attention.\n        n_encoders: number of encoders.\n        hidden_dim: dimension of the feedforward network.\n        output_dim: number of output classes. If it is 1, the model will be a regression model. Otherwise, it should be at least 3 (3 for binary classification) for classification tasks.\n        dropout: dropout rate.\n        learning_rate: learning rate for the optimizer.\n        is_regression: whether the task is a regression task or not.\n\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n\n    self.output_dim = output_dim\n    self.learning_rate = learning_rate\n    self.is_regression = is_regression\n\n    self._define_metrics(output_dim, is_regression)\n\n    self.DEM_model = DEM(\n        omics_dim=omics_dim,\n        n_heads=n_heads,\n        n_encoders=n_encoders,\n        hidden_dim=hidden_dim,\n        output_dim=output_dim,\n        dropout=dropout,\n    )\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/","title":"biodem.dem.pipeline","text":""},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline","title":"<code>biodem.dem.pipeline</code>","text":"<p>DEM model training and hyperparameter optimization.</p>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFit","title":"<code>DEMFit</code>","text":"Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>class DEMFit:\n    def __init__(\n            self,\n            log_dir: str,\n            log_name: str,\n            litdata_dir: str,\n            which_outer_testset: int,\n            which_inner_valset: int,\n            regression: bool,\n            devices: Union[List[int], str, int] = const.default.devices,\n            accelerator: str = const.default.accelerator,\n            n_jobs: int = const.default.n_jobs,\n            n_heads: int = const.default.n_heads,\n            n_encoders: int = const.default.n_encoders,\n            hidden_dim: int = const.default.hidden_dim,\n            learning_rate: float = const.default.lr,\n            dropout: float = const.default.dropout,\n            patience: int = const.default.patience,\n            max_epochs: int = const.default.max_epochs,\n            min_epochs: int = const.default.min_epochs,\n            batch_size: int = const.default.batch_size,\n            in_dev: bool = False,\n        ):\n        r\"\"\"DEM model training with hyperparameter optimization\n\n        Args:\n            log_dir: Directory for saving the training logs and models' checkpoints.\n\n            log_name: Name of the training log.\n\n            litdata_dir: Directory for loading the nested cross-validation data.\n\n            which_outer_testset: Index of the outer test set.\n\n            which_inner_valset: Index of the inner validation set.\n\n            regression: Whether the task is regression or classification.\n\n            devices: Devices to use.\n                Default: \"auto\".\n\n            accelerator: Accelerator to use.\n                Default: \"auto\".\n\n            n_jobs: Number of jobs to use for parallel hyperparameter optimization.\n                Default: 1.\n\n            n_heads: Number of heads in the attention mechanism.\n\n            n_encoders: Number of Transformer Encoders.\n\n            hidden_dim: Hidden dimension in the Transformer Encoder.\n\n            learning_rate: Learning rate.\n\n            dropout: Dropout rate.\n\n            patience: Patience for early stopping.\n\n            max_epochs: Maximum number of epochs.\n\n            min_epochs: Minimum number of epochs.\n\n            batch_size: Batch size.\n\n            in_dev: Whether to run in development mode.\n\n        \"\"\"\n        self.in_dev = in_dev\n        self.log_dir = log_dir\n        self.log_name = log_name\n        self.devices = devices\n        self.accelerator = accelerator\n        self.n_jobs = n_jobs\n        self.model_out_dim = pl.read_csv(os.path.join(litdata_dir, const.fname.output_dim), has_header=True)[0,0]\n        self.is_regression = regression\n        self.datamodule = DEMDataModule4Train(litdata_dir, which_outer_testset, which_inner_valset, batch_size, n_jobs)\n        self.datamodule.setup()\n        self.omics_dims = self.datamodule.read_omics_dims()\n\n        self.hparams = self.hparams_fit(\n            n_heads=n_heads,\n            n_encoders=n_encoders,\n            hidden_dim=hidden_dim,\n            learning_rate=learning_rate,\n            dropout=dropout,\n            patience=patience,\n            max_epochs=max_epochs,\n            min_epochs=min_epochs,\n            batch_size=batch_size,\n        )\n\n    def hparams_fit(\n            self,\n            n_heads: int,\n            n_encoders: int,\n            hidden_dim: int,\n            learning_rate: float,\n            dropout: float,\n            patience: int,\n            max_epochs: int,\n            min_epochs: int,\n            batch_size: int,\n        ):\n        r\"\"\"Generate a dictionary of hyperparameters for DEM model training.\n        \"\"\"\n        hparams = {\n            const.dkey.num_heads: n_heads,\n            const.dkey.num_encoders: n_encoders,\n            const.dkey.hidden_dim: hidden_dim,\n            const.dkey.lr: learning_rate,\n            const.dkey.dropout: dropout,\n            const.dkey.patience: patience,\n            const.dkey.max_epochs: max_epochs,\n            const.dkey.min_epochs: min_epochs,\n            const.dkey.bsize: batch_size,\n        }\n        return hparams\n\n    def dem_fit(\n            self,\n            hparams:Dict[str, Any],\n            devices: Union[List[int], str, int],\n            accelerator: str,\n        ):\n        r\"\"\"Train DEM model with given hyperparameters and input/output paths.\n        \"\"\"\n        _model = DEMLTN(\n            omics_dim=self.omics_dims,\n            n_heads=hparams[const.dkey.num_heads],\n            n_encoders=hparams[const.dkey.num_encoders],\n            hidden_dim=hparams[const.dkey.hidden_dim],\n            output_dim=self.model_out_dim,\n            dropout=hparams[const.dkey.dropout],\n            learning_rate=hparams[const.dkey.lr],\n            is_regression=self.is_regression,\n        )\n\n        log_dir_uniq_model = os.path.join(self.log_dir, self.log_name, random_string())\n\n        val_loss_min = train_model(\n            model=_model,\n            datamodule=self.datamodule,\n            es_patience=hparams[const.dkey.patience],\n            max_epochs=hparams[const.dkey.max_epochs],\n            min_epochs=hparams[const.dkey.min_epochs],\n            log_dir=log_dir_uniq_model,\n            devices=devices,\n            accelerator=accelerator,\n            in_dev=self.in_dev,\n        )\n        if val_loss_min is None:\n            raise ValueError(\"Training failed.\")\n        return val_loss_min\n\n    def manual_train(self):\n        r\"\"\"Train DEM model with manually specified hyperparameters.\n        \"\"\"\n        val_loss_min = self.dem_fit(\n            hparams = self.hparams,\n            devices=self.devices,\n            accelerator=self.accelerator,\n        )\n        return val_loss_min\n\n    def objective(self, trial: optuna.Trial) -&gt; float:\n        r\"\"\"Objective function for DEM model training with Optuna.\n        \"\"\"\n        print(\"Trial number:\", trial.number)\n        if self.n_jobs &gt; 1:\n            time_delay = (trial.number + self.n_jobs) % self.n_jobs * const.default.time_delay\n            time.sleep(time_delay)\n\n        # Generate hyperparameters\n        batch_size = trial.suggest_categorical(const.dkey.bsize, const.hparam_candidates.batch_size)\n        n_heads = trial.suggest_categorical(const.dkey.num_heads, const.hparam_candidates.n_heads)\n        n_encoders = trial.suggest_categorical(const.dkey.num_encoders, const.hparam_candidates.n_encoders)\n        hidden_dim = trial.suggest_categorical(const.dkey.hidden_dim, const.hparam_candidates.hidden_dim)\n        dropout = trial.suggest_float(const.dkey.dropout, 0.0, const.hparam_candidates.dropout_high, step=const.hparam_candidates.dropout_step)\n        lr = trial.suggest_categorical(const.dkey.lr, const.hparam_candidates.lr)\n\n        # Update hyperparameters in DEMTrain object based on manual parameters in initialization\n        hparams_tmp = self.hparams.copy()\n        hparams_tmp[const.dkey.bsize] = batch_size\n        hparams_tmp[const.dkey.num_heads] = n_heads\n        hparams_tmp[const.dkey.num_encoders] = n_encoders\n        hparams_tmp[const.dkey.hidden_dim] = hidden_dim\n        hparams_tmp[const.dkey.lr] = lr\n        hparams_tmp[const.dkey.dropout] = dropout\n\n        val_loss_min = self.dem_fit(\n            hparams = hparams_tmp,\n            devices=self.devices,\n            accelerator=self.accelerator,\n        )\n\n        return val_loss_min\n\n    def optimize(\n            self,\n            n_trials: Optional[int] = None,\n            storage: str = const.default.optuna_db,\n            gc_after_trial: bool = True,\n        ):\n        r\"\"\"Optimize hyperparameters of DEM model with Optuna.\n        \"\"\"\n        study = optuna.create_study(\n            storage = storage,\n            study_name = self.log_name + \"_\" + time_string(),\n            direction = \"minimize\",\n            load_if_exists = True,\n        )\n        study.optimize(self.objective, n_trials=n_trials, n_jobs=self.n_jobs, gc_after_trial=gc_after_trial)\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFit.__init__","title":"<code>__init__(log_dir, log_name, litdata_dir, which_outer_testset, which_inner_valset, regression, devices=const.default.devices, accelerator=const.default.accelerator, n_jobs=const.default.n_jobs, n_heads=const.default.n_heads, n_encoders=const.default.n_encoders, hidden_dim=const.default.hidden_dim, learning_rate=const.default.lr, dropout=const.default.dropout, patience=const.default.patience, max_epochs=const.default.max_epochs, min_epochs=const.default.min_epochs, batch_size=const.default.batch_size, in_dev=False)</code>","text":"<p>DEM model training with hyperparameter optimization</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str</code> <p>Directory for saving the training logs and models' checkpoints.</p> required <code>log_name</code> <code>str</code> <p>Name of the training log.</p> required <code>litdata_dir</code> <code>str</code> <p>Directory for loading the nested cross-validation data.</p> required <code>which_outer_testset</code> <code>int</code> <p>Index of the outer test set.</p> required <code>which_inner_valset</code> <code>int</code> <p>Index of the inner validation set.</p> required <code>regression</code> <code>bool</code> <p>Whether the task is regression or classification.</p> required <code>devices</code> <code>Union[List[int], str, int]</code> <p>Devices to use. Default: \"auto\".</p> <code>devices</code> <code>accelerator</code> <code>str</code> <p>Accelerator to use. Default: \"auto\".</p> <code>accelerator</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to use for parallel hyperparameter optimization. Default: 1.</p> <code>n_jobs</code> <code>n_heads</code> <code>int</code> <p>Number of heads in the attention mechanism.</p> <code>n_heads</code> <code>n_encoders</code> <code>int</code> <p>Number of Transformer Encoders.</p> <code>n_encoders</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension in the Transformer Encoder.</p> <code>hidden_dim</code> <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> <code>lr</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>dropout</code> <code>patience</code> <code>int</code> <p>Patience for early stopping.</p> <code>patience</code> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs.</p> <code>max_epochs</code> <code>min_epochs</code> <code>int</code> <p>Minimum number of epochs.</p> <code>min_epochs</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>batch_size</code> <code>in_dev</code> <code>bool</code> <p>Whether to run in development mode.</p> <code>False</code> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def __init__(\n        self,\n        log_dir: str,\n        log_name: str,\n        litdata_dir: str,\n        which_outer_testset: int,\n        which_inner_valset: int,\n        regression: bool,\n        devices: Union[List[int], str, int] = const.default.devices,\n        accelerator: str = const.default.accelerator,\n        n_jobs: int = const.default.n_jobs,\n        n_heads: int = const.default.n_heads,\n        n_encoders: int = const.default.n_encoders,\n        hidden_dim: int = const.default.hidden_dim,\n        learning_rate: float = const.default.lr,\n        dropout: float = const.default.dropout,\n        patience: int = const.default.patience,\n        max_epochs: int = const.default.max_epochs,\n        min_epochs: int = const.default.min_epochs,\n        batch_size: int = const.default.batch_size,\n        in_dev: bool = False,\n    ):\n    r\"\"\"DEM model training with hyperparameter optimization\n\n    Args:\n        log_dir: Directory for saving the training logs and models' checkpoints.\n\n        log_name: Name of the training log.\n\n        litdata_dir: Directory for loading the nested cross-validation data.\n\n        which_outer_testset: Index of the outer test set.\n\n        which_inner_valset: Index of the inner validation set.\n\n        regression: Whether the task is regression or classification.\n\n        devices: Devices to use.\n            Default: \"auto\".\n\n        accelerator: Accelerator to use.\n            Default: \"auto\".\n\n        n_jobs: Number of jobs to use for parallel hyperparameter optimization.\n            Default: 1.\n\n        n_heads: Number of heads in the attention mechanism.\n\n        n_encoders: Number of Transformer Encoders.\n\n        hidden_dim: Hidden dimension in the Transformer Encoder.\n\n        learning_rate: Learning rate.\n\n        dropout: Dropout rate.\n\n        patience: Patience for early stopping.\n\n        max_epochs: Maximum number of epochs.\n\n        min_epochs: Minimum number of epochs.\n\n        batch_size: Batch size.\n\n        in_dev: Whether to run in development mode.\n\n    \"\"\"\n    self.in_dev = in_dev\n    self.log_dir = log_dir\n    self.log_name = log_name\n    self.devices = devices\n    self.accelerator = accelerator\n    self.n_jobs = n_jobs\n    self.model_out_dim = pl.read_csv(os.path.join(litdata_dir, const.fname.output_dim), has_header=True)[0,0]\n    self.is_regression = regression\n    self.datamodule = DEMDataModule4Train(litdata_dir, which_outer_testset, which_inner_valset, batch_size, n_jobs)\n    self.datamodule.setup()\n    self.omics_dims = self.datamodule.read_omics_dims()\n\n    self.hparams = self.hparams_fit(\n        n_heads=n_heads,\n        n_encoders=n_encoders,\n        hidden_dim=hidden_dim,\n        learning_rate=learning_rate,\n        dropout=dropout,\n        patience=patience,\n        max_epochs=max_epochs,\n        min_epochs=min_epochs,\n        batch_size=batch_size,\n    )\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFit.dem_fit","title":"<code>dem_fit(hparams, devices, accelerator)</code>","text":"<p>Train DEM model with given hyperparameters and input/output paths.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def dem_fit(\n        self,\n        hparams:Dict[str, Any],\n        devices: Union[List[int], str, int],\n        accelerator: str,\n    ):\n    r\"\"\"Train DEM model with given hyperparameters and input/output paths.\n    \"\"\"\n    _model = DEMLTN(\n        omics_dim=self.omics_dims,\n        n_heads=hparams[const.dkey.num_heads],\n        n_encoders=hparams[const.dkey.num_encoders],\n        hidden_dim=hparams[const.dkey.hidden_dim],\n        output_dim=self.model_out_dim,\n        dropout=hparams[const.dkey.dropout],\n        learning_rate=hparams[const.dkey.lr],\n        is_regression=self.is_regression,\n    )\n\n    log_dir_uniq_model = os.path.join(self.log_dir, self.log_name, random_string())\n\n    val_loss_min = train_model(\n        model=_model,\n        datamodule=self.datamodule,\n        es_patience=hparams[const.dkey.patience],\n        max_epochs=hparams[const.dkey.max_epochs],\n        min_epochs=hparams[const.dkey.min_epochs],\n        log_dir=log_dir_uniq_model,\n        devices=devices,\n        accelerator=accelerator,\n        in_dev=self.in_dev,\n    )\n    if val_loss_min is None:\n        raise ValueError(\"Training failed.\")\n    return val_loss_min\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFit.hparams_fit","title":"<code>hparams_fit(n_heads, n_encoders, hidden_dim, learning_rate, dropout, patience, max_epochs, min_epochs, batch_size)</code>","text":"<p>Generate a dictionary of hyperparameters for DEM model training.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def hparams_fit(\n        self,\n        n_heads: int,\n        n_encoders: int,\n        hidden_dim: int,\n        learning_rate: float,\n        dropout: float,\n        patience: int,\n        max_epochs: int,\n        min_epochs: int,\n        batch_size: int,\n    ):\n    r\"\"\"Generate a dictionary of hyperparameters for DEM model training.\n    \"\"\"\n    hparams = {\n        const.dkey.num_heads: n_heads,\n        const.dkey.num_encoders: n_encoders,\n        const.dkey.hidden_dim: hidden_dim,\n        const.dkey.lr: learning_rate,\n        const.dkey.dropout: dropout,\n        const.dkey.patience: patience,\n        const.dkey.max_epochs: max_epochs,\n        const.dkey.min_epochs: min_epochs,\n        const.dkey.bsize: batch_size,\n    }\n    return hparams\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFit.manual_train","title":"<code>manual_train()</code>","text":"<p>Train DEM model with manually specified hyperparameters.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def manual_train(self):\n    r\"\"\"Train DEM model with manually specified hyperparameters.\n    \"\"\"\n    val_loss_min = self.dem_fit(\n        hparams = self.hparams,\n        devices=self.devices,\n        accelerator=self.accelerator,\n    )\n    return val_loss_min\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFit.objective","title":"<code>objective(trial)</code>","text":"<p>Objective function for DEM model training with Optuna.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def objective(self, trial: optuna.Trial) -&gt; float:\n    r\"\"\"Objective function for DEM model training with Optuna.\n    \"\"\"\n    print(\"Trial number:\", trial.number)\n    if self.n_jobs &gt; 1:\n        time_delay = (trial.number + self.n_jobs) % self.n_jobs * const.default.time_delay\n        time.sleep(time_delay)\n\n    # Generate hyperparameters\n    batch_size = trial.suggest_categorical(const.dkey.bsize, const.hparam_candidates.batch_size)\n    n_heads = trial.suggest_categorical(const.dkey.num_heads, const.hparam_candidates.n_heads)\n    n_encoders = trial.suggest_categorical(const.dkey.num_encoders, const.hparam_candidates.n_encoders)\n    hidden_dim = trial.suggest_categorical(const.dkey.hidden_dim, const.hparam_candidates.hidden_dim)\n    dropout = trial.suggest_float(const.dkey.dropout, 0.0, const.hparam_candidates.dropout_high, step=const.hparam_candidates.dropout_step)\n    lr = trial.suggest_categorical(const.dkey.lr, const.hparam_candidates.lr)\n\n    # Update hyperparameters in DEMTrain object based on manual parameters in initialization\n    hparams_tmp = self.hparams.copy()\n    hparams_tmp[const.dkey.bsize] = batch_size\n    hparams_tmp[const.dkey.num_heads] = n_heads\n    hparams_tmp[const.dkey.num_encoders] = n_encoders\n    hparams_tmp[const.dkey.hidden_dim] = hidden_dim\n    hparams_tmp[const.dkey.lr] = lr\n    hparams_tmp[const.dkey.dropout] = dropout\n\n    val_loss_min = self.dem_fit(\n        hparams = hparams_tmp,\n        devices=self.devices,\n        accelerator=self.accelerator,\n    )\n\n    return val_loss_min\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFit.optimize","title":"<code>optimize(n_trials=None, storage=const.default.optuna_db, gc_after_trial=True)</code>","text":"<p>Optimize hyperparameters of DEM model with Optuna.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def optimize(\n        self,\n        n_trials: Optional[int] = None,\n        storage: str = const.default.optuna_db,\n        gc_after_trial: bool = True,\n    ):\n    r\"\"\"Optimize hyperparameters of DEM model with Optuna.\n    \"\"\"\n    study = optuna.create_study(\n        storage = storage,\n        study_name = self.log_name + \"_\" + time_string(),\n        direction = \"minimize\",\n        load_if_exists = True,\n    )\n    study.optimize(self.objective, n_trials=n_trials, n_jobs=self.n_jobs, gc_after_trial=gc_after_trial)\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFitPipe","title":"<code>DEMFitPipe</code>","text":"Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>class DEMFitPipe:\n    def __init__(\n            self,\n            litdata_dir: str,\n            list_ncv: List[List[int]],\n            log_dir: str,\n            regression: bool,\n            devices: Union[List[int], str, int] = const.default.devices,\n            accelerator: str = const.default.accelerator,\n            n_jobs: int = const.default.n_jobs,\n            n_trials: Optional[int] = const.default.n_trials,\n            in_dev: bool = False,\n        ) -&gt; None:\n        r\"\"\"DEM model training pipeline with hyperparameter trials.\n\n        Args:\n            litdata_dir: Path to the directory containing the nested cross-validation litdata.\n\n            list_ncv: List of lists containing the indices of the outer and inner folds for each data slice.\n\n            log_dir: Path to the directory where the training logs and checkpoints will be saved.\n\n            regression: Whether the task is regression or classification.\n\n            devices: Device(s) to use.\n                Default: ``\"auto\"``.\n\n            accelerator: Accelerator to use.\n                Default: ``\"auto\"``.\n\n            n_jobs: Number of jobs to use for parallelization.\n                Default: ``1``.\n\n            n_trials: Number of trials to run for hyperparameter optimization.\n                Default: ``10``.\n\n            in_dev: Whether to run in development mode.\n                Default: ``False``.\n\n        Usage:\n\n            &gt;&gt;&gt; from biodem.dem.pipeline import DEMFitPipe\n            &gt;&gt;&gt; _pipe = DEMFitPipe(...)\n            &gt;&gt;&gt; _pipe.train_pipeline()\n\n        \"\"\"\n        self.in_dev = in_dev\n        # Unique tag for the training log directory\n        tag_str = time_string() + '_' + random_string()\n        self.uniq_logdir = os.path.join(log_dir, const.title_train + \"_\" + tag_str)\n        os.makedirs(self.uniq_logdir, exist_ok=False)\n\n        self.litdata_dir = litdata_dir\n        self.list_ncv = list_ncv\n        self.n_slice = len(list_ncv)\n\n        self.regression = regression\n        self.devices = devices\n        self.accelerator = accelerator\n        self.n_jobs = n_jobs\n        self.n_trials = n_trials\n\n    def train_pipeline(self):\n        r\"\"\"Train DEM model for each fold in nested cross-validation.\n        \"\"\"\n        # Storage for optuna trials in self.log_dir\n        path_storage = 'sqlite:///' + self.uniq_logdir + '/optuna.db'\n\n        print(f\"\\nNumber of data slices to train: {self.n_slice}\\n\")\n        log_names = []\n        for i in range(self.n_slice):\n            log_names.append(f'run_ncv_{self.list_ncv[i][0]}_{self.list_ncv[i][1]}')\n\n        # Train DEM model for each fold in nested cross-validation\n        if self.n_slice == 1:\n            dem_fit_ = DEMFit(\n                log_dir=self.uniq_logdir,\n                log_name=log_names[0],\n                litdata_dir=self.litdata_dir,\n                which_outer_testset=self.list_ncv[0][0],\n                which_inner_valset=self.list_ncv[0][1],\n                regression=self.regression,\n                devices=self.devices,\n                accelerator=self.accelerator,\n                n_jobs=self.n_jobs,\n                in_dev=self.in_dev,\n            )\n            dem_fit_.optimize(n_trials=self.n_trials, storage=path_storage)\n        else:\n            for xfold in range(self.n_slice):\n                dem_fit_ = DEMFit(\n                    log_dir=self.uniq_logdir,\n                    log_name=log_names[xfold],\n                    litdata_dir=self.litdata_dir,\n                    which_outer_testset=self.list_ncv[xfold][0],\n                    which_inner_valset=self.list_ncv[xfold][1],\n                    regression=self.regression,\n                    devices=self.devices,\n                    accelerator=self.accelerator,\n                    n_jobs=self.n_jobs,\n                    in_dev=self.in_dev,\n                )\n                dem_fit_.optimize(n_trials=self.n_trials, storage=path_storage)\n\n        # Remove checkpoints of inferior models\n        _collector = CollectFitLog(self.uniq_logdir)\n        _collector.remove_inferior_models()\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFitPipe.__init__","title":"<code>__init__(litdata_dir, list_ncv, log_dir, regression, devices=const.default.devices, accelerator=const.default.accelerator, n_jobs=const.default.n_jobs, n_trials=const.default.n_trials, in_dev=False)</code>","text":"<p>DEM model training pipeline with hyperparameter trials.</p> <p>Parameters:</p> Name Type Description Default <code>litdata_dir</code> <code>str</code> <p>Path to the directory containing the nested cross-validation litdata.</p> required <code>list_ncv</code> <code>List[List[int]]</code> <p>List of lists containing the indices of the outer and inner folds for each data slice.</p> required <code>log_dir</code> <code>str</code> <p>Path to the directory where the training logs and checkpoints will be saved.</p> required <code>regression</code> <code>bool</code> <p>Whether the task is regression or classification.</p> required <code>devices</code> <code>Union[List[int], str, int]</code> <p>Device(s) to use. Default: <code>\"auto\"</code>.</p> <code>devices</code> <code>accelerator</code> <code>str</code> <p>Accelerator to use. Default: <code>\"auto\"</code>.</p> <code>accelerator</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to use for parallelization. Default: <code>1</code>.</p> <code>n_jobs</code> <code>n_trials</code> <code>Optional[int]</code> <p>Number of trials to run for hyperparameter optimization. Default: <code>10</code>.</p> <code>n_trials</code> <code>in_dev</code> <code>bool</code> <p>Whether to run in development mode. Default: <code>False</code>.</p> <code>False</code> <p>Usage:</p> <pre><code>&gt;&gt;&gt; from biodem.dem.pipeline import DEMFitPipe\n&gt;&gt;&gt; _pipe = DEMFitPipe(...)\n&gt;&gt;&gt; _pipe.train_pipeline()\n</code></pre> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def __init__(\n        self,\n        litdata_dir: str,\n        list_ncv: List[List[int]],\n        log_dir: str,\n        regression: bool,\n        devices: Union[List[int], str, int] = const.default.devices,\n        accelerator: str = const.default.accelerator,\n        n_jobs: int = const.default.n_jobs,\n        n_trials: Optional[int] = const.default.n_trials,\n        in_dev: bool = False,\n    ) -&gt; None:\n    r\"\"\"DEM model training pipeline with hyperparameter trials.\n\n    Args:\n        litdata_dir: Path to the directory containing the nested cross-validation litdata.\n\n        list_ncv: List of lists containing the indices of the outer and inner folds for each data slice.\n\n        log_dir: Path to the directory where the training logs and checkpoints will be saved.\n\n        regression: Whether the task is regression or classification.\n\n        devices: Device(s) to use.\n            Default: ``\"auto\"``.\n\n        accelerator: Accelerator to use.\n            Default: ``\"auto\"``.\n\n        n_jobs: Number of jobs to use for parallelization.\n            Default: ``1``.\n\n        n_trials: Number of trials to run for hyperparameter optimization.\n            Default: ``10``.\n\n        in_dev: Whether to run in development mode.\n            Default: ``False``.\n\n    Usage:\n\n        &gt;&gt;&gt; from biodem.dem.pipeline import DEMFitPipe\n        &gt;&gt;&gt; _pipe = DEMFitPipe(...)\n        &gt;&gt;&gt; _pipe.train_pipeline()\n\n    \"\"\"\n    self.in_dev = in_dev\n    # Unique tag for the training log directory\n    tag_str = time_string() + '_' + random_string()\n    self.uniq_logdir = os.path.join(log_dir, const.title_train + \"_\" + tag_str)\n    os.makedirs(self.uniq_logdir, exist_ok=False)\n\n    self.litdata_dir = litdata_dir\n    self.list_ncv = list_ncv\n    self.n_slice = len(list_ncv)\n\n    self.regression = regression\n    self.devices = devices\n    self.accelerator = accelerator\n    self.n_jobs = n_jobs\n    self.n_trials = n_trials\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMFitPipe.train_pipeline","title":"<code>train_pipeline()</code>","text":"<p>Train DEM model for each fold in nested cross-validation.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def train_pipeline(self):\n    r\"\"\"Train DEM model for each fold in nested cross-validation.\n    \"\"\"\n    # Storage for optuna trials in self.log_dir\n    path_storage = 'sqlite:///' + self.uniq_logdir + '/optuna.db'\n\n    print(f\"\\nNumber of data slices to train: {self.n_slice}\\n\")\n    log_names = []\n    for i in range(self.n_slice):\n        log_names.append(f'run_ncv_{self.list_ncv[i][0]}_{self.list_ncv[i][1]}')\n\n    # Train DEM model for each fold in nested cross-validation\n    if self.n_slice == 1:\n        dem_fit_ = DEMFit(\n            log_dir=self.uniq_logdir,\n            log_name=log_names[0],\n            litdata_dir=self.litdata_dir,\n            which_outer_testset=self.list_ncv[0][0],\n            which_inner_valset=self.list_ncv[0][1],\n            regression=self.regression,\n            devices=self.devices,\n            accelerator=self.accelerator,\n            n_jobs=self.n_jobs,\n            in_dev=self.in_dev,\n        )\n        dem_fit_.optimize(n_trials=self.n_trials, storage=path_storage)\n    else:\n        for xfold in range(self.n_slice):\n            dem_fit_ = DEMFit(\n                log_dir=self.uniq_logdir,\n                log_name=log_names[xfold],\n                litdata_dir=self.litdata_dir,\n                which_outer_testset=self.list_ncv[xfold][0],\n                which_inner_valset=self.list_ncv[xfold][1],\n                regression=self.regression,\n                devices=self.devices,\n                accelerator=self.accelerator,\n                n_jobs=self.n_jobs,\n                in_dev=self.in_dev,\n            )\n            dem_fit_.optimize(n_trials=self.n_trials, storage=path_storage)\n\n    # Remove checkpoints of inferior models\n    _collector = CollectFitLog(self.uniq_logdir)\n    _collector.remove_inferior_models()\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMPredict","title":"<code>DEMPredict</code>","text":"Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>class DEMPredict:\n    def __init__(self):\n        r\"\"\"Prediction pipeline for DEM model.\n        \"\"\"\n\n    def runs(\n            self,\n            litdata_dir: str,\n            dir_fit_logs: str,\n            dir_output: str,\n            list_ncv: Optional[List[List[int]]] = None,\n            overwrite_collected_log: bool = False,\n            accelerator: str = const.default.accelerator,\n            batch_size: int = const.default.batch_size,\n            n_workers: int = const.default.n_workers,\n        ):\n        r\"\"\"Run prediction for each fold in nested cross-validation.\n\n        Args:\n            litdata_dir: Path to the directory containing the nested cross-validation litdata.\n\n            dir_fit_logs: Path to the directory containing the training logs.\n\n            dir_output: Path to the directory where the prediction results will be saved.\n\n            list_ncv: List of lists containing the indices of the outer and inner folds for each data slice.\n\n            overwrite_collected_log: Whether to overwrite the collected log file.\n\n            accelerator: Accelerator to use.\n                Default: ``\"auto\"``.\n\n            batch_size: Batch size to use.\n                Default: ``32``.\n\n            n_workers: Number of workers to use for dataloader.\n                Default: ``1``.\n\n        \"\"\"\n        os.makedirs(dir_output, exist_ok=True)\n        if not hasattr(self, 'models_bv'):\n            self.collect_models(dir_fit_logs, dir_output, overwrite_collected_log)\n\n        if list_ncv is None:\n            # Take the best model's path overall by searching the line min `val_loss` in models_bi.\n            path_best_model = self.models_bi.filter(pl.col(const.dkey.val_loss) == self.models_bi.select(const.dkey.val_loss)).min().select(const.dkey.ckpt_path)[0,0]\n            output = self.predict(litdata_dir, path_best_model, dir_output, batch_size, accelerator, n_workers)\n            output.write_parquet(os.path.join(dir_output, const.fname.predicted_labels))\n\n            return None\n\n        # Else for each inner fold\n        for data_xx in list_ncv:\n            self.run_xo_xi(data_xx[0], data_xx[1], litdata_dir, dir_output, batch_size, accelerator, n_workers)\n\n        return None\n\n    def run_xo_xi(self, x_outer: int, x_inner: int, litdata_dir: str, dir_output: str, batch_size: int, accelerator: str = const.default.accelerator, n_workers: int = const.default.n_workers):\n        r\"\"\"Run prediction for a given outer and inner fold.\n        \"\"\"\n        os.makedirs(dir_output, exist_ok=True)\n        path_o_pred_trn = os.path.join(dir_output, const.fname.predicted_labels.replace(\".parquet\", f'_{x_outer}_{x_inner}_trn.parquet'))\n        path_o_pred_val = os.path.join(dir_output, const.fname.predicted_labels.replace(\".parquet\", f'_{x_outer}_{x_inner}_val.parquet'))\n        path_o_pred_tst = os.path.join(dir_output, const.fname.predicted_labels.replace(\".parquet\", f'_{x_outer}_{x_inner}_tst.parquet'))\n\n        path_mdl = self.models_bv.filter((pl.col(const.dkey.which_outer) == x_outer) &amp; (pl.col(const.dkey.which_inner) == x_inner)).select(const.dkey.ckpt_path)[0,0]\n        print(f'\\nUsing model {path_mdl}\\n')\n\n        ncv_data = DEMDataModule4Train(litdata_dir, x_outer, x_inner, batch_size, n_workers)\n        dir_train, dir_valid, dir_test = ncv_data.get_dir_ncv_litdata()\n\n        pred_trn = self.predict(dir_train, path_mdl, dir_output, batch_size, accelerator, n_workers)\n        pred_trn.write_parquet(path_o_pred_trn)\n        pred_val = self.predict(dir_valid, path_mdl, dir_output, batch_size, accelerator, n_workers)\n        pred_val.write_parquet(path_o_pred_val)\n        pred_tst = self.predict(dir_test, path_mdl, dir_output, batch_size, accelerator, n_workers)\n        pred_tst.write_parquet(path_o_pred_tst)\n        print(f'\\nPredicted labels saved to {path_o_pred_trn}, {path_o_pred_val}, {path_o_pred_tst}\\n')\n\n    def load_model(self, model_path: str, map_location: Optional[str] = None):\n        self._model = DEMLTN.load_from_checkpoint(\n            checkpoint_path=model_path,\n            map_location=get_map_location(map_location),\n        )\n        self._model.eval()\n        self._model.freeze()\n\n    def collect_models(self, dir_fit_logs: str, dir_output: str, overwrite_collected_log: bool = False):\n        r\"\"\"Collect trained models for each fold in nested cross-validation.\n        \"\"\"\n        os.makedirs(dir_output, exist_ok=True)\n        collector = CollectFitLog(dir_fit_logs)\n        models_bv, models_bi = collector.get_df_csv(dir_output, overwrite_collected_log)\n        self.models_bv = models_bv\n        self.models_bi = models_bi\n\n    def predict(\n            self,\n            litdata_dir: str,\n            path_model_ckpt: str,\n            dir_log_predict: Optional[str],\n            batch_size: int = const.default.batch_size,\n            accelerator: str = const.default.accelerator,\n            n_workers: int = const.default.n_workers,\n        ):\n        r\"\"\"Predict phenotypes from omics data using a trained DEM model.\n\n        Args:\n            litdata_dir: Path to the directory containing the litdata.\n                The directory should have a parent directory which is for nested cross-validation (e.g., ``ncv_test_0_val_0``).\n\n            path_model_ckpt: Path to a trained DEM model.\n\n            dir_log_predict: The directory to save the prediction logs.\n\n            batch_size: Batch size to use.\n                Default: ``32``.\n\n            accelerator: Accelerator to use.\n                Default: ``\"auto\"``.\n\n            n_workers: Number of workers to use for dataloader.\n                Default: ``1``.\n\n        \"\"\"\n        parent_dir = os.path.dirname(litdata_dir)\n        datamodule_ = DEMDataModule4Uni(litdata_dir, batch_size, n_workers)\n        datamodule_.setup()\n\n        if not hasattr(self, \"_model\"):\n            self.load_model(path_model_ckpt)\n\n        available_devices = get_avail_nvgpu()\n\n        trainer = Trainer(accelerator=accelerator, devices=available_devices, default_root_dir=dir_log_predict, logger=False)\n\n        pred_and_loss = trainer.predict(model=self._model, datamodule=datamodule_)\n        assert pred_and_loss is not None\n\n        predictions: List[np.ndarray] = []\n        for i_batch in range(len(pred_and_loss)):\n            predictions.append(pred_and_loss[i_batch][0])\n\n        pred_array = np.concatenate(predictions)\n        print(f\"Shape of prediction results: {pred_array.shape}\")\n\n        # Output\n\n        path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids)\n        path_label_names = os.path.join(parent_dir, const.fname.predata_label_names)\n\n        data_dir_name = os.path.basename(litdata_dir)\n        if data_dir_name.startswith(const.title_train):\n            path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids_trn)\n        elif data_dir_name.startswith(const.title_val):\n            path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids_val)\n        elif data_dir_name.startswith(const.title_test):\n            path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids_tst)\n        else:\n            raise ValueError(f'Unknown directory name: {litdata_dir}')\n\n        df_sample_ids = pl.read_csv(path_sample_ids)\n        assert len(df_sample_ids) == len(pred_array)\n\n        label_names = pl.read_csv(path_label_names)[const.dkey.label].to_list()\n\n        pred_df = pl.DataFrame(pred_array, schema=label_names)\n        pred_df = df_sample_ids.hstack(pred_df)\n        return pred_df\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMPredict.__init__","title":"<code>__init__()</code>","text":"<p>Prediction pipeline for DEM model.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def __init__(self):\n    r\"\"\"Prediction pipeline for DEM model.\n    \"\"\"\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMPredict.collect_models","title":"<code>collect_models(dir_fit_logs, dir_output, overwrite_collected_log=False)</code>","text":"<p>Collect trained models for each fold in nested cross-validation.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def collect_models(self, dir_fit_logs: str, dir_output: str, overwrite_collected_log: bool = False):\n    r\"\"\"Collect trained models for each fold in nested cross-validation.\n    \"\"\"\n    os.makedirs(dir_output, exist_ok=True)\n    collector = CollectFitLog(dir_fit_logs)\n    models_bv, models_bi = collector.get_df_csv(dir_output, overwrite_collected_log)\n    self.models_bv = models_bv\n    self.models_bi = models_bi\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMPredict.predict","title":"<code>predict(litdata_dir, path_model_ckpt, dir_log_predict, batch_size=const.default.batch_size, accelerator=const.default.accelerator, n_workers=const.default.n_workers)</code>","text":"<p>Predict phenotypes from omics data using a trained DEM model.</p> <p>Parameters:</p> Name Type Description Default <code>litdata_dir</code> <code>str</code> <p>Path to the directory containing the litdata. The directory should have a parent directory which is for nested cross-validation (e.g., <code>ncv_test_0_val_0</code>).</p> required <code>path_model_ckpt</code> <code>str</code> <p>Path to a trained DEM model.</p> required <code>dir_log_predict</code> <code>Optional[str]</code> <p>The directory to save the prediction logs.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use. Default: <code>32</code>.</p> <code>batch_size</code> <code>accelerator</code> <code>str</code> <p>Accelerator to use. Default: <code>\"auto\"</code>.</p> <code>accelerator</code> <code>n_workers</code> <code>int</code> <p>Number of workers to use for dataloader. Default: <code>1</code>.</p> <code>n_workers</code> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def predict(\n        self,\n        litdata_dir: str,\n        path_model_ckpt: str,\n        dir_log_predict: Optional[str],\n        batch_size: int = const.default.batch_size,\n        accelerator: str = const.default.accelerator,\n        n_workers: int = const.default.n_workers,\n    ):\n    r\"\"\"Predict phenotypes from omics data using a trained DEM model.\n\n    Args:\n        litdata_dir: Path to the directory containing the litdata.\n            The directory should have a parent directory which is for nested cross-validation (e.g., ``ncv_test_0_val_0``).\n\n        path_model_ckpt: Path to a trained DEM model.\n\n        dir_log_predict: The directory to save the prediction logs.\n\n        batch_size: Batch size to use.\n            Default: ``32``.\n\n        accelerator: Accelerator to use.\n            Default: ``\"auto\"``.\n\n        n_workers: Number of workers to use for dataloader.\n            Default: ``1``.\n\n    \"\"\"\n    parent_dir = os.path.dirname(litdata_dir)\n    datamodule_ = DEMDataModule4Uni(litdata_dir, batch_size, n_workers)\n    datamodule_.setup()\n\n    if not hasattr(self, \"_model\"):\n        self.load_model(path_model_ckpt)\n\n    available_devices = get_avail_nvgpu()\n\n    trainer = Trainer(accelerator=accelerator, devices=available_devices, default_root_dir=dir_log_predict, logger=False)\n\n    pred_and_loss = trainer.predict(model=self._model, datamodule=datamodule_)\n    assert pred_and_loss is not None\n\n    predictions: List[np.ndarray] = []\n    for i_batch in range(len(pred_and_loss)):\n        predictions.append(pred_and_loss[i_batch][0])\n\n    pred_array = np.concatenate(predictions)\n    print(f\"Shape of prediction results: {pred_array.shape}\")\n\n    # Output\n\n    path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids)\n    path_label_names = os.path.join(parent_dir, const.fname.predata_label_names)\n\n    data_dir_name = os.path.basename(litdata_dir)\n    if data_dir_name.startswith(const.title_train):\n        path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids_trn)\n    elif data_dir_name.startswith(const.title_val):\n        path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids_val)\n    elif data_dir_name.startswith(const.title_test):\n        path_sample_ids = os.path.join(parent_dir, const.fname.predata_ids_tst)\n    else:\n        raise ValueError(f'Unknown directory name: {litdata_dir}')\n\n    df_sample_ids = pl.read_csv(path_sample_ids)\n    assert len(df_sample_ids) == len(pred_array)\n\n    label_names = pl.read_csv(path_label_names)[const.dkey.label].to_list()\n\n    pred_df = pl.DataFrame(pred_array, schema=label_names)\n    pred_df = df_sample_ids.hstack(pred_df)\n    return pred_df\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMPredict.run_xo_xi","title":"<code>run_xo_xi(x_outer, x_inner, litdata_dir, dir_output, batch_size, accelerator=const.default.accelerator, n_workers=const.default.n_workers)</code>","text":"<p>Run prediction for a given outer and inner fold.</p> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def run_xo_xi(self, x_outer: int, x_inner: int, litdata_dir: str, dir_output: str, batch_size: int, accelerator: str = const.default.accelerator, n_workers: int = const.default.n_workers):\n    r\"\"\"Run prediction for a given outer and inner fold.\n    \"\"\"\n    os.makedirs(dir_output, exist_ok=True)\n    path_o_pred_trn = os.path.join(dir_output, const.fname.predicted_labels.replace(\".parquet\", f'_{x_outer}_{x_inner}_trn.parquet'))\n    path_o_pred_val = os.path.join(dir_output, const.fname.predicted_labels.replace(\".parquet\", f'_{x_outer}_{x_inner}_val.parquet'))\n    path_o_pred_tst = os.path.join(dir_output, const.fname.predicted_labels.replace(\".parquet\", f'_{x_outer}_{x_inner}_tst.parquet'))\n\n    path_mdl = self.models_bv.filter((pl.col(const.dkey.which_outer) == x_outer) &amp; (pl.col(const.dkey.which_inner) == x_inner)).select(const.dkey.ckpt_path)[0,0]\n    print(f'\\nUsing model {path_mdl}\\n')\n\n    ncv_data = DEMDataModule4Train(litdata_dir, x_outer, x_inner, batch_size, n_workers)\n    dir_train, dir_valid, dir_test = ncv_data.get_dir_ncv_litdata()\n\n    pred_trn = self.predict(dir_train, path_mdl, dir_output, batch_size, accelerator, n_workers)\n    pred_trn.write_parquet(path_o_pred_trn)\n    pred_val = self.predict(dir_valid, path_mdl, dir_output, batch_size, accelerator, n_workers)\n    pred_val.write_parquet(path_o_pred_val)\n    pred_tst = self.predict(dir_test, path_mdl, dir_output, batch_size, accelerator, n_workers)\n    pred_tst.write_parquet(path_o_pred_tst)\n    print(f'\\nPredicted labels saved to {path_o_pred_trn}, {path_o_pred_val}, {path_o_pred_tst}\\n')\n</code></pre>"},{"location":"reference/biodem.dem.pipeline/#biodem.dem.pipeline.DEMPredict.runs","title":"<code>runs(litdata_dir, dir_fit_logs, dir_output, list_ncv=None, overwrite_collected_log=False, accelerator=const.default.accelerator, batch_size=const.default.batch_size, n_workers=const.default.n_workers)</code>","text":"<p>Run prediction for each fold in nested cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>litdata_dir</code> <code>str</code> <p>Path to the directory containing the nested cross-validation litdata.</p> required <code>dir_fit_logs</code> <code>str</code> <p>Path to the directory containing the training logs.</p> required <code>dir_output</code> <code>str</code> <p>Path to the directory where the prediction results will be saved.</p> required <code>list_ncv</code> <code>Optional[List[List[int]]]</code> <p>List of lists containing the indices of the outer and inner folds for each data slice.</p> <code>None</code> <code>overwrite_collected_log</code> <code>bool</code> <p>Whether to overwrite the collected log file.</p> <code>False</code> <code>accelerator</code> <code>str</code> <p>Accelerator to use. Default: <code>\"auto\"</code>.</p> <code>accelerator</code> <code>batch_size</code> <code>int</code> <p>Batch size to use. Default: <code>32</code>.</p> <code>batch_size</code> <code>n_workers</code> <code>int</code> <p>Number of workers to use for dataloader. Default: <code>1</code>.</p> <code>n_workers</code> Source code in <code>src\\biodem\\dem\\pipeline.py</code> <pre><code>def runs(\n        self,\n        litdata_dir: str,\n        dir_fit_logs: str,\n        dir_output: str,\n        list_ncv: Optional[List[List[int]]] = None,\n        overwrite_collected_log: bool = False,\n        accelerator: str = const.default.accelerator,\n        batch_size: int = const.default.batch_size,\n        n_workers: int = const.default.n_workers,\n    ):\n    r\"\"\"Run prediction for each fold in nested cross-validation.\n\n    Args:\n        litdata_dir: Path to the directory containing the nested cross-validation litdata.\n\n        dir_fit_logs: Path to the directory containing the training logs.\n\n        dir_output: Path to the directory where the prediction results will be saved.\n\n        list_ncv: List of lists containing the indices of the outer and inner folds for each data slice.\n\n        overwrite_collected_log: Whether to overwrite the collected log file.\n\n        accelerator: Accelerator to use.\n            Default: ``\"auto\"``.\n\n        batch_size: Batch size to use.\n            Default: ``32``.\n\n        n_workers: Number of workers to use for dataloader.\n            Default: ``1``.\n\n    \"\"\"\n    os.makedirs(dir_output, exist_ok=True)\n    if not hasattr(self, 'models_bv'):\n        self.collect_models(dir_fit_logs, dir_output, overwrite_collected_log)\n\n    if list_ncv is None:\n        # Take the best model's path overall by searching the line min `val_loss` in models_bi.\n        path_best_model = self.models_bi.filter(pl.col(const.dkey.val_loss) == self.models_bi.select(const.dkey.val_loss)).min().select(const.dkey.ckpt_path)[0,0]\n        output = self.predict(litdata_dir, path_best_model, dir_output, batch_size, accelerator, n_workers)\n        output.write_parquet(os.path.join(dir_output, const.fname.predicted_labels))\n\n        return None\n\n    # Else for each inner fold\n    for data_xx in list_ncv:\n        self.run_xo_xi(data_xx[0], data_xx[1], litdata_dir, dir_output, batch_size, accelerator, n_workers)\n\n    return None\n</code></pre>"},{"location":"reference/biodem.dem.rank/","title":"biodem.dem.rank","text":""},{"location":"reference/biodem.dem.rank/#biodem.dem.rank","title":"<code>biodem.dem.rank</code>","text":"<p>Feature ranking.</p>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking","title":"<code>DEMFeatureRanking</code>","text":"Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>class DEMFeatureRanking:\n    def __init__(\n            self,\n            batch_size: int = const.default.batch_size,\n            n_workers: int = const.default.n_workers,\n            accelerator: str = const.default.accelerator,\n            map_location: Optional[str] = None,\n        ):\n        r\"\"\"Feature ranking using a trained DEM model.\n\n        Args:\n            batch_size: Batch size for prediction.\n\n            n_workers: Number of workers for data loading.\n\n            accelerator: Accelerator for inference.\n\n            map_location: Map location for loading the model.\n\n        \"\"\"\n        self.batch_size = batch_size\n        self.n_workers = n_workers\n        self.accelerator = accelerator\n        self.map_location = map_location\n\n    def run_a_outer(self, ncv_litdata_dir: str, fit_log_dir: str, which_outer: int, output_path: str, random_states: list[int]):\n        r\"\"\"Run feature ranking for a single outer fold in nested cross-validation.\n        This function searches the best inner fold for the specified outer fold and runs feature ranking on the test set.\n\n        Args:\n            ncv_litdata_dir: Path to the directory containing the nested cross-validation litdata.\n\n            fit_log_dir: Path to the directory containing the training logs.\n\n            which_outer: Which outer fold to run.\n\n            output_path: Path to the file to save the results.\n\n            random_states: List of random states for shuffling values of each feature.\n\n        \"\"\"\n        # Collect fit logs\n        collector = CollectFitLog(fit_log_dir)\n        fit_logs = collector.collect()\n        log_best_inner_foreach_outer = fit_logs[const.dkey.best_inner_folds]\n\n        row_x_outer = log_best_inner_foreach_outer.filter(pl.col(const.dkey.which_outer)==which_outer)\n        _model_path = row_x_outer[const.dkey.ckpt_path][0]\n        _which_inner = row_x_outer[const.dkey.which_inner][0]\n        _litdata_dir = os.path.join(ncv_litdata_dir, f\"ncv_test_{which_outer}_val_{_which_inner}\", const.title_test)\n\n        self.run(_model_path, _litdata_dir, output_path, random_states)\n\n    def run(self, model_path: str, litdata_dir: str, output_path: str, random_states: list[int]):\n        r\"\"\"Rank features.\n\n        Args:\n            model_path: Path to the model's checkpoint.\n\n            litdata_dir: Path to the directory containing litdata.\n\n            output_path: Path to the file to save the results.\n\n            random_states: List of random states for shuffling features.\n\n        \"\"\"\n        _filename = os.path.splitext(output_path)[0]\n        dir_save_pred = _filename + \"_pred\"\n        os.makedirs(dir_save_pred, exist_ok=True)\n\n        # Load model\n        self.load_model(model_path)\n\n        # Load data\n        self._datamodule = DEMDataModule4Uni(litdata_dir, self.batch_size, self.n_workers)\n        self._datamodule.setup()\n\n        # Get original prediction loss\n        prediction_and_loss_values = self.trainer.predict(model=self._model, datamodule=self._datamodule)\n        assert prediction_and_loss_values is not None\n        predictions, loss = self.prep_pred_and_loss(prediction_and_loss_values)\n        print(f\"\\nOriginal loss: {loss:.4f}\\n\")\n        # Write original prediction and loss\n        _path_npz = os.path.join(dir_save_pred, \"_original.npz\")\n        np.savez(_path_npz, predictions, loss)\n\n        # Shuffle features and get shuffled prediction loss.\n        omics_names, omics_feat_paths = read_omics_names(litdata_dir, True)\n        importance_scores: List[float] = []\n        feature_names: List[str] = []\n        which_omics = []\n        for x_om in range(len(omics_names)):\n            _feat_names: List[str] = pl.read_csv(omics_feat_paths[x_om]).to_series().to_list()\n            n_features = len(_feat_names)\n            for x_feat in range(n_features):\n                feature_names.append(_feat_names[x_feat])\n                which_omics.append(omics_names[x_om])\n                _loss, _pred = self.run_a_feat(x_om, x_feat, random_states)\n                _score = np.mean([np.abs(_iv - loss) for _iv in _loss]).item() / loss\n                print(f\"\\nAverage impact of {_feat_names[x_feat]} on {omics_names[x_om]} : {_score:.4f}\\n\")\n                importance_scores.append(_score.item())\n\n                # Write predicted labels `_pred` (List[np.ndarray]) to npz file\n                _path_npz = os.path.join(dir_save_pred, f\"om+{x_om}_feat+{x_feat}.npz\")\n                np.savez(_path_npz, _pred, _loss)\n\n                # Save/Append omics name and feature name to a text file\n                with open(os.path.join(dir_save_pred, \"_log.txt\"), \"a\") as f:\n                    f.write(f\"{x_om}\\t{x_feat}\\t{omics_names[x_om]}\\t{_feat_names[x_feat]}\\t{_score}\\t{time_string()}\\n\")\n\n        # Rank features by their importance scores\n        _sortperm = np.argsort(importance_scores)[::-1].tolist()\n\n        importance_scores = [importance_scores[i] for i in _sortperm]\n        feature_names = [feature_names[i] for i in _sortperm]\n        which_omics = [which_omics[i] for i in _sortperm]\n\n        # Save feature ranking results as CSV and Parquet\n        df_o = pl.DataFrame({const.dkey.omics: which_omics, const.dkey.feature: feature_names, const.dkey.feat_importance: importance_scores})\n        df_o.write_csv(_filename + \".csv\")\n        df_o.write_parquet(_filename + \".parquet\")\n\n    def run_a_feat(self, which_omics: Union[int, str], which_feature: int, random_states: List[int], litdata_dir: Optional[str]=None, model_path: Optional[str]=None):\n        r\"\"\" Get average loss for a single shuffled feature.\n\n        Args:\n            which_omics: Which omics to shuffle.\n\n            which_feature: Which feature to shuffle.\n\n            random_states: List of random states for shuffling values of the specified feature.\n\n            litdata_dir: Path to the directory containing litdata.\n\n            model_path: Path to the model's checkpoint.\n\n        \"\"\"\n        if not hasattr(self, '_datamodule'):\n            if litdata_dir is not None:\n                self._datamodule = DEMDataModule4Uni(litdata_dir, self.batch_size, self.n_workers)\n            else:\n                raise ValueError(\"Please specify litdata_dir.\")\n        if not hasattr(self, \"trainer\"):\n            if model_path is not None:\n                self.load_model(model_path)\n            else:\n                raise ValueError(\"Please specify model_path.\")\n\n        losses: List[float] = []\n        predictions: List[np.ndarray] = []\n\n        for random_state in random_states:\n            _dataloader = self._datamodule.shuffle_a_feat(which_omics, which_feature, random_state)\n\n            prediction_and_loss_shuffled = self.trainer.predict(self._model, _dataloader)\n            assert prediction_and_loss_shuffled is not None\n            predictions_shuffled, losses_shuffled = self.prep_pred_and_loss(prediction_and_loss_shuffled)\n            print(f\"\\nShuffled loss: {losses_shuffled:.4f}\\n\")\n            losses.append(losses_shuffled.item())\n            predictions.append(predictions_shuffled)\n\n        return losses, predictions\n\n    def collect_ranks(self, log_dir: str, output_path: str, overwrite: bool = True):\n        r\"\"\"Collect feature ranking results from log files of multiple outer test sets.\n\n        Args:\n            log_dir: Path to the directory containing ranking log files.\n\n            output_path: Path to the output file.\n\n            overwrite: Whether to overwrite the output file if it already exists.\n\n        \"\"\"\n        # Read log files in parquet format for outer test sets\n        fpaths = sorted([os.path.join(log_dir, f) for f in os.listdir(log_dir) if f.endswith(\".parquet\")])\n        dfs = {i: pl.read_parquet(fpaths[i]).rename({\"importance\": f\"importance_{str(i+1)}\"}) for i in range(len(fpaths))}\n\n        # Merge dataframes and save shared features\n        df_merged = dfs[0]\n        for i in range(1, len(dfs)):\n            df_merged = df_merged.join(dfs[i], on=[\"omics\", \"feature\"])\n\n        # Normalize importance scores for each outer test set\n        np_merged_norm = df_merged.drop([\"omics\", \"feature\"]).to_numpy()\n        min_vals = np_merged_norm.min(axis=0)\n        max_vals = np_merged_norm.max(axis=0)\n        np_merged_norm = (np_merged_norm - min_vals) / (max_vals - min_vals)\n        df_merged_norm = df_merged.select([\"omics\", \"feature\"]).hstack(pl.DataFrame(np_merged_norm, schema=[\"test_\"+str(i+1) for i in range(np_merged_norm.shape[1])]))\n\n        # Average normalized importance scores\n        np_mean = np_merged_norm.mean(axis=1)\n        df_merged_norm = df_merged_norm.hstack(pl.DataFrame({\"average\": np_mean})).sort([\"average\"], descending=True)\n        df_merged_norm = df_merged_norm.select([\"omics\", \"feature\", \"average\"]).hstack(df_merged_norm.drop([\"omics\", \"feature\", \"average\"]))\n\n        # Write results to files\n        _output_path = os.path.splitext(output_path)[0]\n        _output_path_1 = _output_path + \".parquet\"\n        _output_path_2 = _output_path + \".csv\"\n        if not os.path.exists(_output_path_1) or overwrite:\n            df_merged_norm.write_parquet(_output_path_1)\n            df_merged_norm.write_csv(_output_path_2)\n\n    def load_model(self, model_path: str):\n        r\"\"\"Load a model's checkpoint to specified device and define a trainer.\n\n        Args:\n            model_path: Path to the model's checkpoint.\n\n        \"\"\"\n        self._model = DEMLTN.load_from_checkpoint(\n            checkpoint_path=model_path,\n            map_location=get_map_location(self.map_location),\n        )\n        self._model.eval()\n        self._model.freeze()\n        self.available_devices = get_avail_nvgpu()\n        self.trainer = Trainer(accelerator=self.accelerator, devices=self.available_devices, default_root_dir=None, logger=False)\n\n    def prep_pred_and_loss(self, _pred):\n        r\"\"\"Prepare the output of \"predict\" step.\n        \"\"\"\n        _predicted_each_batch = [np.array(i[0]) for i in _pred]\n        _predicted = np.concatenate(_predicted_each_batch, axis=0)\n        print(f\"Shape of predicted: {_predicted.shape}\")\n\n        # Get actual batch sizes (The last batch may be smaller than others)\n        _batch_sizes = [len(i) for i in _predicted_each_batch]\n\n        _loss_each_batch = np.concatenate([np.array(i[1], ndmin=1) for i in _pred])\n\n        # Weight the loss by batch size\n        _loss = np.average(_loss_each_batch, weights=_batch_sizes)\n\n        return _predicted, _loss\n</code></pre>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking.__init__","title":"<code>__init__(batch_size=const.default.batch_size, n_workers=const.default.n_workers, accelerator=const.default.accelerator, map_location=None)</code>","text":"<p>Feature ranking using a trained DEM model.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> <code>batch_size</code> <code>n_workers</code> <code>int</code> <p>Number of workers for data loading.</p> <code>n_workers</code> <code>accelerator</code> <code>str</code> <p>Accelerator for inference.</p> <code>accelerator</code> <code>map_location</code> <code>Optional[str]</code> <p>Map location for loading the model.</p> <code>None</code> Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>def __init__(\n        self,\n        batch_size: int = const.default.batch_size,\n        n_workers: int = const.default.n_workers,\n        accelerator: str = const.default.accelerator,\n        map_location: Optional[str] = None,\n    ):\n    r\"\"\"Feature ranking using a trained DEM model.\n\n    Args:\n        batch_size: Batch size for prediction.\n\n        n_workers: Number of workers for data loading.\n\n        accelerator: Accelerator for inference.\n\n        map_location: Map location for loading the model.\n\n    \"\"\"\n    self.batch_size = batch_size\n    self.n_workers = n_workers\n    self.accelerator = accelerator\n    self.map_location = map_location\n</code></pre>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking.collect_ranks","title":"<code>collect_ranks(log_dir, output_path, overwrite=True)</code>","text":"<p>Collect feature ranking results from log files of multiple outer test sets.</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str</code> <p>Path to the directory containing ranking log files.</p> required <code>output_path</code> <code>str</code> <p>Path to the output file.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file if it already exists.</p> <code>True</code> Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>def collect_ranks(self, log_dir: str, output_path: str, overwrite: bool = True):\n    r\"\"\"Collect feature ranking results from log files of multiple outer test sets.\n\n    Args:\n        log_dir: Path to the directory containing ranking log files.\n\n        output_path: Path to the output file.\n\n        overwrite: Whether to overwrite the output file if it already exists.\n\n    \"\"\"\n    # Read log files in parquet format for outer test sets\n    fpaths = sorted([os.path.join(log_dir, f) for f in os.listdir(log_dir) if f.endswith(\".parquet\")])\n    dfs = {i: pl.read_parquet(fpaths[i]).rename({\"importance\": f\"importance_{str(i+1)}\"}) for i in range(len(fpaths))}\n\n    # Merge dataframes and save shared features\n    df_merged = dfs[0]\n    for i in range(1, len(dfs)):\n        df_merged = df_merged.join(dfs[i], on=[\"omics\", \"feature\"])\n\n    # Normalize importance scores for each outer test set\n    np_merged_norm = df_merged.drop([\"omics\", \"feature\"]).to_numpy()\n    min_vals = np_merged_norm.min(axis=0)\n    max_vals = np_merged_norm.max(axis=0)\n    np_merged_norm = (np_merged_norm - min_vals) / (max_vals - min_vals)\n    df_merged_norm = df_merged.select([\"omics\", \"feature\"]).hstack(pl.DataFrame(np_merged_norm, schema=[\"test_\"+str(i+1) for i in range(np_merged_norm.shape[1])]))\n\n    # Average normalized importance scores\n    np_mean = np_merged_norm.mean(axis=1)\n    df_merged_norm = df_merged_norm.hstack(pl.DataFrame({\"average\": np_mean})).sort([\"average\"], descending=True)\n    df_merged_norm = df_merged_norm.select([\"omics\", \"feature\", \"average\"]).hstack(df_merged_norm.drop([\"omics\", \"feature\", \"average\"]))\n\n    # Write results to files\n    _output_path = os.path.splitext(output_path)[0]\n    _output_path_1 = _output_path + \".parquet\"\n    _output_path_2 = _output_path + \".csv\"\n    if not os.path.exists(_output_path_1) or overwrite:\n        df_merged_norm.write_parquet(_output_path_1)\n        df_merged_norm.write_csv(_output_path_2)\n</code></pre>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking.load_model","title":"<code>load_model(model_path)</code>","text":"<p>Load a model's checkpoint to specified device and define a trainer.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model's checkpoint.</p> required Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>def load_model(self, model_path: str):\n    r\"\"\"Load a model's checkpoint to specified device and define a trainer.\n\n    Args:\n        model_path: Path to the model's checkpoint.\n\n    \"\"\"\n    self._model = DEMLTN.load_from_checkpoint(\n        checkpoint_path=model_path,\n        map_location=get_map_location(self.map_location),\n    )\n    self._model.eval()\n    self._model.freeze()\n    self.available_devices = get_avail_nvgpu()\n    self.trainer = Trainer(accelerator=self.accelerator, devices=self.available_devices, default_root_dir=None, logger=False)\n</code></pre>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking.prep_pred_and_loss","title":"<code>prep_pred_and_loss(_pred)</code>","text":"<p>Prepare the output of \"predict\" step.</p> Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>def prep_pred_and_loss(self, _pred):\n    r\"\"\"Prepare the output of \"predict\" step.\n    \"\"\"\n    _predicted_each_batch = [np.array(i[0]) for i in _pred]\n    _predicted = np.concatenate(_predicted_each_batch, axis=0)\n    print(f\"Shape of predicted: {_predicted.shape}\")\n\n    # Get actual batch sizes (The last batch may be smaller than others)\n    _batch_sizes = [len(i) for i in _predicted_each_batch]\n\n    _loss_each_batch = np.concatenate([np.array(i[1], ndmin=1) for i in _pred])\n\n    # Weight the loss by batch size\n    _loss = np.average(_loss_each_batch, weights=_batch_sizes)\n\n    return _predicted, _loss\n</code></pre>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking.run","title":"<code>run(model_path, litdata_dir, output_path, random_states)</code>","text":"<p>Rank features.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model's checkpoint.</p> required <code>litdata_dir</code> <code>str</code> <p>Path to the directory containing litdata.</p> required <code>output_path</code> <code>str</code> <p>Path to the file to save the results.</p> required <code>random_states</code> <code>list[int]</code> <p>List of random states for shuffling features.</p> required Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>def run(self, model_path: str, litdata_dir: str, output_path: str, random_states: list[int]):\n    r\"\"\"Rank features.\n\n    Args:\n        model_path: Path to the model's checkpoint.\n\n        litdata_dir: Path to the directory containing litdata.\n\n        output_path: Path to the file to save the results.\n\n        random_states: List of random states for shuffling features.\n\n    \"\"\"\n    _filename = os.path.splitext(output_path)[0]\n    dir_save_pred = _filename + \"_pred\"\n    os.makedirs(dir_save_pred, exist_ok=True)\n\n    # Load model\n    self.load_model(model_path)\n\n    # Load data\n    self._datamodule = DEMDataModule4Uni(litdata_dir, self.batch_size, self.n_workers)\n    self._datamodule.setup()\n\n    # Get original prediction loss\n    prediction_and_loss_values = self.trainer.predict(model=self._model, datamodule=self._datamodule)\n    assert prediction_and_loss_values is not None\n    predictions, loss = self.prep_pred_and_loss(prediction_and_loss_values)\n    print(f\"\\nOriginal loss: {loss:.4f}\\n\")\n    # Write original prediction and loss\n    _path_npz = os.path.join(dir_save_pred, \"_original.npz\")\n    np.savez(_path_npz, predictions, loss)\n\n    # Shuffle features and get shuffled prediction loss.\n    omics_names, omics_feat_paths = read_omics_names(litdata_dir, True)\n    importance_scores: List[float] = []\n    feature_names: List[str] = []\n    which_omics = []\n    for x_om in range(len(omics_names)):\n        _feat_names: List[str] = pl.read_csv(omics_feat_paths[x_om]).to_series().to_list()\n        n_features = len(_feat_names)\n        for x_feat in range(n_features):\n            feature_names.append(_feat_names[x_feat])\n            which_omics.append(omics_names[x_om])\n            _loss, _pred = self.run_a_feat(x_om, x_feat, random_states)\n            _score = np.mean([np.abs(_iv - loss) for _iv in _loss]).item() / loss\n            print(f\"\\nAverage impact of {_feat_names[x_feat]} on {omics_names[x_om]} : {_score:.4f}\\n\")\n            importance_scores.append(_score.item())\n\n            # Write predicted labels `_pred` (List[np.ndarray]) to npz file\n            _path_npz = os.path.join(dir_save_pred, f\"om+{x_om}_feat+{x_feat}.npz\")\n            np.savez(_path_npz, _pred, _loss)\n\n            # Save/Append omics name and feature name to a text file\n            with open(os.path.join(dir_save_pred, \"_log.txt\"), \"a\") as f:\n                f.write(f\"{x_om}\\t{x_feat}\\t{omics_names[x_om]}\\t{_feat_names[x_feat]}\\t{_score}\\t{time_string()}\\n\")\n\n    # Rank features by their importance scores\n    _sortperm = np.argsort(importance_scores)[::-1].tolist()\n\n    importance_scores = [importance_scores[i] for i in _sortperm]\n    feature_names = [feature_names[i] for i in _sortperm]\n    which_omics = [which_omics[i] for i in _sortperm]\n\n    # Save feature ranking results as CSV and Parquet\n    df_o = pl.DataFrame({const.dkey.omics: which_omics, const.dkey.feature: feature_names, const.dkey.feat_importance: importance_scores})\n    df_o.write_csv(_filename + \".csv\")\n    df_o.write_parquet(_filename + \".parquet\")\n</code></pre>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking.run_a_feat","title":"<code>run_a_feat(which_omics, which_feature, random_states, litdata_dir=None, model_path=None)</code>","text":"<p>Get average loss for a single shuffled feature.</p> <p>Parameters:</p> Name Type Description Default <code>which_omics</code> <code>Union[int, str]</code> <p>Which omics to shuffle.</p> required <code>which_feature</code> <code>int</code> <p>Which feature to shuffle.</p> required <code>random_states</code> <code>List[int]</code> <p>List of random states for shuffling values of the specified feature.</p> required <code>litdata_dir</code> <code>Optional[str]</code> <p>Path to the directory containing litdata.</p> <code>None</code> <code>model_path</code> <code>Optional[str]</code> <p>Path to the model's checkpoint.</p> <code>None</code> Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>def run_a_feat(self, which_omics: Union[int, str], which_feature: int, random_states: List[int], litdata_dir: Optional[str]=None, model_path: Optional[str]=None):\n    r\"\"\" Get average loss for a single shuffled feature.\n\n    Args:\n        which_omics: Which omics to shuffle.\n\n        which_feature: Which feature to shuffle.\n\n        random_states: List of random states for shuffling values of the specified feature.\n\n        litdata_dir: Path to the directory containing litdata.\n\n        model_path: Path to the model's checkpoint.\n\n    \"\"\"\n    if not hasattr(self, '_datamodule'):\n        if litdata_dir is not None:\n            self._datamodule = DEMDataModule4Uni(litdata_dir, self.batch_size, self.n_workers)\n        else:\n            raise ValueError(\"Please specify litdata_dir.\")\n    if not hasattr(self, \"trainer\"):\n        if model_path is not None:\n            self.load_model(model_path)\n        else:\n            raise ValueError(\"Please specify model_path.\")\n\n    losses: List[float] = []\n    predictions: List[np.ndarray] = []\n\n    for random_state in random_states:\n        _dataloader = self._datamodule.shuffle_a_feat(which_omics, which_feature, random_state)\n\n        prediction_and_loss_shuffled = self.trainer.predict(self._model, _dataloader)\n        assert prediction_and_loss_shuffled is not None\n        predictions_shuffled, losses_shuffled = self.prep_pred_and_loss(prediction_and_loss_shuffled)\n        print(f\"\\nShuffled loss: {losses_shuffled:.4f}\\n\")\n        losses.append(losses_shuffled.item())\n        predictions.append(predictions_shuffled)\n\n    return losses, predictions\n</code></pre>"},{"location":"reference/biodem.dem.rank/#biodem.dem.rank.DEMFeatureRanking.run_a_outer","title":"<code>run_a_outer(ncv_litdata_dir, fit_log_dir, which_outer, output_path, random_states)</code>","text":"<p>Run feature ranking for a single outer fold in nested cross-validation. This function searches the best inner fold for the specified outer fold and runs feature ranking on the test set.</p> <p>Parameters:</p> Name Type Description Default <code>ncv_litdata_dir</code> <code>str</code> <p>Path to the directory containing the nested cross-validation litdata.</p> required <code>fit_log_dir</code> <code>str</code> <p>Path to the directory containing the training logs.</p> required <code>which_outer</code> <code>int</code> <p>Which outer fold to run.</p> required <code>output_path</code> <code>str</code> <p>Path to the file to save the results.</p> required <code>random_states</code> <code>list[int]</code> <p>List of random states for shuffling values of each feature.</p> required Source code in <code>src\\biodem\\dem\\rank.py</code> <pre><code>def run_a_outer(self, ncv_litdata_dir: str, fit_log_dir: str, which_outer: int, output_path: str, random_states: list[int]):\n    r\"\"\"Run feature ranking for a single outer fold in nested cross-validation.\n    This function searches the best inner fold for the specified outer fold and runs feature ranking on the test set.\n\n    Args:\n        ncv_litdata_dir: Path to the directory containing the nested cross-validation litdata.\n\n        fit_log_dir: Path to the directory containing the training logs.\n\n        which_outer: Which outer fold to run.\n\n        output_path: Path to the file to save the results.\n\n        random_states: List of random states for shuffling values of each feature.\n\n    \"\"\"\n    # Collect fit logs\n    collector = CollectFitLog(fit_log_dir)\n    fit_logs = collector.collect()\n    log_best_inner_foreach_outer = fit_logs[const.dkey.best_inner_folds]\n\n    row_x_outer = log_best_inner_foreach_outer.filter(pl.col(const.dkey.which_outer)==which_outer)\n    _model_path = row_x_outer[const.dkey.ckpt_path][0]\n    _which_inner = row_x_outer[const.dkey.which_inner][0]\n    _litdata_dir = os.path.join(ncv_litdata_dir, f\"ncv_test_{which_outer}_val_{_which_inner}\", const.title_test)\n\n    self.run(_model_path, _litdata_dir, output_path, random_states)\n</code></pre>"},{"location":"reference/biodem.dem.usage/","title":"How to use <code>biodem.dem</code>","text":""},{"location":"reference/biodem.dem.usage/#1-nested-cross-validation-and-data-preprocessing","title":"1. Nested cross-validation and data preprocessing","text":"<p>Please checkout the documentations at Modules &gt; Utilities &gt; Preprocessing Data &gt; <code>OptimizeLitdataNCV</code>.</p> <p>This is an example of running the module:</p> run_dem_prep.py<pre><code>import os\nimport sys\nfrom biodem import OptimizeLitdataNCV\n\n\ntrait_name = sys.argv[1]\nwhich_o = int(sys.argv[2])\nwhich_i = int(sys.argv[3])\n\nif trait_name.startswith(\"all\"):\n    which_trait = None\nelse:\n    which_trait = [trait_name]\n\n\nif __name__ == \"__main__\":\n    k_outer = 10\n    k_inner = 5\n    dir_home = os.path.dirname(os.path.abspath(__file__))\n    output_dir = os.path.join(dir_home, \"run_dem\", trait_name, \"litdata\")\n\n    path_labels = os.path.join(dir_home, \"data_prep\", \"phenotypes.csv\")\n\n    path_transformed_genotypes = os.path.join(dir_home, \"run_s2g\", trait_name, \"transf\")\n    path_metabolome = os.path.join(dir_home, \"data_prep\", \"omics_metabolome.parquet\")\n    path_fpkm = os.path.join(dir_home, \"data_prep\", \"omics_fpkm_log2.parquet\")\n\n    dict_omics = {\n        \"transcriptome\": path_fpkm,\n        \"metabolome\": path_metabolome,\n        \"genotype\": path_transformed_genotypes,\n    }\n\n    _opt = OptimizeLitdataNCV(\n        paths_omics = dict_omics,\n        path_label = path_labels,\n        output_dir = output_dir,\n        k_outer = k_outer,\n        k_inner = k_inner,\n        which_outer_inner = [which_o, which_i],\n        col2use_in_labels = which_trait,\n    )\n    _opt.run_optimization()\n</code></pre>"},{"location":"reference/biodem.dem.usage/#2-dual-extraction-modeling","title":"2. Dual-extraction modeling","text":"<p>Please checkout the documentations at Modules &gt; DEM &gt; Pipeline &gt; <code>DEMFitPipe</code>.</p> <p>This is an example of running the module:</p> run_dem_fit.py<pre><code>import os\nimport sys\nfrom biodem import DEMFitPipe\n\n\nif len(sys.argv) &lt; 2:\n    raise ValueError('Please specify the TRAIT NAME')\ntrait_name = sys.argv[1]\n\nif len(sys.argv) &lt; 4:\n    print('Start default NCV: 10 outer folds and 5 inner folds')\n    list_ncv = [[i,j] for i in range(10) for j in range(5)]\nelse:\n    print('Start with NCV: {} outer folds and {} inner folds'.format(sys.argv[2], sys.argv[3]))\n    list_ncv = [[int(sys.argv[2]), int(sys.argv[3])]]\n\nwork_dir_home = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run_dem\", trait_name)\nlitdata_dir = os.path.join(work_dir_home, 'litdata')\nis_regression = True\nlog_dir = os.path.join(work_dir_home, 'models')\n\n\nif __name__ == '__main__':\n    _pipe = DEMFitPipe(\n        litdata_dir=litdata_dir,\n        list_ncv=list_ncv,\n        log_dir=log_dir,\n        regression=is_regression,\n    )\n    _pipe.train_pipeline()\n</code></pre>"},{"location":"reference/biodem.dem.usage/#3-feature-ranking","title":"3. Feature ranking","text":"<p>Please checkout the documentations at Modules &gt; DEM &gt; Feature ranking &gt; <code>DEMFeatureRanking</code>.</p> <p>This is an example of running the module:</p> run_dem_rank.py<pre><code>import os\nimport sys\nfrom biodem import DEMFeatureRanking\n\n\nif len(sys.argv) &lt; 3:\n    raise ValueError('Please input trait name and outer index')\ntrait_name = sys.argv[1]\nwhich_outer = int(sys.argv[2])\n\nwork_dir_home = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run_dem\", trait_name)\nlitdata_dir = os.path.join(work_dir_home, 'litdata')\nlog_dir = os.path.join(work_dir_home, 'models')\n\nrank_result_path = os.path.join(work_dir_home, \"feature_rank\", f\"rank_result_{trait_name}_outer+{which_outer}.csv\")\nrandom_seeds = [1000+i for i in range(20)]\n\n\nif __name__ == \"__main__\":\n    _feat_rank = DEMFeatureRanking()\n    _feat_rank.run_a_outer(\n        ncv_litdata_dir = litdata_dir,\n        fit_log_dir = log_dir,\n        which_outer = which_outer,\n        output_path = rank_result_path,\n        random_states = random_seeds,\n    )\n\n    # Collect ranks if several outer testset ranking results already exist\n    # _feat_rank.collect_ranks(os.path.dirname(rank_result_path), os.path.join(os.path.dirname(rank_result_path), \"rank_merged_sorted.csv\"))\n</code></pre>"},{"location":"reference/biodem.s2g.model/","title":"biodem.s2g.model","text":""},{"location":"reference/biodem.s2g.model/#biodem.s2g.model","title":"<code>biodem.s2g.model</code>","text":"<p>This code aims to reduce the dimensionality of SNPs, assumming that SNPs are located in genome regions. The SNP-genome block relation is pre-defined. The input is the one-hot SNPs, and the first layer of the network is a sparse linear layer that maps the SNPs to a low-dimensional space with features representing genome regions. The following layers are dense layers, that could be trained to predict phenotypes based on the low-dimensional features.</p>"},{"location":"reference/biodem.s2g.model/#biodem.s2g.model.SNP2GB","title":"<code>SNP2GB</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>src\\biodem\\s2g\\model.py</code> <pre><code>class SNP2GB(ltn.LightningModule):\n    def __init__(\n            self,\n            path_pretrained_model: str,\n            blocks_gt: List[List[int]],\n            snp_onehot_bits: int,\n            map_location: Optional[str] = None,\n        ):\n        r\"\"\"Transform SNPs to genome blocks using a pre-trained model.\n        \"\"\"\n        super().__init__()\n        self.n_blocks = len(blocks_gt)\n\n        self.indices_gt = [idx_convert(block, snp_onehot_bits) for block in blocks_gt]\n\n        # Load the pre-trained model\n        pretrained_model = SNPReductionNet.load_from_checkpoint(\n            checkpoint_path = path_pretrained_model,\n            map_location = get_map_location(map_location),\n        )\n        pretrained_model.eval()\n        pretrained_model.freeze()\n\n        # Extract the sparse layer\n        self.sparse_layers = pretrained_model.model.sparse_layers\n\n        # Freeze the sparse layer\n        self.sparse_layers.requires_grad_(False)\n\n    def forward(self, x):\n        # Map SNPs to genome blocks\n        g_features = [layer(x[:, indices]) for layer, indices in zip(self.sparse_layers, self.indices_gt)]\n        gblocks = torch.cat(g_features, dim=1)\n        return gblocks\n\n    def predict_step(self, batch, batch_idx) -&gt; torch.Tensor:\n        return self.forward(batch[const.dkey.litdata_omics][0])\n</code></pre>"},{"location":"reference/biodem.s2g.model/#biodem.s2g.model.SNP2GB.__init__","title":"<code>__init__(path_pretrained_model, blocks_gt, snp_onehot_bits, map_location=None)</code>","text":"<p>Transform SNPs to genome blocks using a pre-trained model.</p> Source code in <code>src\\biodem\\s2g\\model.py</code> <pre><code>def __init__(\n        self,\n        path_pretrained_model: str,\n        blocks_gt: List[List[int]],\n        snp_onehot_bits: int,\n        map_location: Optional[str] = None,\n    ):\n    r\"\"\"Transform SNPs to genome blocks using a pre-trained model.\n    \"\"\"\n    super().__init__()\n    self.n_blocks = len(blocks_gt)\n\n    self.indices_gt = [idx_convert(block, snp_onehot_bits) for block in blocks_gt]\n\n    # Load the pre-trained model\n    pretrained_model = SNPReductionNet.load_from_checkpoint(\n        checkpoint_path = path_pretrained_model,\n        map_location = get_map_location(map_location),\n    )\n    pretrained_model.eval()\n    pretrained_model.freeze()\n\n    # Extract the sparse layer\n    self.sparse_layers = pretrained_model.model.sparse_layers\n\n    # Freeze the sparse layer\n    self.sparse_layers.requires_grad_(False)\n</code></pre>"},{"location":"reference/biodem.s2g.model/#biodem.s2g.model.SNPReductionNet","title":"<code>SNPReductionNet</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>src\\biodem\\s2g\\model.py</code> <pre><code>class SNPReductionNet(ltn.LightningModule):\n    def __init__(\n            self,\n            output_dim: int,\n            blocks_gt: List[List[int]],\n            snp_onehot_bits: int,\n            dense_layer_dims: List[int],\n            learning_rate: float,\n            regression: bool,\n        ):\n        r\"\"\"A PyTorch Lightning module for SNP reduction and phenotype prediction.\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self.output_dim = output_dim\n        self.learning_rate = learning_rate\n        self.regression = regression\n\n        self._define_metrics()\n\n        self.model = SNPReductionNetModel(\n            output_dim=output_dim,\n            blocks_gt=blocks_gt,\n            snp_onehot_bits=snp_onehot_bits,\n            dense_layer_dims=dense_layer_dims,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x = batch[const.dkey.litdata_omics][0]\n        y = batch[const.dkey.litdata_label]\n\n        y_pred = self.forward(x)\n        loss = self._loss(y_pred, y, const.title_train)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x = batch[const.dkey.litdata_omics][0]\n        y = batch[const.dkey.litdata_label]\n\n        y_pred = self.forward(x)\n        loss = self._loss(y_pred, y, const.title_val)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x = batch[const.dkey.litdata_omics][0]\n        y = batch[const.dkey.litdata_label]\n\n        y_pred = self.forward(x)\n        loss = self._loss(y_pred, y, const.title_test)\n        return loss\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        x = batch[const.dkey.litdata_omics][0]\n        y_pred = self.forward(x)\n        return y_pred\n\n    def configure_optimizers(self):\n        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n\n    def _define_metrics(self):\n        \"\"\"\n        Define the loss function and the metrics.\n        \"\"\"\n        if self.output_dim == 1:\n            self.loss_fn = nn.MSELoss()\n            self.mae = MeanAbsoluteError()\n            self.r2 = R2Score()\n            self.pcc = PearsonCorrCoef()\n        else:\n            if self.regression:\n                # Multi-label regression\n                self.loss_fn = nn.MSELoss(reduction='none')\n                # self.mae = MeanAbsoluteError()\n                # self.r2 = R2Score()\n                # self.pcc = PearsonCorrCoef()\n            else:\n                self.loss_fn = nn.CrossEntropyLoss()\n                self.recall_micro = MulticlassRecall(average=\"micro\", num_classes=self.output_dim)\n                self.recall_macro = MulticlassRecall(average=\"macro\", num_classes=self.output_dim)\n                self.recall_weighted = MulticlassRecall(average=\"weighted\", num_classes=self.output_dim)\n                #\n                self.precision_micro = MulticlassPrecision(average=\"micro\", num_classes=self.output_dim)\n                self.precision_macro = MulticlassPrecision(average=\"macro\", num_classes=self.output_dim)\n                self.precision_weighted = MulticlassPrecision(average=\"weighted\", num_classes=self.output_dim)\n                #\n                self.f1_micro = MulticlassF1Score(average=\"micro\", num_classes=self.output_dim)\n                self.f1_macro = MulticlassF1Score(average=\"macro\", num_classes=self.output_dim)\n                self.f1_weighted = MulticlassF1Score(average=\"weighted\", num_classes=self.output_dim)\n                #\n                self.accuracy_micro = MulticlassAccuracy(average=\"micro\", num_classes=self.output_dim)\n                self.accuracy_macro = MulticlassAccuracy(average=\"macro\", num_classes=self.output_dim)\n                self.accuracy_weighted = MulticlassAccuracy(average=\"weighted\", num_classes=self.output_dim)\n                #\n                self.auroc_macro = MulticlassAUROC(average=\"macro\", num_classes=self.output_dim)\n                self.auroc_weighted = MulticlassAUROC(average=\"weighted\", num_classes=self.output_dim)\n\n    def _loss(self, y_pred: torch.Tensor, y: torch.Tensor, which_step: str):\n        #!!!!!!!!!!!!!!!!!!!!!!!!!\n        if self.output_dim &gt; 1 and not self.regression:\n            y = y.argmax(dim=-1)\n\n        if self.output_dim == 1:\n            loss = self.loss_fn(y_pred, y)\n            self.log(f\"{which_step}_loss\", loss, sync_dist=True)\n            self.log(f\"{which_step}_mae\", self.mae(y_pred, y), sync_dist=True)\n            if y.shape[0] &lt; 2:\n                return loss\n            self.log(f\"{which_step}_pcc\", self.pcc(y_pred, y), sync_dist=True)\n            self.log(f\"{which_step}_r2\", self.r2(y_pred, y), sync_dist=True)\n        else:\n            if self.regression:\n                loss = self.loss_fn(y_pred, y)\n                loss = loss.mean(dim=0)\n                loss = loss.sum()\n                self.log(f\"{which_step}_loss\", loss, sync_dist=True)\n                # self.log(f\"{which_step}_mae\", self.mae(y_pred, y), sync_dist=True)\n                # if y.shape[0] &lt; 2:\n                #     return loss\n                # self.log(f\"{which_step}_pcc\", self.pcc(y_pred, y), sync_dist=True)\n                # self.log(f\"{which_step}_r2\", self.r2(y_pred, y), sync_dist=True)\n            else:\n                loss = self.loss_fn(y_pred, y)\n                self.log(f\"{which_step}_loss\", loss, sync_dist=True)\n\n                self.log(f\"{which_step}_f1_micro\", self.f1_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_f1_macro\", self.f1_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_f1_weighted\", self.f1_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_recall_micro\", self.recall_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_recall_macro\", self.recall_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_recall_weighted\", self.recall_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_precision_micro\", self.precision_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_precision_macro\", self.precision_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_precision_weighted\", self.precision_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_accuracy_micro\", self.accuracy_micro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_accuracy_macro\", self.accuracy_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_accuracy_weighted\", self.accuracy_weighted(y_pred, y), sync_dist=True)\n                #\n                self.log(f\"{which_step}_auroc_macro\", self.auroc_macro(y_pred, y), sync_dist=True)\n                self.log(f\"{which_step}_auroc_weighted\", self.auroc_weighted(y_pred, y), sync_dist=True)\n\n        return loss\n</code></pre>"},{"location":"reference/biodem.s2g.model/#biodem.s2g.model.SNPReductionNet.__init__","title":"<code>__init__(output_dim, blocks_gt, snp_onehot_bits, dense_layer_dims, learning_rate, regression)</code>","text":"<p>A PyTorch Lightning module for SNP reduction and phenotype prediction.</p> Source code in <code>src\\biodem\\s2g\\model.py</code> <pre><code>def __init__(\n        self,\n        output_dim: int,\n        blocks_gt: List[List[int]],\n        snp_onehot_bits: int,\n        dense_layer_dims: List[int],\n        learning_rate: float,\n        regression: bool,\n    ):\n    r\"\"\"A PyTorch Lightning module for SNP reduction and phenotype prediction.\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters()\n    self.output_dim = output_dim\n    self.learning_rate = learning_rate\n    self.regression = regression\n\n    self._define_metrics()\n\n    self.model = SNPReductionNetModel(\n        output_dim=output_dim,\n        blocks_gt=blocks_gt,\n        snp_onehot_bits=snp_onehot_bits,\n        dense_layer_dims=dense_layer_dims,\n    )\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/","title":"biodem.s2g.pipeline","text":""},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline","title":"<code>biodem.s2g.pipeline</code>","text":"<p>The pipeline for converting SNPs to genome blocks representations.</p>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFit","title":"<code>SNP2GBFit</code>","text":"Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>class SNP2GBFit:\n    def __init__(\n            self,\n            log_dir: str,\n            log_name: str,\n            litdata_dir: str,\n            which_outer_testset: int,\n            which_inner_valset: int,\n            regression: bool,\n            dense_layer_dims: List[int],\n            snp_onehot_bits: int = const.default.snp_onehot_bits,\n            devices: Union[List[int], str, int] = const.default.devices,\n            accelerator: str = const.default.accelerator,\n            n_jobs: int = const.default.n_jobs,\n            learning_rate: float = const.default.lr,\n            patience: int = const.default.patience,\n            max_epochs: int = const.default.max_epochs,\n            min_epochs: int = const.default.min_epochs,\n            batch_size: int = const.default.batch_size,\n        ):\n        r\"\"\"SNP-to-genome-block model training with hyperparameter optimization.\n        \"\"\"\n        self.log_dir = log_dir\n        self.log_name = log_name\n        self.regression = regression\n        self.devices = devices\n        self.accelerator = accelerator\n        self.n_jobs = n_jobs\n        self.snp_onehot_bits = snp_onehot_bits\n        self.blocks_gt = read_pkl_gv(os.path.join(litdata_dir, const.fname.genotypes))[const.dkey.gblock2gtype]\n        self.model_out_dim = pl.read_csv(os.path.join(litdata_dir, const.fname.output_dim), has_header=True)[0,0]\n\n        self.datamodule = DEMDataModule4Train(litdata_dir, which_outer_testset, which_inner_valset, batch_size, n_jobs)\n        self.datamodule.setup()\n\n        self.hparams = self.hparams_fit(\n            learning_rate=learning_rate,\n            patience=patience,\n            max_epochs=max_epochs,\n            min_epochs=min_epochs,\n            batch_size=batch_size,\n            dense_layer_dims=dense_layer_dims,\n        )\n\n    def hparams_fit(\n            self,\n            learning_rate: float,\n            patience: int,\n            max_epochs: int,\n            min_epochs: int,\n            batch_size: int,\n            dense_layer_dims: List[int],\n        ) -&gt; Dict[str, Any]:\n        r\"\"\"Generate a dictionary of hyperparameters for SNP2GB model training.\n        \"\"\"\n        hparams = {\n            const.dkey.lr: learning_rate,\n            const.dkey.patience: patience,\n            const.dkey.max_epochs: max_epochs,\n            const.dkey.min_epochs: min_epochs,\n            const.dkey.bsize: batch_size,\n            const.dkey.s2g_dense_layer_dims: dense_layer_dims,\n        }\n        return hparams\n\n    def snp2gb_fit(\n            self,\n            hparams: Dict[str, Any],\n            devices: Union[List[int], str, int],\n            accelerator: str,\n        ):\n        r\"\"\"Train SNP2GB model with given hyperparameters.\n        \"\"\"\n        _model = SNPReductionNet(\n            output_dim=self.model_out_dim,\n            blocks_gt=self.blocks_gt,\n            snp_onehot_bits=self.snp_onehot_bits,\n            dense_layer_dims=hparams[const.dkey.s2g_dense_layer_dims],\n            learning_rate=hparams[const.dkey.lr],\n            regression=self.regression,\n        )\n        # Try compiling\n        _model.compile()\n\n        # Unique tag for the experiment\n        log_dir_uniq_model = os.path.join(self.log_dir, self.log_name, random_string())\n\n        val_loss_min = train_model(\n            model=_model,\n            datamodule=self.datamodule,\n            es_patience=hparams[const.dkey.patience],\n            max_epochs=hparams[const.dkey.max_epochs],\n            min_epochs=hparams[const.dkey.min_epochs],\n            log_dir=log_dir_uniq_model,\n            devices=devices,\n            accelerator=accelerator,\n        )\n        if val_loss_min is None:\n            raise ValueError('\\nTraining failed.\\n')\n        return val_loss_min\n\n    def manual_fit(self):\n        r\"\"\"Train SNP2GB model with manually set hyperparameters.\n        \"\"\"\n        val_loss_min = self.snp2gb_fit(\n            hparams=self.hparams,\n            devices=self.devices,\n            accelerator=self.accelerator,\n        )\n        return val_loss_min\n\n    def objective(self, trial: optuna.Trial) -&gt; float:\n        r\"\"\"Objective function for SNP2GB model hyperparameter optimization.\n        \"\"\"\n        print('Trial number:', trial.number)\n        if self.n_jobs &gt; 1:\n            time_delay = (trial.number + self.n_jobs) % self.n_jobs * const.default.time_delay\n            time.sleep(time_delay)\n\n        lr = trial.suggest_categorical(const.dkey.lr, const.hparam_candidates.lr)\n        batch_size = trial.suggest_categorical(const.dkey.bsize, const.hparam_candidates.batch_size)\n\n        hparams_trial = self.hparams.copy()\n        hparams_trial[const.dkey.lr] = lr\n        hparams_trial[const.dkey.bsize] = batch_size\n\n        val_loss_min = self.snp2gb_fit(\n            hparams = hparams_trial,\n            devices = self.devices,\n            accelerator=self.accelerator,\n        )\n\n        return val_loss_min\n\n    def optimize(\n            self,\n            n_trials: Optional[int] = const.default.n_trials,\n            storage: str = const.default.optuna_db,\n            gc_after_trial: bool = True,\n        ):\n        r\"\"\"Hyperparameters optimization for SNP2GB model.\n        \"\"\"\n        study = optuna.create_study(\n            storage = storage,\n            study_name = self.log_name + '_' + time_string(),\n            load_if_exists = True,\n            direction = 'minimize',\n        )\n        study.optimize(self.objective, n_jobs=self.n_jobs, n_trials=n_trials, gc_after_trial=gc_after_trial)\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFit.__init__","title":"<code>__init__(log_dir, log_name, litdata_dir, which_outer_testset, which_inner_valset, regression, dense_layer_dims, snp_onehot_bits=const.default.snp_onehot_bits, devices=const.default.devices, accelerator=const.default.accelerator, n_jobs=const.default.n_jobs, learning_rate=const.default.lr, patience=const.default.patience, max_epochs=const.default.max_epochs, min_epochs=const.default.min_epochs, batch_size=const.default.batch_size)</code>","text":"<p>SNP-to-genome-block model training with hyperparameter optimization.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def __init__(\n        self,\n        log_dir: str,\n        log_name: str,\n        litdata_dir: str,\n        which_outer_testset: int,\n        which_inner_valset: int,\n        regression: bool,\n        dense_layer_dims: List[int],\n        snp_onehot_bits: int = const.default.snp_onehot_bits,\n        devices: Union[List[int], str, int] = const.default.devices,\n        accelerator: str = const.default.accelerator,\n        n_jobs: int = const.default.n_jobs,\n        learning_rate: float = const.default.lr,\n        patience: int = const.default.patience,\n        max_epochs: int = const.default.max_epochs,\n        min_epochs: int = const.default.min_epochs,\n        batch_size: int = const.default.batch_size,\n    ):\n    r\"\"\"SNP-to-genome-block model training with hyperparameter optimization.\n    \"\"\"\n    self.log_dir = log_dir\n    self.log_name = log_name\n    self.regression = regression\n    self.devices = devices\n    self.accelerator = accelerator\n    self.n_jobs = n_jobs\n    self.snp_onehot_bits = snp_onehot_bits\n    self.blocks_gt = read_pkl_gv(os.path.join(litdata_dir, const.fname.genotypes))[const.dkey.gblock2gtype]\n    self.model_out_dim = pl.read_csv(os.path.join(litdata_dir, const.fname.output_dim), has_header=True)[0,0]\n\n    self.datamodule = DEMDataModule4Train(litdata_dir, which_outer_testset, which_inner_valset, batch_size, n_jobs)\n    self.datamodule.setup()\n\n    self.hparams = self.hparams_fit(\n        learning_rate=learning_rate,\n        patience=patience,\n        max_epochs=max_epochs,\n        min_epochs=min_epochs,\n        batch_size=batch_size,\n        dense_layer_dims=dense_layer_dims,\n    )\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFit.hparams_fit","title":"<code>hparams_fit(learning_rate, patience, max_epochs, min_epochs, batch_size, dense_layer_dims)</code>","text":"<p>Generate a dictionary of hyperparameters for SNP2GB model training.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def hparams_fit(\n        self,\n        learning_rate: float,\n        patience: int,\n        max_epochs: int,\n        min_epochs: int,\n        batch_size: int,\n        dense_layer_dims: List[int],\n    ) -&gt; Dict[str, Any]:\n    r\"\"\"Generate a dictionary of hyperparameters for SNP2GB model training.\n    \"\"\"\n    hparams = {\n        const.dkey.lr: learning_rate,\n        const.dkey.patience: patience,\n        const.dkey.max_epochs: max_epochs,\n        const.dkey.min_epochs: min_epochs,\n        const.dkey.bsize: batch_size,\n        const.dkey.s2g_dense_layer_dims: dense_layer_dims,\n    }\n    return hparams\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFit.manual_fit","title":"<code>manual_fit()</code>","text":"<p>Train SNP2GB model with manually set hyperparameters.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def manual_fit(self):\n    r\"\"\"Train SNP2GB model with manually set hyperparameters.\n    \"\"\"\n    val_loss_min = self.snp2gb_fit(\n        hparams=self.hparams,\n        devices=self.devices,\n        accelerator=self.accelerator,\n    )\n    return val_loss_min\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFit.objective","title":"<code>objective(trial)</code>","text":"<p>Objective function for SNP2GB model hyperparameter optimization.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def objective(self, trial: optuna.Trial) -&gt; float:\n    r\"\"\"Objective function for SNP2GB model hyperparameter optimization.\n    \"\"\"\n    print('Trial number:', trial.number)\n    if self.n_jobs &gt; 1:\n        time_delay = (trial.number + self.n_jobs) % self.n_jobs * const.default.time_delay\n        time.sleep(time_delay)\n\n    lr = trial.suggest_categorical(const.dkey.lr, const.hparam_candidates.lr)\n    batch_size = trial.suggest_categorical(const.dkey.bsize, const.hparam_candidates.batch_size)\n\n    hparams_trial = self.hparams.copy()\n    hparams_trial[const.dkey.lr] = lr\n    hparams_trial[const.dkey.bsize] = batch_size\n\n    val_loss_min = self.snp2gb_fit(\n        hparams = hparams_trial,\n        devices = self.devices,\n        accelerator=self.accelerator,\n    )\n\n    return val_loss_min\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFit.optimize","title":"<code>optimize(n_trials=const.default.n_trials, storage=const.default.optuna_db, gc_after_trial=True)</code>","text":"<p>Hyperparameters optimization for SNP2GB model.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def optimize(\n        self,\n        n_trials: Optional[int] = const.default.n_trials,\n        storage: str = const.default.optuna_db,\n        gc_after_trial: bool = True,\n    ):\n    r\"\"\"Hyperparameters optimization for SNP2GB model.\n    \"\"\"\n    study = optuna.create_study(\n        storage = storage,\n        study_name = self.log_name + '_' + time_string(),\n        load_if_exists = True,\n        direction = 'minimize',\n    )\n    study.optimize(self.objective, n_jobs=self.n_jobs, n_trials=n_trials, gc_after_trial=gc_after_trial)\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFit.snp2gb_fit","title":"<code>snp2gb_fit(hparams, devices, accelerator)</code>","text":"<p>Train SNP2GB model with given hyperparameters.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def snp2gb_fit(\n        self,\n        hparams: Dict[str, Any],\n        devices: Union[List[int], str, int],\n        accelerator: str,\n    ):\n    r\"\"\"Train SNP2GB model with given hyperparameters.\n    \"\"\"\n    _model = SNPReductionNet(\n        output_dim=self.model_out_dim,\n        blocks_gt=self.blocks_gt,\n        snp_onehot_bits=self.snp_onehot_bits,\n        dense_layer_dims=hparams[const.dkey.s2g_dense_layer_dims],\n        learning_rate=hparams[const.dkey.lr],\n        regression=self.regression,\n    )\n    # Try compiling\n    _model.compile()\n\n    # Unique tag for the experiment\n    log_dir_uniq_model = os.path.join(self.log_dir, self.log_name, random_string())\n\n    val_loss_min = train_model(\n        model=_model,\n        datamodule=self.datamodule,\n        es_patience=hparams[const.dkey.patience],\n        max_epochs=hparams[const.dkey.max_epochs],\n        min_epochs=hparams[const.dkey.min_epochs],\n        log_dir=log_dir_uniq_model,\n        devices=devices,\n        accelerator=accelerator,\n    )\n    if val_loss_min is None:\n        raise ValueError('\\nTraining failed.\\n')\n    return val_loss_min\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFitPipe","title":"<code>SNP2GBFitPipe</code>","text":"Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>class SNP2GBFitPipe:\n    def __init__(\n            self,\n            litdata_dir: str,\n            list_ncv: List[List[int]],\n            log_dir: str,\n            regression: bool,\n            devices: Union[List[int], str, int] = const.default.devices,\n            accelerator: str = const.default.accelerator,\n            n_jobs: int = const.default.n_jobs,\n            n_trials: Optional[int] = const.default.n_trials,\n            dense_layer_dims: Optional[List[int]] = None,\n            snp_onehot_bits: int = const.default.snp_onehot_bits,\n        ):\n        r\"\"\"SNP2GB model training pipeline.\n\n        Hyperparameters are optimized for each fold in nested cross-validation.\n\n        The best model for each fold is used to convert SNPs to genome blocks.\n\n        Args:\n            litdata_dir: Path to the directory containing the nested cross-validation data.\n\n            list_ncv: List of nested cross-validation folds. e.g., ``[[0,0], [0,1], [9,4]]``.\n\n            log_dir: Path to the directory for saving the training logs and models' checkpoints.\n\n            regression: Whether the task is regression or classification.\n\n            devices: Devices for training.\n\n            accelerator: Accelerator for training.\n\n            n_jobs: Number of jobs for parallel hyperparameter optimization.\n\n            n_trials: Number of trials for hyperparameter optimization.\n\n            dense_layer_dims: List of hidden dimensions for the dense layers.\n\n            snp_onehot_bits: Length of the one-hot representation for SNPs.\n\n        \"\"\"\n        # Unique tag for the training log directory\n        tag_str = time_string() + '_' + random_string()\n        self.uniq_logdir = os.path.join(log_dir, const.title_train + '_' + tag_str)\n        os.makedirs(self.uniq_logdir, exist_ok=False)\n\n        self.litdata_dir = litdata_dir\n        self.list_ncv = list_ncv\n        self.n_slice = len(list_ncv)\n\n        if dense_layer_dims is None:\n            self.dense_layer_dims = const.hparam_candidates.s2g_dense_layer_dims[0]\n        else:\n            self.dense_layer_dims = dense_layer_dims\n\n        self.regression = regression\n        self.snp_onehot_bits = snp_onehot_bits\n        self.devices = devices\n        self.accelerator = accelerator\n        self.n_jobs = n_jobs\n        self.n_trials = n_trials\n\n    def train_pipeline(self):\n        r\"\"\"Train SNP2GB model for each fold in nested cross-validation.\n        \"\"\"\n        # Storage for optuna trials in self.log_dir\n        path_storage = 'sqlite:///' + self.uniq_logdir + '/optuna_s2g' + '.db'\n\n        print(f\"\\nNumber of data slices to train: {self.n_slice}\\n\")\n        log_names = []\n        for i in range(self.n_slice):\n            log_names.append(f'run_ncv_{self.list_ncv[i][0]}_{self.list_ncv[i][1]}')\n\n        # Train SNP2GB model for each fold in nested cross-validation\n        if self.n_slice == 1:\n            snp2gb_train_x = SNP2GBFit(\n                log_dir = self.uniq_logdir,\n                log_name = log_names[0],\n                litdata_dir = self.litdata_dir,\n                which_outer_testset = self.list_ncv[0][0],\n                which_inner_valset = self.list_ncv[0][1],\n                regression = self.regression,\n                dense_layer_dims = self.dense_layer_dims,\n                snp_onehot_bits = self.snp_onehot_bits,\n                devices = self.devices,\n                accelerator=self.accelerator,\n                n_jobs=self.n_jobs,\n            )\n            snp2gb_train_x.optimize(n_trials=self.n_trials, storage=path_storage)\n        else:\n            for xfold in range(self.n_slice):\n                snp2gb_train_x = SNP2GBFit(\n                    log_dir = self.uniq_logdir,\n                    log_name = log_names[xfold],\n                    litdata_dir = self.litdata_dir,\n                    which_outer_testset = self.list_ncv[xfold][0],\n                    which_inner_valset = self.list_ncv[xfold][1],\n                    regression = self.regression,\n                    dense_layer_dims = self.dense_layer_dims,\n                    snp_onehot_bits = self.snp_onehot_bits,\n                    devices = self.devices,\n                    accelerator=self.accelerator,\n                    n_jobs=self.n_jobs,\n                )\n                snp2gb_train_x.optimize(n_trials=self.n_trials, storage=path_storage)\n\n        # Remove checkpoints of inferior models\n        _collector = CollectFitLog(self.uniq_logdir)\n        _collector.remove_inferior_models()\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFitPipe.__init__","title":"<code>__init__(litdata_dir, list_ncv, log_dir, regression, devices=const.default.devices, accelerator=const.default.accelerator, n_jobs=const.default.n_jobs, n_trials=const.default.n_trials, dense_layer_dims=None, snp_onehot_bits=const.default.snp_onehot_bits)</code>","text":"<p>SNP2GB model training pipeline.</p> <p>Hyperparameters are optimized for each fold in nested cross-validation.</p> <p>The best model for each fold is used to convert SNPs to genome blocks.</p> <p>Parameters:</p> Name Type Description Default <code>litdata_dir</code> <code>str</code> <p>Path to the directory containing the nested cross-validation data.</p> required <code>list_ncv</code> <code>List[List[int]]</code> <p>List of nested cross-validation folds. e.g., <code>[[0,0], [0,1], [9,4]]</code>.</p> required <code>log_dir</code> <code>str</code> <p>Path to the directory for saving the training logs and models' checkpoints.</p> required <code>regression</code> <code>bool</code> <p>Whether the task is regression or classification.</p> required <code>devices</code> <code>Union[List[int], str, int]</code> <p>Devices for training.</p> <code>devices</code> <code>accelerator</code> <code>str</code> <p>Accelerator for training.</p> <code>accelerator</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs for parallel hyperparameter optimization.</p> <code>n_jobs</code> <code>n_trials</code> <code>Optional[int]</code> <p>Number of trials for hyperparameter optimization.</p> <code>n_trials</code> <code>dense_layer_dims</code> <code>Optional[List[int]]</code> <p>List of hidden dimensions for the dense layers.</p> <code>None</code> <code>snp_onehot_bits</code> <code>int</code> <p>Length of the one-hot representation for SNPs.</p> <code>snp_onehot_bits</code> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def __init__(\n        self,\n        litdata_dir: str,\n        list_ncv: List[List[int]],\n        log_dir: str,\n        regression: bool,\n        devices: Union[List[int], str, int] = const.default.devices,\n        accelerator: str = const.default.accelerator,\n        n_jobs: int = const.default.n_jobs,\n        n_trials: Optional[int] = const.default.n_trials,\n        dense_layer_dims: Optional[List[int]] = None,\n        snp_onehot_bits: int = const.default.snp_onehot_bits,\n    ):\n    r\"\"\"SNP2GB model training pipeline.\n\n    Hyperparameters are optimized for each fold in nested cross-validation.\n\n    The best model for each fold is used to convert SNPs to genome blocks.\n\n    Args:\n        litdata_dir: Path to the directory containing the nested cross-validation data.\n\n        list_ncv: List of nested cross-validation folds. e.g., ``[[0,0], [0,1], [9,4]]``.\n\n        log_dir: Path to the directory for saving the training logs and models' checkpoints.\n\n        regression: Whether the task is regression or classification.\n\n        devices: Devices for training.\n\n        accelerator: Accelerator for training.\n\n        n_jobs: Number of jobs for parallel hyperparameter optimization.\n\n        n_trials: Number of trials for hyperparameter optimization.\n\n        dense_layer_dims: List of hidden dimensions for the dense layers.\n\n        snp_onehot_bits: Length of the one-hot representation for SNPs.\n\n    \"\"\"\n    # Unique tag for the training log directory\n    tag_str = time_string() + '_' + random_string()\n    self.uniq_logdir = os.path.join(log_dir, const.title_train + '_' + tag_str)\n    os.makedirs(self.uniq_logdir, exist_ok=False)\n\n    self.litdata_dir = litdata_dir\n    self.list_ncv = list_ncv\n    self.n_slice = len(list_ncv)\n\n    if dense_layer_dims is None:\n        self.dense_layer_dims = const.hparam_candidates.s2g_dense_layer_dims[0]\n    else:\n        self.dense_layer_dims = dense_layer_dims\n\n    self.regression = regression\n    self.snp_onehot_bits = snp_onehot_bits\n    self.devices = devices\n    self.accelerator = accelerator\n    self.n_jobs = n_jobs\n    self.n_trials = n_trials\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBFitPipe.train_pipeline","title":"<code>train_pipeline()</code>","text":"<p>Train SNP2GB model for each fold in nested cross-validation.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def train_pipeline(self):\n    r\"\"\"Train SNP2GB model for each fold in nested cross-validation.\n    \"\"\"\n    # Storage for optuna trials in self.log_dir\n    path_storage = 'sqlite:///' + self.uniq_logdir + '/optuna_s2g' + '.db'\n\n    print(f\"\\nNumber of data slices to train: {self.n_slice}\\n\")\n    log_names = []\n    for i in range(self.n_slice):\n        log_names.append(f'run_ncv_{self.list_ncv[i][0]}_{self.list_ncv[i][1]}')\n\n    # Train SNP2GB model for each fold in nested cross-validation\n    if self.n_slice == 1:\n        snp2gb_train_x = SNP2GBFit(\n            log_dir = self.uniq_logdir,\n            log_name = log_names[0],\n            litdata_dir = self.litdata_dir,\n            which_outer_testset = self.list_ncv[0][0],\n            which_inner_valset = self.list_ncv[0][1],\n            regression = self.regression,\n            dense_layer_dims = self.dense_layer_dims,\n            snp_onehot_bits = self.snp_onehot_bits,\n            devices = self.devices,\n            accelerator=self.accelerator,\n            n_jobs=self.n_jobs,\n        )\n        snp2gb_train_x.optimize(n_trials=self.n_trials, storage=path_storage)\n    else:\n        for xfold in range(self.n_slice):\n            snp2gb_train_x = SNP2GBFit(\n                log_dir = self.uniq_logdir,\n                log_name = log_names[xfold],\n                litdata_dir = self.litdata_dir,\n                which_outer_testset = self.list_ncv[xfold][0],\n                which_inner_valset = self.list_ncv[xfold][1],\n                regression = self.regression,\n                dense_layer_dims = self.dense_layer_dims,\n                snp_onehot_bits = self.snp_onehot_bits,\n                devices = self.devices,\n                accelerator=self.accelerator,\n                n_jobs=self.n_jobs,\n            )\n            snp2gb_train_x.optimize(n_trials=self.n_trials, storage=path_storage)\n\n    # Remove checkpoints of inferior models\n    _collector = CollectFitLog(self.uniq_logdir)\n    _collector.remove_inferior_models()\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBTransPipe","title":"<code>SNP2GBTransPipe</code>","text":"Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>class SNP2GBTransPipe:\n    def __init__(\n            self,\n            dir_log: str,\n            dir_output: str,\n            overwrite_collected_log: bool = False,\n        ):\n        r\"\"\"The pipeline to transform SNP features to genome block features.\n\n        1. Collect trained models for each fold in nested cross-validation.\n        2. Transform SNP features to genome block features.\n\n        Args:\n            dir_log: The log directory of the SNP2GB models.\n\n            dir_output: The output directory.\n\n            overwrite_collected_log: Whether to overwrite existing collected log.\n                Default: ``False``.\n\n        \"\"\"\n        self.dir_log = dir_log\n        self.dir_output = dir_output\n        os.makedirs(self.dir_output, exist_ok=True)\n        self.overwrite_collected_log = overwrite_collected_log\n\n    def collect_models(self):\n        r\"\"\"Collect fitted models for each fold in nested cross-validation.\n        \"\"\"\n        collector = CollectFitLog(self.dir_log)\n        models_bv, models_bi = collector.get_df_csv(self.dir_output, self.overwrite_collected_log)\n        self.models_bv = models_bv\n        self.models_bi = models_bi\n\n    def convert_snp(\n            self,\n            dir_litdata: str,\n            list_ncv: Optional[List[List[int]]] = None,\n            snp_onehot_bits: int = const.default.snp_onehot_bits,\n            accelerator: str = const.default.accelerator,\n            batch_size: int = const.default.batch_size,\n            n_workers: int = const.default.n_workers,\n        ):\n        r\"\"\"Convert SNPs to genome blocks features using the best model for each fold in nested cross-validation.\n\n        Args:\n            dir_litdata: The directory containing nested cross-validation data for S2G.\n\n            list_ncv: The list of outer-inner folds to use.\n                Default: ``None``.\n                If it is not ``None``, it should be a list of lists of two integers,\n                where the first integer is the outer fold index and the second integer is the inner fold index.\n                If it is ``None``, the best model overall is used.\n\n            snp_onehot_bits: The number of bits for one-hot representation of SNPs.\n                Default: ``10``.\n\n            accelerator: The accelerator to use.\n                Default: ``\"auto\"``.\n\n            batch_size: The batch size to use.\n                Default: ``32``.\n\n            n_workers: The number of workers to use for dataloader.\n                Default: ``1``.\n\n        \"\"\"\n        if not hasattr(self,'models_bv'):\n            self.collect_models()\n\n        path_gtype_pkl = os.path.join(dir_litdata, const.fname.genotypes)\n\n        if list_ncv is None:\n            # Take the best model's path overall by searching the line min `val_loss` in models_bi.\n            path_best_model = self.models_bi.filter(pl.col(const.title_val_loss) == self.models_bi.select(const.title_val_loss).min()).select(const.dkey.ckpt_path)[0,0]\n            output = execute_s2g(dir_litdata, path_gtype_pkl, path_best_model, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n            output.write_parquet(os.path.join(self.dir_output, const.fname.transformed_genotypes))\n\n            return None\n\n        # For each inner fold\n        for data_xx in list_ncv:\n            x_outer, x_inner = data_xx\n            path_o_pred_trn = os.path.join(self.dir_output, const.fname.transformed_genotypes.removesuffix(\".parquet\") + f\"_{x_outer}_{x_inner}_trn.parquet\")\n            path_o_pred_val = os.path.join(self.dir_output, const.fname.transformed_genotypes.removesuffix(\".parquet\") + f\"_{x_outer}_{x_inner}_val.parquet\")\n            path_o_pred_tst = os.path.join(self.dir_output, const.fname.transformed_genotypes.removesuffix(\".parquet\") + f\"_{x_outer}_{x_inner}_tst.parquet\")\n\n            path_mdl = self.models_bv.filter((pl.col(const.dkey.which_outer) == x_outer) &amp; (pl.col(const.dkey.which_inner) == x_inner)).select(const.dkey.ckpt_path)[0,0]\n            print(f'\\nUsing model {path_mdl}\\n')\n\n            ncv_data = DEMDataModule4Train(dir_litdata, x_outer, x_inner, batch_size, n_workers)\n            dir_train, dir_valid, dir_test = ncv_data.get_dir_ncv_litdata()\n\n            pred_trn = execute_s2g(dir_train, path_gtype_pkl, path_mdl, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n            pred_trn.write_parquet(path_o_pred_trn)\n            pred_val = execute_s2g(dir_valid, path_gtype_pkl, path_mdl, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n            pred_val.write_parquet(path_o_pred_val)\n            pred_tst = execute_s2g(dir_test, path_gtype_pkl, path_mdl, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n            pred_tst.write_parquet(path_o_pred_tst)\n\n        return None\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBTransPipe.__init__","title":"<code>__init__(dir_log, dir_output, overwrite_collected_log=False)</code>","text":"<p>The pipeline to transform SNP features to genome block features.</p> <ol> <li>Collect trained models for each fold in nested cross-validation.</li> <li>Transform SNP features to genome block features.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>dir_log</code> <code>str</code> <p>The log directory of the SNP2GB models.</p> required <code>dir_output</code> <code>str</code> <p>The output directory.</p> required <code>overwrite_collected_log</code> <code>bool</code> <p>Whether to overwrite existing collected log. Default: <code>False</code>.</p> <code>False</code> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def __init__(\n        self,\n        dir_log: str,\n        dir_output: str,\n        overwrite_collected_log: bool = False,\n    ):\n    r\"\"\"The pipeline to transform SNP features to genome block features.\n\n    1. Collect trained models for each fold in nested cross-validation.\n    2. Transform SNP features to genome block features.\n\n    Args:\n        dir_log: The log directory of the SNP2GB models.\n\n        dir_output: The output directory.\n\n        overwrite_collected_log: Whether to overwrite existing collected log.\n            Default: ``False``.\n\n    \"\"\"\n    self.dir_log = dir_log\n    self.dir_output = dir_output\n    os.makedirs(self.dir_output, exist_ok=True)\n    self.overwrite_collected_log = overwrite_collected_log\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBTransPipe.collect_models","title":"<code>collect_models()</code>","text":"<p>Collect fitted models for each fold in nested cross-validation.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def collect_models(self):\n    r\"\"\"Collect fitted models for each fold in nested cross-validation.\n    \"\"\"\n    collector = CollectFitLog(self.dir_log)\n    models_bv, models_bi = collector.get_df_csv(self.dir_output, self.overwrite_collected_log)\n    self.models_bv = models_bv\n    self.models_bi = models_bi\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.SNP2GBTransPipe.convert_snp","title":"<code>convert_snp(dir_litdata, list_ncv=None, snp_onehot_bits=const.default.snp_onehot_bits, accelerator=const.default.accelerator, batch_size=const.default.batch_size, n_workers=const.default.n_workers)</code>","text":"<p>Convert SNPs to genome blocks features using the best model for each fold in nested cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>dir_litdata</code> <code>str</code> <p>The directory containing nested cross-validation data for S2G.</p> required <code>list_ncv</code> <code>Optional[List[List[int]]]</code> <p>The list of outer-inner folds to use. Default: <code>None</code>. If it is not <code>None</code>, it should be a list of lists of two integers, where the first integer is the outer fold index and the second integer is the inner fold index. If it is <code>None</code>, the best model overall is used.</p> <code>None</code> <code>snp_onehot_bits</code> <code>int</code> <p>The number of bits for one-hot representation of SNPs. Default: <code>10</code>.</p> <code>snp_onehot_bits</code> <code>accelerator</code> <code>str</code> <p>The accelerator to use. Default: <code>\"auto\"</code>.</p> <code>accelerator</code> <code>batch_size</code> <code>int</code> <p>The batch size to use. Default: <code>32</code>.</p> <code>batch_size</code> <code>n_workers</code> <code>int</code> <p>The number of workers to use for dataloader. Default: <code>1</code>.</p> <code>n_workers</code> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def convert_snp(\n        self,\n        dir_litdata: str,\n        list_ncv: Optional[List[List[int]]] = None,\n        snp_onehot_bits: int = const.default.snp_onehot_bits,\n        accelerator: str = const.default.accelerator,\n        batch_size: int = const.default.batch_size,\n        n_workers: int = const.default.n_workers,\n    ):\n    r\"\"\"Convert SNPs to genome blocks features using the best model for each fold in nested cross-validation.\n\n    Args:\n        dir_litdata: The directory containing nested cross-validation data for S2G.\n\n        list_ncv: The list of outer-inner folds to use.\n            Default: ``None``.\n            If it is not ``None``, it should be a list of lists of two integers,\n            where the first integer is the outer fold index and the second integer is the inner fold index.\n            If it is ``None``, the best model overall is used.\n\n        snp_onehot_bits: The number of bits for one-hot representation of SNPs.\n            Default: ``10``.\n\n        accelerator: The accelerator to use.\n            Default: ``\"auto\"``.\n\n        batch_size: The batch size to use.\n            Default: ``32``.\n\n        n_workers: The number of workers to use for dataloader.\n            Default: ``1``.\n\n    \"\"\"\n    if not hasattr(self,'models_bv'):\n        self.collect_models()\n\n    path_gtype_pkl = os.path.join(dir_litdata, const.fname.genotypes)\n\n    if list_ncv is None:\n        # Take the best model's path overall by searching the line min `val_loss` in models_bi.\n        path_best_model = self.models_bi.filter(pl.col(const.title_val_loss) == self.models_bi.select(const.title_val_loss).min()).select(const.dkey.ckpt_path)[0,0]\n        output = execute_s2g(dir_litdata, path_gtype_pkl, path_best_model, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n        output.write_parquet(os.path.join(self.dir_output, const.fname.transformed_genotypes))\n\n        return None\n\n    # For each inner fold\n    for data_xx in list_ncv:\n        x_outer, x_inner = data_xx\n        path_o_pred_trn = os.path.join(self.dir_output, const.fname.transformed_genotypes.removesuffix(\".parquet\") + f\"_{x_outer}_{x_inner}_trn.parquet\")\n        path_o_pred_val = os.path.join(self.dir_output, const.fname.transformed_genotypes.removesuffix(\".parquet\") + f\"_{x_outer}_{x_inner}_val.parquet\")\n        path_o_pred_tst = os.path.join(self.dir_output, const.fname.transformed_genotypes.removesuffix(\".parquet\") + f\"_{x_outer}_{x_inner}_tst.parquet\")\n\n        path_mdl = self.models_bv.filter((pl.col(const.dkey.which_outer) == x_outer) &amp; (pl.col(const.dkey.which_inner) == x_inner)).select(const.dkey.ckpt_path)[0,0]\n        print(f'\\nUsing model {path_mdl}\\n')\n\n        ncv_data = DEMDataModule4Train(dir_litdata, x_outer, x_inner, batch_size, n_workers)\n        dir_train, dir_valid, dir_test = ncv_data.get_dir_ncv_litdata()\n\n        pred_trn = execute_s2g(dir_train, path_gtype_pkl, path_mdl, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n        pred_trn.write_parquet(path_o_pred_trn)\n        pred_val = execute_s2g(dir_valid, path_gtype_pkl, path_mdl, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n        pred_val.write_parquet(path_o_pred_val)\n        pred_tst = execute_s2g(dir_test, path_gtype_pkl, path_mdl, self.dir_output, snp_onehot_bits, batch_size, accelerator)\n        pred_tst.write_parquet(path_o_pred_tst)\n\n    return None\n</code></pre>"},{"location":"reference/biodem.s2g.pipeline/#biodem.s2g.pipeline.execute_s2g","title":"<code>execute_s2g(dir_litdata, path_gtype_pkl, path_pretrained_model, dir_log_predict=os.getcwd(), snp_onehot_bits=const.default.snp_onehot_bits, batch_size=const.default.batch_size, accelerator=const.default.accelerator)</code>","text":"<p>Run the SNP2GB model for independent test / prediction.</p> Source code in <code>src\\biodem\\s2g\\pipeline.py</code> <pre><code>def execute_s2g(\n        dir_litdata: str,\n        path_gtype_pkl: str,\n        path_pretrained_model: str,\n        dir_log_predict: str = os.getcwd(),\n        snp_onehot_bits: int = const.default.snp_onehot_bits,\n        batch_size: int = const.default.batch_size,\n        accelerator: str = const.default.accelerator,\n    ):\n    r\"\"\"Run the SNP2GB model for independent test / prediction.\n    \"\"\"\n    g_data_dict = read_pkl_gv(path_gtype_pkl)\n    datamodule_s2g = DEMDataModule4Uni(dir_litdata, batch_size)\n    datamodule_s2g.setup()\n\n    model4gene = SNP2GB(\n        path_pretrained_model=path_pretrained_model,\n        blocks_gt=g_data_dict[const.dkey.gblock2gtype],\n        snp_onehot_bits=snp_onehot_bits,\n    )\n\n    avail_dev = get_avail_nvgpu()\n\n    trainer = Trainer(accelerator=accelerator, devices=avail_dev, default_root_dir=dir_log_predict, logger=False)\n\n    predictions = trainer.predict(model=model4gene, datamodule=datamodule_s2g)\n    assert predictions is not None\n    pred_array = np.concatenate(predictions)\n    print(f\"\\nShape of prediction results: {pred_array.shape}\\n\")\n\n    # Rename index to sample_ids\n    # - Prepare sample ids\n    sample_ids = []\n    for batch in datamodule_s2g.predict_dataloader():\n        sample_ids.extend(batch[const.dkey.litdata_id])\n    print(f'Number of samples: {len(sample_ids)}')\n    print(sample_ids)\n    assert len(sample_ids) == len(pred_array)\n    assert len(g_data_dict[const.dkey.gblock_ids]) == pred_array.shape[1]\n\n    # Prepare prediction dataframe\n    pred_df = pl.DataFrame(pred_array, schema=g_data_dict[const.dkey.gblock_ids])\n    # Add a column of sample ids\n    df_ids = pl.DataFrame(sample_ids, schema=[const.dkey.id])\n    pred_df = df_ids.hstack(pred_df)\n\n    return pred_df\n</code></pre>"},{"location":"reference/biodem.s2g.usage/","title":"How to use <code>biodem.s2g</code>","text":"<p>If you have genomic variants and a reference genome's annotations, please encode SNPs into dense and continuous feature vectors that can be used for downstream analysis.</p> <p></p>"},{"location":"reference/biodem.s2g.usage/#1-search-and-encode-snps","title":"1. Search and encode SNPs","text":"<p>We provide a fast &amp; lightweight tool <code>pregv</code> for VCF &amp; GFF file processing and SNP encoding.</p> <pre><code>$ pregv --help\nEncode genotypes from VCF file based on GFF info.\n\nUsage: pregv &lt;COMMAND&gt;\n\nCommands:\n  gff2bin  Build GFF dict\n  vcf2enc  Encode genotypes from VCF file based on GFF info.\n  help     Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version\n</code></pre>"},{"location":"reference/biodem.s2g.usage/#11-build-gff-dict","title":"1.1 Build GFF dict","text":"<ul> <li>Input file: a GFF file.</li> </ul> <pre><code>$ pregv gff2bin --help\nBuild GFF dict\n\nUsage: pregv gff2bin --input-gff &lt;gff&gt; --output &lt;output&gt;\n\nOptions:\n  -g, --input-gff &lt;gff&gt;  Input GFF file.\n  -o, --output &lt;output&gt;  Output bin.gz file.\n  -h, --help             Print help\n  -V, --version          Print version\n</code></pre>"},{"location":"reference/biodem.s2g.usage/#12-encode-genotypes-from-vcf-file","title":"1.2 Encode genotypes from VCF file","text":"<ul> <li>Input files: a VCF file and a dictionary of GFF.</li> <li>Output: one-hot encoding SNPs based on the actual di-nucleotide composition.</li> </ul> <pre><code>$ pregv vcf2enc --help\nEncode genotypes from VCF file based on GFF info.\n\nUsage: pregv vcf2enc [OPTIONS] --input-vcf &lt;vcf&gt; --input-gffdict &lt;gffdict&gt; --output &lt;output&gt;\n\nOptions:\n  -v, --input-vcf &lt;vcf&gt;          Input VCF file\n  -d, --input-gffdict &lt;gffdict&gt;  Input GFF dict file\n  -o, --output &lt;output&gt;          Output pickle file\n  -s, --strand &lt;strand&gt;          Use \"+\", \"-\" or \".\"(both) to specify strand [default: .]\n  -m, --more-mem                 Use more RAM\n  -t, --threads &lt;nthreads&gt;       Number of threads [default: 0]\n  -h, --help                     Print help\n  -V, --version                  Print version\n</code></pre>"},{"location":"reference/biodem.s2g.usage/#2-transform-encoded-snps-to-genomic-embeddings","title":"2. Transform encoded SNPs to genomic embeddings","text":"<ul> <li>The one-hot encoding SNPs are transformed into dense and continuous features that represent genomic variation (each feature corresponds to a gene).</li> </ul>"},{"location":"reference/biodem.s2g.usage/#21-nested-cross-validation-and-data-preprocessing","title":"2.1 Nested cross-validation and data preprocessing","text":"<p>Please checkout the documentations at Modules &gt; Utilities &gt; Preprocessing Data &gt; <code>OptimizeLitdataNCV</code>.</p> <p>This is an example of running the module:</p> run_s2g_prep.py<pre><code>import os\nimport sys\nfrom biodem import OptimizeLitdataNCV\n\n\ntrait_name = sys.argv[1]\nwhich_o = int(sys.argv[2])\nwhich_i = int(sys.argv[3])\n\nif trait_name.startswith(\"all\"):\n    which_trait = None\nelse:\n    which_trait = [trait_name]\n\n\nif __name__ == \"__main__\":\n    k_outer = 10\n    k_inner = 5\n    dir_home = os.path.dirname(os.path.abspath(__file__))\n    output_dir = os.path.join(dir_home, \"run_s2g\", trait_name, \"litdata\")\n    path_gt = os.path.join(dir_home, \"data_prep\", \"snp.pkl.gz\")\n    path_labels = os.path.join(dir_home, \"data_prep\", \"phenotypes.csv\")\n\n    _opt = OptimizeLitdataNCV(\n        paths_omics={\"gv\": path_gt},\n        path_label=path_labels,\n        output_dir=output_dir,\n        k_outer=k_outer,\n        k_inner=k_inner,\n        which_outer_inner=[which_o, which_i],\n        col2use_in_labels=which_trait,\n    )\n    _opt.run_optimization()\n</code></pre>"},{"location":"reference/biodem.s2g.usage/#22-s2g-modeling","title":"2.2 S2G modeling","text":"<p>Please checkout the documentations at Modules &gt; S2G &gt; Pipeline &gt; <code>SNP2GBFitPipe</code>.</p> <p>This is an example of running the module:</p> run_s2g_fit.py<pre><code>import os\nimport sys\nfrom biodem import SNP2GBFitPipe\n\n\nif len(sys.argv) &lt; 2:\n    raise ValueError('Please specify the TRAIT NAME')\ntrait_name = sys.argv[1]\n\nif len(sys.argv) &lt; 4:\n    print('Start default NCV: 10 outer folds and 5 inner folds')\n    list_ncv = [[i,j] for i in range(10) for j in range(5)]\nelse:\n    print('Start with NCV: {} outer folds and {} inner folds'.format(sys.argv[2], sys.argv[3]))\n    list_ncv = [[int(sys.argv[2]), int(sys.argv[3])]]\n\nwork_dir_home = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run_s2g\", trait_name)\nlitdata_dir = os.path.join(work_dir_home, 'litdata')\nis_regression = True\nlog_dir = os.path.join(work_dir_home, 'models')\nn_jobs = 1\nn_trials = 12\n\n\nif __name__ == '__main__':\n    pipe_fit = SNP2GBFitPipe(\n        litdata_dir=litdata_dir,\n        list_ncv=list_ncv,\n        log_dir=log_dir,\n        regression=is_regression,\n        n_jobs=n_jobs,\n        n_trials=n_trials,\n    )\n    pipe_fit.train_pipeline()\n</code></pre>"},{"location":"reference/biodem.s2g.usage/#23-s2g-transformation","title":"2.3 S2G Transformation","text":"<p>Please checkout the documentations at Modules &gt; S2G &gt; Pipeline &gt; <code>SNP2GBTransPipe</code>.</p> <p>This is an example of running the module:</p> run_s2g_transf.py<pre><code>import os\nimport sys\nfrom biodem import SNP2GBTransPipe\n\n\nif len(sys.argv) &lt; 2:\n    raise ValueError('Please specify the TRAIT NAME')\ntrait_name = sys.argv[1]\n\nif len(sys.argv) &lt; 4:\n    print('Start default NCV: 10 outer folds and 5 inner folds')\n    list_ncv = [[i,j] for i in range(10) for j in range(5)]\nelse:\n    print('Start with NCV: {} outer folds and {} inner folds'.format(sys.argv[2], sys.argv[3]))\n    list_ncv = [[int(sys.argv[2]), int(sys.argv[3])]]\n\nwork_dir_home = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run_s2g\", trait_name)\nlitdata_dir = os.path.join(work_dir_home, 'litdata')\nmodels_dir = os.path.join(work_dir_home, \"models\")\noutput_dir = os.path.join(work_dir_home, \"transf\")\n\nif __name__ == '__main__':\n    pipe_transf = SNP2GBTransPipe(\n        dir_log=models_dir,\n        dir_output=output_dir,\n    )\n    pipe_transf.convert_snp(\n        dir_litdata=litdata_dir,\n        list_ncv=list_ncv,\n    )\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/","title":"biodem.utils.data_ncv","text":""},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv","title":"<code>biodem.utils.data_ncv</code>","text":"<p>Read multi-omics data (and labels) and split into nested train/val/test sets.</p> <p>The processed data will be stored in litdata's format.</p>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataModule4Train","title":"<code>DEMDataModule4Train</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>class DEMDataModule4Train(LightningDataModule):\n    def __init__(\n            self,\n            litdata_dir: str,\n            which_outer_testset: int,\n            which_inner_valset: int,\n            batch_size: int,\n            n_workers: int = const.default.n_workers,\n        ):\n        r\"\"\"LightningDataModule for training.\n\n        Args:\n            litdata_dir: Directory containing the LitData for nested cross-validation.\n\n            which_outer_testset: Index of the outer test set fold.\n\n            which_inner_valset: Index of the inner validation set fold.\n\n            batch_size: Batch size for dataloader.\n\n            n_workers: Number of workers for dataloader.\n\n        \"\"\"\n        super().__init__()\n        self.litdata_dir = litdata_dir\n        self.which_outer_testset = which_outer_testset\n        self.which_inner_valset = which_inner_valset\n        self.batch_size = batch_size\n        self.n_workers = n_workers\n\n    def setup(self, stage=None):\n        self.dataloder_trn, self.dataloader_val, self.dataloader_test = self.read_litdata_ncv()\n\n    def train_dataloader(self):\n        return self.dataloder_trn\n    def val_dataloader(self):\n        return self.dataloader_val\n    def test_dataloader(self):\n        return self.dataloader_test\n\n    def read_litdata_ncv(self):\n        \"\"\"\n        Read litdata from directories and return dataloaders for NCV.\n        \"\"\"\n        dir_train, dir_valid, dir_test = self.get_dir_ncv_litdata()\n        dataloader_train = StreamingDataLoader(StreamingDataset(dir_train), batch_size=self.batch_size, num_workers=self.n_workers, shuffle=True)\n        dataloader_valid = StreamingDataLoader(StreamingDataset(dir_valid), batch_size=self.batch_size, num_workers=self.n_workers)\n        dataloader_test = StreamingDataLoader(StreamingDataset(dir_test), batch_size=self.batch_size, num_workers=self.n_workers)\n        return dataloader_train, dataloader_valid, dataloader_test\n\n    def get_dir_ncv_litdata(self):\n        self.dir_xoi = os.path.join(self.litdata_dir, f\"ncv_test_{self.which_outer_testset}_val_{self.which_inner_valset}\")\n        dir_trn = os.path.join(self.dir_xoi, const.title_train)\n        dir_val = os.path.join(self.dir_xoi, const.title_val)\n        dir_tst = os.path.join(self.dir_xoi, const.title_test)\n        return dir_trn, dir_val, dir_tst\n\n    def read_omics_dims(self):\n        return pl.read_csv(os.path.join(self.dir_xoi, const.fname.predata_omics_dims)).select(const.dkey.omics_dim).to_series().to_list()\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataModule4Train.__init__","title":"<code>__init__(litdata_dir, which_outer_testset, which_inner_valset, batch_size, n_workers=const.default.n_workers)</code>","text":"<p>LightningDataModule for training.</p> <p>Parameters:</p> Name Type Description Default <code>litdata_dir</code> <code>str</code> <p>Directory containing the LitData for nested cross-validation.</p> required <code>which_outer_testset</code> <code>int</code> <p>Index of the outer test set fold.</p> required <code>which_inner_valset</code> <code>int</code> <p>Index of the inner validation set fold.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for dataloader.</p> required <code>n_workers</code> <code>int</code> <p>Number of workers for dataloader.</p> <code>n_workers</code> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def __init__(\n        self,\n        litdata_dir: str,\n        which_outer_testset: int,\n        which_inner_valset: int,\n        batch_size: int,\n        n_workers: int = const.default.n_workers,\n    ):\n    r\"\"\"LightningDataModule for training.\n\n    Args:\n        litdata_dir: Directory containing the LitData for nested cross-validation.\n\n        which_outer_testset: Index of the outer test set fold.\n\n        which_inner_valset: Index of the inner validation set fold.\n\n        batch_size: Batch size for dataloader.\n\n        n_workers: Number of workers for dataloader.\n\n    \"\"\"\n    super().__init__()\n    self.litdata_dir = litdata_dir\n    self.which_outer_testset = which_outer_testset\n    self.which_inner_valset = which_inner_valset\n    self.batch_size = batch_size\n    self.n_workers = n_workers\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataModule4Train.read_litdata_ncv","title":"<code>read_litdata_ncv()</code>","text":"<p>Read litdata from directories and return dataloaders for NCV.</p> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def read_litdata_ncv(self):\n    \"\"\"\n    Read litdata from directories and return dataloaders for NCV.\n    \"\"\"\n    dir_train, dir_valid, dir_test = self.get_dir_ncv_litdata()\n    dataloader_train = StreamingDataLoader(StreamingDataset(dir_train), batch_size=self.batch_size, num_workers=self.n_workers, shuffle=True)\n    dataloader_valid = StreamingDataLoader(StreamingDataset(dir_valid), batch_size=self.batch_size, num_workers=self.n_workers)\n    dataloader_test = StreamingDataLoader(StreamingDataset(dir_test), batch_size=self.batch_size, num_workers=self.n_workers)\n    return dataloader_train, dataloader_valid, dataloader_test\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataModule4Uni","title":"<code>DEMDataModule4Uni</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>class DEMDataModule4Uni(LightningDataModule):\n    def __init__(\n            self,\n            litdata_dir: str,\n            batch_size: int = const.default.batch_size,\n            n_workers: int = const.default.n_workers,\n        ):\n        r\"\"\"LightningDataModule for prediction.\n\n        Args:\n            litdata_dir: Directory containing the LitData for prediction.\n\n            batch_size: Batch size for prediction.\n\n            n_workers: Number of workers for dataloader.\n\n        \"\"\"\n        super().__init__()\n        self.litdata_dir = litdata_dir\n        self.batch_size = batch_size\n        self.n_workers = n_workers\n\n    def setup(self, stage=None):\n        self._dataset = StreamingDataset(self.litdata_dir)\n        print(f\"Length of dataset: {len(self._dataset)}\")\n        self._dataloader_pred = StreamingDataLoader(self._dataset, batch_size=self.batch_size, num_workers=self.n_workers)\n        self._dataloader_test = StreamingDataLoader(self._dataset, batch_size=self.batch_size, num_workers=self.n_workers)\n\n    def predict_dataloader(self):\n        return self._dataloader_pred\n\n    def test_dataloader(self):\n        return self._dataloader_test\n\n    def shuffle_a_feat(\n            self,\n            which_omics: Union[int, str],\n            which_feature: int,\n            random_state: int,\n            save_litdata: bool = False,\n            chunk_bytes: str = const.default.chunk_bytes,\n            compression: str = const.default.compression_alg,\n        ) -&gt; str | DataLoader:\n        r\"\"\"\n        Shuffle one feature in one omics data.\n\n        Args:\n            which_omics: The index or name of the omics data to shuffle.\n\n            which_feature: The index of the feature to shuffle.\n\n            random_state: The random seed for reproducibility.\n\n        \"\"\"\n        if not hasattr(self, 'data_all'):\n            self.data_all = self.read_dataloader()\n\n        if not hasattr(self, 'omics_names'):\n            self.omics_names = read_omics_names(self.litdata_dir)\n\n        if isinstance(which_omics, str):\n            which_om = self.omics_names.index(which_omics)\n        else:\n            if (which_omics + 1) &gt; len(self.omics_names):\n                raise ValueError(\"The specified omics index is out of range.\")\n            which_om = which_omics\n\n        \"\"\"\n        Shuffle a feature's values\n        \"\"\"\n        data_all = deepcopy(self.data_all)\n        _tmp: np.ndarray = data_all[const.dkey.litdata_omics][which_om]\n\n        np.random.seed(random_state)\n        _tmp[:, which_feature] = np.random.permutation(_tmp[:, which_feature])\n        data_all[const.dkey.litdata_omics][which_om] = _tmp\n\n        \"\"\"\n        Create new dataset and save as litdata\n        \"\"\"\n        _dataset = Dict2Dataset(data_all)\n        if save_litdata:\n            output_dir = self.litdata_dir + f\"_shuffle_om+{which_om}_feat+{which_feature}_rand+{random_state}\"\n            optimize(\n                fn = _dataset.__getitem__,\n                inputs = range(len(_dataset)),\n                output_dir = output_dir,\n                chunk_bytes = chunk_bytes,\n                compression = compression,\n            )\n            return output_dir\n        else:\n            return DataLoader(_dataset, self.batch_size, num_workers=self.n_workers)\n\n    def read_dataloader(self) -&gt; Dict[str, Any]:\n        if not hasattr(self, \"_dataloader_pred\"):\n            self.setup()\n\n        \"\"\"\n        Read data from dataloader.\n        \"\"\"\n        data_all = {}\n        for batch in self.predict_dataloader():\n            for xkey in batch.keys():\n                if xkey not in data_all:\n                    data_all[xkey] = batch[xkey]\n                else:\n                    if xkey == const.dkey.litdata_omics:\n                        data_all[xkey] = [np.concatenate([data_all[xkey][i], batch[xkey][i]], axis=0) for i in range(len(data_all[xkey]))]\n                        # for i in range(len(data_all[xkey])):\n                        #     print(f\"shape of {xkey}: {data_all[xkey][i].shape}, type: {type(data_all[xkey][i])}\")\n                    else:\n                        data_all[xkey] = np.concatenate((data_all[xkey], batch[xkey]), axis=0)\n                        # print(f\"shape of {xkey}: {data_all[xkey].shape}, type: {type(data_all[xkey])}\")\n        return data_all\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataModule4Uni.__init__","title":"<code>__init__(litdata_dir, batch_size=const.default.batch_size, n_workers=const.default.n_workers)</code>","text":"<p>LightningDataModule for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>litdata_dir</code> <code>str</code> <p>Directory containing the LitData for prediction.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> <code>batch_size</code> <code>n_workers</code> <code>int</code> <p>Number of workers for dataloader.</p> <code>n_workers</code> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def __init__(\n        self,\n        litdata_dir: str,\n        batch_size: int = const.default.batch_size,\n        n_workers: int = const.default.n_workers,\n    ):\n    r\"\"\"LightningDataModule for prediction.\n\n    Args:\n        litdata_dir: Directory containing the LitData for prediction.\n\n        batch_size: Batch size for prediction.\n\n        n_workers: Number of workers for dataloader.\n\n    \"\"\"\n    super().__init__()\n    self.litdata_dir = litdata_dir\n    self.batch_size = batch_size\n    self.n_workers = n_workers\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataModule4Uni.shuffle_a_feat","title":"<code>shuffle_a_feat(which_omics, which_feature, random_state, save_litdata=False, chunk_bytes=const.default.chunk_bytes, compression=const.default.compression_alg)</code>","text":"<p>Shuffle one feature in one omics data.</p> <p>Parameters:</p> Name Type Description Default <code>which_omics</code> <code>Union[int, str]</code> <p>The index or name of the omics data to shuffle.</p> required <code>which_feature</code> <code>int</code> <p>The index of the feature to shuffle.</p> required <code>random_state</code> <code>int</code> <p>The random seed for reproducibility.</p> required Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def shuffle_a_feat(\n        self,\n        which_omics: Union[int, str],\n        which_feature: int,\n        random_state: int,\n        save_litdata: bool = False,\n        chunk_bytes: str = const.default.chunk_bytes,\n        compression: str = const.default.compression_alg,\n    ) -&gt; str | DataLoader:\n    r\"\"\"\n    Shuffle one feature in one omics data.\n\n    Args:\n        which_omics: The index or name of the omics data to shuffle.\n\n        which_feature: The index of the feature to shuffle.\n\n        random_state: The random seed for reproducibility.\n\n    \"\"\"\n    if not hasattr(self, 'data_all'):\n        self.data_all = self.read_dataloader()\n\n    if not hasattr(self, 'omics_names'):\n        self.omics_names = read_omics_names(self.litdata_dir)\n\n    if isinstance(which_omics, str):\n        which_om = self.omics_names.index(which_omics)\n    else:\n        if (which_omics + 1) &gt; len(self.omics_names):\n            raise ValueError(\"The specified omics index is out of range.\")\n        which_om = which_omics\n\n    \"\"\"\n    Shuffle a feature's values\n    \"\"\"\n    data_all = deepcopy(self.data_all)\n    _tmp: np.ndarray = data_all[const.dkey.litdata_omics][which_om]\n\n    np.random.seed(random_state)\n    _tmp[:, which_feature] = np.random.permutation(_tmp[:, which_feature])\n    data_all[const.dkey.litdata_omics][which_om] = _tmp\n\n    \"\"\"\n    Create new dataset and save as litdata\n    \"\"\"\n    _dataset = Dict2Dataset(data_all)\n    if save_litdata:\n        output_dir = self.litdata_dir + f\"_shuffle_om+{which_om}_feat+{which_feature}_rand+{random_state}\"\n        optimize(\n            fn = _dataset.__getitem__,\n            inputs = range(len(_dataset)),\n            output_dir = output_dir,\n            chunk_bytes = chunk_bytes,\n            compression = compression,\n        )\n        return output_dir\n    else:\n        return DataLoader(_dataset, self.batch_size, num_workers=self.n_workers)\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataset","title":"<code>DEMDataset</code>","text":"Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>class DEMDataset:\n    def __init__(\n            self,\n            reproduction_mode: bool,\n            paths_omics: Dict[str, str],\n            path_label: Optional[str] = None,\n            col2use_in_label: Optional[Union[List[str], List[int]]] = None,\n            sample_ind_for_preproc: Optional[List[int]] = None,\n            dir_preprocessors: Optional[str] = None,\n            target_n_samples: Optional[int] = None,\n            seed_resample: int = const.default.seed_1,\n            n_fragments: int = 1,\n            prepr_labels: bool = True,\n            prepr_omics: bool = True,\n            snp_onehot_bits: int = const.default.snp_onehot_bits,\n            which_outer_test: Optional[int] = None,\n            which_inner_val: Optional[int] = None,\n            variance_threshold: float = const.default.variance_threshold,\n            save_n_feat: int = const.default.n_feat2save,\n            n_estimators: int = const.default.n_estimators,\n            random_states: List[int] = const.default.random_states,\n            n_jobs_rf: int = const.default.n_jobs_rf,\n        ) -&gt; None:\n        r\"\"\"Read data for litdata optimization. Preprocessing is optional.\n\n        If you have labels (phenotypes) and want to use them,\n        the input labels (phenotypes) are expected as follows:\n\n        + For **REGRESSION** task\n            + Please keep original values that are not preprocessed.\n            + If you have MULTIPLE traits, please set different columns names in CSV file.\n            + The pipeline ***standardize/normalize data AFTER splitting*** to ***avoid data leakage.***\n            + The method and parameters of standardization/normalization are kept the same as those in training data.\n        + For **CLASSIFICATION** task\n            + Please transform labels by one-hot encoder ***MANUALLY BEFORE input***.\n            + The length of one-hot vectors is recommended to be **n_categories + 1** for **UNPRECEDENTED labels**.\n            + If you have MULTIPLE traits, please **concatenate** one-hot encoded matrix along the horizontal axis before input.\n\n        Args:\n            reproduction_mode: Whether to use existing processors.\n                Please provide ``dir_preprocessors`` if ``reproduction_mode`` is ``True``.\n\n            paths_omics: The ``Dict`` ``{name: path}`` of paths to multiple ``.csv`` or ``.parquet`` files or directories. For genotypes, it is a path to a ``.pkl.gz`` file.\n                If some paths are directories, the files inside will be read as existing data, and ``target_n_samples`` will be ignored.\n\n            path_label: The path to label data (a ``.csv`` or ``.parquet`` file).\n                If it is `None`, no labels(phenotypes) are provided.\n\n            col2use_in_label: The columns to use in label data.\n                If it is `List[int]`, its numbers are the indices **(1-based)** of the columns to be used.\n                If it is `None`, all columns are used.\n\n            sample_ind_for_preproc: The indices for selecting samples for preprocessors fitting.\n                If it is `None`, all samples are used.\n                It is ignored when existing data are used.\n\n            dir_preprocessors: The directory used to save data processors that fitted on training data.\n                If it is `None`, preprocessing is not performed.\n\n            target_n_samples: The target sample size for expanding the dataset through random sampling.\n                Resampling is not performed when existing data are used.\n\n            seed_resample: The random seed for sampling new samples.\n                Default: ``const.default.seed_1``.\n\n            n_fragments: The number of fragments (= k_outer * k_inner).\n                Default: ``1``.\n\n            prepr_labels: Whether to preprocess labels or not.\n                Default: ``True``.\n\n            prepr_omics: Whether to preprocess omics data or not.\n                Default: ``True``.\n\n            snp_onehot_bits: The number of bits for one-hot encoding SNPs.\n                Default: ``const.default.snp_onehot_bits``.\n\n            which_outer_test: The index of outer test set. It is used for reading existing data.\n\n            which_inner_val: The index of inner validation set. It is used for reading existing data.\n\n            variance_threshold: The threshold for variance selection.\n\n            save_n_feat: The number of features to save when random forest selection is performed.\n                Default: ``const.default.n_feat2save``.\n\n            n_estimators: The number of trees in the random forest.\n                Default: ``const.default.n_estimators``.\n\n            random_states: The random states for random forest selection.\n                Default: ``const.default.random_states``.\n\n            n_jobs_rf: The number of jobs for parallelized random forest fitting.\n                Default: ``const.default.n_jobs_rf``.\n\n        Usage:\n\n            &gt;&gt;&gt; from biodem.utils.data_ncv import DEMDataset\n            &gt;&gt;&gt; _dataset = DEMDataset(...)\n            &gt;&gt;&gt; _dataset._setup()\n\n        \"\"\"\n        super().__init__()\n        self.reproduction_mode = reproduction_mode\n        self.paths_omics = paths_omics\n        self.path_label = path_label\n        self.col2use_in_label = col2use_in_label\n\n        self.sample_ind_for_preproc = sample_ind_for_preproc\n        self.dir_preprocessors = dir_preprocessors\n\n        self.prepr_labels = prepr_labels\n        self.prepr_omics = prepr_omics\n        self.variance_threshold = variance_threshold\n        self.save_n_feat = save_n_feat\n        self.n_estimators = n_estimators\n        self.random_states = random_states\n        self.n_jobs_rf = n_jobs_rf\n\n        self.snp_onehot_bits = snp_onehot_bits\n\n        self.target_n_samples = target_n_samples\n        self.seed_resample = seed_resample\n        self.n_fragments = n_fragments\n\n        self.which_outer_test = which_outer_test\n        self.which_inner_val = which_inner_val\n\n        self.existing_omics_sample_id: Dict[str, Dict[str, List[str]]] = {}\n        self.existing_omics: Dict[str, Dict[str, pl.DataFrame]] = {}\n\n        self.omics_name = sorted(list(paths_omics.keys()))\n        self.omics_name_new: List[str] = []\n        self.omics_name_existing: List[str] = []\n        self.indices_trn: List[int] | None = None\n        self.indices_val: List[int] | None = None\n        self.indices_tst: List[int] | None = None\n\n        self.key_gv = None\n        self.n_omics = len(self.omics_name)\n        self.omics_dfs: Dict[str, pl.DataFrame] = {}\n\n        self.sample_ids: List[str] = []\n        self.labels_df = None\n        self.dim_model_output = None\n\n        self.omics_data: List[np.ndarray] = []\n        self.omics_features: List[List[str]] = []\n        self.omics_dims: List[int] = []\n\n    def _setup(self):\n        self._pick_shared_samples_in_omics()\n\n        if self.path_label is not None:\n            self._pick_shared_samples_in_omics_and_labels(self.path_label, self.col2use_in_label)\n\n        self.n_samples = len(self.sample_ids)\n\n        # Try reading existing (treated) omics data\n        self._read_existing_omics(self.paths_omics)\n\n        # ! Pick samples that are shared in all omics, especially for the case of existing data\n        self.recommend_index_by_existing_omics()\n\n        # !!! Resampling is not necessary when using existing data !!!\n        if len(self.omics_name_existing) &lt; 1:\n            self._calc_n_samples2sample(self.target_n_samples, self.n_fragments)\n\n            if self.n_samples_to_add &gt; 0:\n                self._sample_new2add(self.seed_resample)\n\n        # Preprocess omics data\n        if self.dir_preprocessors is not None:\n            if self.prepr_labels:\n                self._proc_labels(self.sample_ind_for_preproc, self.dir_preprocessors, self.reproduction_mode)\n            if self.prepr_omics:\n                self._proc_omics(self.sample_ind_for_preproc, self.dir_preprocessors, self.reproduction_mode)\n\n        # Preprocess genotype data\n        if self.key_gv is not None:\n            gv_np = self.omics_dfs[self.key_gv].drop(const.dkey.id).to_numpy()\n            gv_np_onehot = onehot_encode_snp_mat(gv_np, self.snp_onehot_bits)\n            self.omics_dfs[self.key_gv] = pl.DataFrame({const.dkey.id: self.sample_ids}).hstack(pl.DataFrame(data=gv_np_onehot))\n\n        # Get omics data properties\n        for i in range(self.n_omics):\n            self.omics_data.append(self.omics_dfs[self.omics_name[i]].drop(const.dkey.id).to_numpy().astype(np.float32))\n            self.omics_features.append(self.omics_dfs[self.omics_name[i]].drop(const.dkey.id).columns)\n            self.omics_dims.append(self.omics_data[i].shape[1])\n\n        if self.dir_preprocessors is not None:\n            # Write omics' dimensions for the initialization of the model\n            pl.DataFrame(data={const.dkey.omics_dim: self.omics_dims}).write_csv(os.path.join(self.dir_preprocessors, const.fname.predata_omics_dims))\n            # Write omics' features\n            for i in range(self.n_omics):\n                pl.DataFrame(data={const.dkey.omics_feature: self.omics_features[i]}).write_csv(os.path.join(self.dir_preprocessors, f\"{const.fname.predata_omics_features_prefix}_{self.omics_name[i]}.csv\"))\n            # Write labels' names\n            if self.labels_df is not None:\n                pl.DataFrame(data={const.dkey.label: self.labels_df.columns[1:]}).write_csv(os.path.join(self.dir_preprocessors, const.fname.predata_label_names))\n\n    def __len__(self):\n        return self.n_samples\n\n    def __getitem__(self, index):\n        omics_data_i = [omics_x[index, :] for omics_x in self.omics_data]\n        sample_id_i = self.sample_ids[index]\n        if hasattr(self, \"label_data\"):\n            label_data_i = self.label_data[index, :]\n            data_o = {const.dkey.litdata_index: index, const.dkey.litdata_omics: deepcopy(omics_data_i), const.dkey.litdata_id: deepcopy(sample_id_i), const.dkey.litdata_label: deepcopy(label_data_i)}\n        else:\n            data_o = {const.dkey.litdata_index: index, const.dkey.litdata_omics: deepcopy(omics_data_i), const.dkey.litdata_id: deepcopy(sample_id_i)}\n        return data_o\n\n    def _read_existing_omics(self, paths_omics: Dict[str, str], default_file_ext: str = const.fname.data_ext):\n        r\"\"\"Read existing omics data from the given paths.\n        If the path is a directory, then the data will be read from the files that names could be recognized by the function ``read_omics_xoxi``.\n        \"\"\"\n        for ikey in self.omics_name:\n            _tmp_path = paths_omics[ikey]\n\n            if os.path.isdir(_tmp_path):\n                if self.which_inner_val is not None and self.which_outer_test is not None:\n                    _tmp_trn = read_omics_xoxi(_tmp_path, self.which_outer_test, self.which_inner_val, const.abbr_train, default_file_ext)\n                    _tmp_val = read_omics_xoxi(_tmp_path, self.which_outer_test, self.which_inner_val, const.abbr_val, default_file_ext)\n                    _tmp_tst = read_omics_xoxi(_tmp_path, self.which_outer_test, self.which_inner_val, const.abbr_test, default_file_ext)\n                    _tmp_trn_sample_id = _tmp_trn.select(const.dkey.id).to_series().to_list()\n                    _tmp_val_sample_id = _tmp_val.select(const.dkey.id).to_series().to_list()\n                    _tmp_tst_sample_id = _tmp_tst.select(const.dkey.id).to_series().to_list()\n                    self.existing_omics_sample_id[ikey] = {const.abbr_train: _tmp_trn_sample_id, const.abbr_val: _tmp_val_sample_id, const.abbr_test: _tmp_tst_sample_id}\n                    self.existing_omics[ikey] = {const.abbr_train: _tmp_trn, const.abbr_val: _tmp_val, const.abbr_test: _tmp_tst}\n                    self.omics_name_existing.append(ikey)\n                else:\n                    raise NotImplementedError\n            else:\n                continue\n        return None\n\n    def recommend_index_by_existing_omics(self):\n        r\"\"\"Recommend indices for the samples that are shared in all omics if existing data is used.\n        \"\"\"\n        if len(self.omics_name_existing) &lt; 1:\n            return None\n        inters_trn, indices_trn = intersect_lists([self.sample_ids, *[self.existing_omics_sample_id[ikey][const.abbr_train] for ikey in self.omics_name_existing]])\n        inters_val, indices_val = intersect_lists([self.sample_ids, *[self.existing_omics_sample_id[ikey][const.abbr_val] for ikey in self.omics_name_existing]])\n        inters_tst, indices_tst = intersect_lists([self.sample_ids, *[self.existing_omics_sample_id[ikey][const.abbr_test] for ikey in self.omics_name_existing]])\n\n        for i in range(len(self.omics_name_existing)):\n            _tmp_part_trn = self.existing_omics[self.omics_name_existing[i]][const.abbr_train][indices_trn[i+1], :]\n            _tmp_part_val = self.existing_omics[self.omics_name_existing[i]][const.abbr_val][indices_val[i+1], :]\n            _tmp_part_tst = self.existing_omics[self.omics_name_existing[i]][const.abbr_test][indices_tst[i+1], :]\n            self.omics_dfs[self.omics_name_existing[i]] = _tmp_part_trn.vstack(_tmp_part_val).vstack(_tmp_part_tst)\n\n        for i in range(len(self.omics_name_new)):\n            _tmp_part_trn = self.omics_dfs[self.omics_name_new[i]][indices_trn[0],:]\n            _tmp_part_val = self.omics_dfs[self.omics_name_new[i]][indices_val[0],:]\n            _tmp_part_tst = self.omics_dfs[self.omics_name_new[i]][indices_tst[0],:]\n            self.omics_dfs[self.omics_name_new[i]] = _tmp_part_trn.vstack(_tmp_part_val).vstack(_tmp_part_tst)\n\n        if self.labels_df is not None:\n            self.labels_df = self.labels_df[indices_trn[0]+indices_val[0]+indices_tst[0],:]\n\n        n_id_trn = len(inters_trn)\n        n_id_val = len(inters_val)\n        n_id_tst = len(inters_tst)\n        self.indices_trn = [i for i in range(n_id_trn)]\n        self.indices_val = [i+n_id_trn for i in range(n_id_val)]\n        self.indices_tst = [i+n_id_trn+n_id_val for i in range(n_id_tst)]\n\n        self.sample_ids = inters_trn + inters_val + inters_tst\n        self.sample_ids_trn = inters_trn\n        self.sample_ids_val = inters_val\n        self.sample_ids_tst = inters_tst\n\n        self.n_samples = len(self.sample_ids)\n        self.sample_ind_for_preproc = self.indices_trn\n\n    def _pick_shared_samples_in_omics(self):\n        r\"\"\"Pick samples that are shared between omics.\n        \"\"\"\n        original_omics_IDs = []\n        for ikey in self.omics_name:\n            _tmp_path = self.paths_omics[ikey]\n\n            if os.path.isdir(_tmp_path):\n                continue\n            else:\n                self.omics_dfs[ikey] = read_omics(_tmp_path)\n                if _tmp_path.lower().endswith(\".pkl.gz\"):\n                    self.key_gv = ikey\n\n                original_omics_IDs.append(self.omics_dfs[ikey].select(const.dkey.id).to_series().to_list())\n                self.omics_name_new.append(ikey)\n\n        intersect_ids_in_omics, _indices = intersect_lists(original_omics_IDs)\n        for i in range(len(self.omics_name_new)):\n            self.omics_dfs[self.omics_name_new[i]] = self.omics_dfs[self.omics_name_new[i]][_indices[i],:].sort(const.dkey.id)\n\n        self.sample_ids = intersect_ids_in_omics\n\n    def _pick_shared_samples_in_omics_and_labels(self, path_label: str, col2use_in_label: Optional[Union[List[str], List[int]]]):\n        r\"\"\"Pick samples that are shared between omics and labels.\n        \"\"\"\n        labels_df, dim_model_output, sample_ids_in_labels = read_labels(path_label, col2use_in_label)\n        intersect_ids, _indices = intersect_lists([self.sample_ids, sample_ids_in_labels])\n\n        for i in range(len(self.omics_name_new)):\n            self.omics_dfs[self.omics_name_new[i]] = self.omics_dfs[self.omics_name_new[i]][_indices[0],:].sort(const.dkey.id)\n        labels_df = labels_df[_indices[1],:].sort(const.dkey.id)\n\n        self.sample_ids = intersect_ids\n        self.labels_df = labels_df\n        self.model_output_dim = dim_model_output\n\n    def _calc_n_samples2sample(self, target_n_samples: Optional[int], n_fragments: int):\n        r\"\"\"Calculate the number of samples to add to the dataset.\n        \"\"\"\n        n_samples_to_add = 0\n        match target_n_samples:\n            case None:\n                if n_fragments &gt; 1:\n                    if self.n_samples % n_fragments != 0:\n                        n_samples_to_add = n_fragments - (self.n_samples % n_fragments)\n            case x if x &gt; self.n_samples:\n                n_samples_to_add = x - self.n_samples\n            case _:\n                raise Warning(\"target_n_samples must be larger than the number of samples\")\n        self.n_samples_to_add = n_samples_to_add\n        self.n_samples_target = self.n_samples + n_samples_to_add\n\n    def _sample_new2add(self, seed_resample: int):\n        r\"\"\"Generate new samples by resampling existing samples.\n        \"\"\"\n        np.random.seed(seed_resample)\n        new_indices: list[int] = np.random.choice(self.n_samples, self.n_samples_to_add, replace=True).tolist()\n        self.n_samples = self.n_samples_target\n        self.sample_ids = self.sample_ids + [self.sample_ids[i] for i in new_indices]\n\n        if self.labels_df is not None:\n            self.labels_df = self.labels_df.vstack(self.labels_df[new_indices,:])\n\n        for ikey in self.omics_name:\n            self.omics_dfs[ikey] = self.omics_dfs[ikey].vstack(self.omics_dfs[ikey][new_indices,:])\n\n    def _proc_omics(self, sample_ind_for_proc: Optional[List[int]], dir_preprocessors: str, reproduction_mode: bool):\n        r\"\"\"Preprocess omics data.\n        \"\"\"\n        if reproduction_mode:\n            if os.path.exists(dir_preprocessors):\n                for ikey in self.omics_name:\n                    if self.key_gv is not None and self.key_gv == ikey:\n                        continue\n                    _loaded_proc = ProcOnTrainSet(self.omics_dfs[ikey], None)\n                    _loaded_proc.load_run_preprocessors(dir_preprocessors, f'preprocessors_for_omics_{ikey}.pkl')\n                    self.omics_dfs[ikey] = _loaded_proc._df\n            else:\n                raise FileNotFoundError(f\"Processor files for omics data are not found in {dir_preprocessors}\")\n        else:\n            for ikey in self.omics_name:\n                if self.key_gv is not None and self.key_gv == ikey:\n                    continue\n                _tmp_proc = ProcOnTrainSet(self.omics_dfs[ikey], sample_ind_for_proc, self.save_n_feat, self.labels_df)\n                _tmp_proc.pr_impute(strategy=\"mean\")\n                print(f\"\\nDone imputation for {ikey}\")\n                _tmp_proc.pr_var(threshold=self.variance_threshold)\n                print(f\"\\nDone variance filtering for {ikey}\")\n                _tmp_proc.pr_minmax()\n                print(f\"\\nDone min-max scaling for {ikey}\")\n                _tmp_proc.pr_rf(self.random_states, self.n_estimators, self.n_jobs_rf)\n                print(f\"\\nDone random forest filtering for {ikey}\")\n                _tmp_proc.save_preprocessors(dir_preprocessors, f'preprocessors_for_omics_{ikey}.pkl')\n                self.omics_dfs[ikey] = _tmp_proc._df\n\n    def _proc_labels(self, sample_ind_for_proc: Optional[List[int]], dir_preprocessors: str, reproduction_mode: bool):\n        r\"\"\"Preprocess labels.\n        \"\"\"\n        if self.labels_df is not None:\n            if reproduction_mode:\n                if os.path.exists(dir_preprocessors):\n                    labels_processor = ProcOnTrainSet(self.labels_df, None)\n                    labels_processor.load_run_preprocessors(dir_preprocessors, 'preprocessors_for_labels.pkl')\n                    self.labels_df = labels_processor._df\n                else:\n                    raise FileNotFoundError(f\"Processor files for labels are not found in {dir_preprocessors}\")\n            else:\n                labels_processor = ProcOnTrainSet(self.labels_df, sample_ind_for_proc)\n                labels_processor.pr_impute(strategy=\"mean\")\n                # labels_processor.pr_var(threshold=const.default.variance_threshold)\n                # labels_processor.pr_minmax()\n                labels_processor.pr_zscore()\n                labels_processor.save_preprocessors(dir_preprocessors, 'preprocessors_for_labels.pkl')\n                self.labels_df = labels_processor._df\n                self.label_data = self.labels_df.drop(const.dkey.id).to_numpy().astype(np.float32)\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataset.__init__","title":"<code>__init__(reproduction_mode, paths_omics, path_label=None, col2use_in_label=None, sample_ind_for_preproc=None, dir_preprocessors=None, target_n_samples=None, seed_resample=const.default.seed_1, n_fragments=1, prepr_labels=True, prepr_omics=True, snp_onehot_bits=const.default.snp_onehot_bits, which_outer_test=None, which_inner_val=None, variance_threshold=const.default.variance_threshold, save_n_feat=const.default.n_feat2save, n_estimators=const.default.n_estimators, random_states=const.default.random_states, n_jobs_rf=const.default.n_jobs_rf)</code>","text":"<p>Read data for litdata optimization. Preprocessing is optional.</p> <p>If you have labels (phenotypes) and want to use them, the input labels (phenotypes) are expected as follows:</p> <ul> <li>For REGRESSION task<ul> <li>Please keep original values that are not preprocessed.</li> <li>If you have MULTIPLE traits, please set different columns names in CSV file.</li> <li>The pipeline standardize/normalize data AFTER splitting to avoid data leakage.</li> <li>The method and parameters of standardization/normalization are kept the same as those in training data.</li> </ul> </li> <li>For CLASSIFICATION task<ul> <li>Please transform labels by one-hot encoder MANUALLY BEFORE input.</li> <li>The length of one-hot vectors is recommended to be n_categories + 1 for UNPRECEDENTED labels.</li> <li>If you have MULTIPLE traits, please concatenate one-hot encoded matrix along the horizontal axis before input.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>reproduction_mode</code> <code>bool</code> <p>Whether to use existing processors. Please provide <code>dir_preprocessors</code> if <code>reproduction_mode</code> is <code>True</code>.</p> required <code>paths_omics</code> <code>Dict[str, str]</code> <p>The <code>Dict</code> <code>{name: path}</code> of paths to multiple <code>.csv</code> or <code>.parquet</code> files or directories. For genotypes, it is a path to a <code>.pkl.gz</code> file. If some paths are directories, the files inside will be read as existing data, and <code>target_n_samples</code> will be ignored.</p> required <code>path_label</code> <code>Optional[str]</code> <p>The path to label data (a <code>.csv</code> or <code>.parquet</code> file). If it is <code>None</code>, no labels(phenotypes) are provided.</p> <code>None</code> <code>col2use_in_label</code> <code>Optional[Union[List[str], List[int]]]</code> <p>The columns to use in label data. If it is <code>List[int]</code>, its numbers are the indices (1-based) of the columns to be used. If it is <code>None</code>, all columns are used.</p> <code>None</code> <code>sample_ind_for_preproc</code> <code>Optional[List[int]]</code> <p>The indices for selecting samples for preprocessors fitting. If it is <code>None</code>, all samples are used. It is ignored when existing data are used.</p> <code>None</code> <code>dir_preprocessors</code> <code>Optional[str]</code> <p>The directory used to save data processors that fitted on training data. If it is <code>None</code>, preprocessing is not performed.</p> <code>None</code> <code>target_n_samples</code> <code>Optional[int]</code> <p>The target sample size for expanding the dataset through random sampling. Resampling is not performed when existing data are used.</p> <code>None</code> <code>seed_resample</code> <code>int</code> <p>The random seed for sampling new samples. Default: <code>const.default.seed_1</code>.</p> <code>seed_1</code> <code>n_fragments</code> <code>int</code> <p>The number of fragments (= k_outer * k_inner). Default: <code>1</code>.</p> <code>1</code> <code>prepr_labels</code> <code>bool</code> <p>Whether to preprocess labels or not. Default: <code>True</code>.</p> <code>True</code> <code>prepr_omics</code> <code>bool</code> <p>Whether to preprocess omics data or not. Default: <code>True</code>.</p> <code>True</code> <code>snp_onehot_bits</code> <code>int</code> <p>The number of bits for one-hot encoding SNPs. Default: <code>const.default.snp_onehot_bits</code>.</p> <code>snp_onehot_bits</code> <code>which_outer_test</code> <code>Optional[int]</code> <p>The index of outer test set. It is used for reading existing data.</p> <code>None</code> <code>which_inner_val</code> <code>Optional[int]</code> <p>The index of inner validation set. It is used for reading existing data.</p> <code>None</code> <code>variance_threshold</code> <code>float</code> <p>The threshold for variance selection.</p> <code>variance_threshold</code> <code>save_n_feat</code> <code>int</code> <p>The number of features to save when random forest selection is performed. Default: <code>const.default.n_feat2save</code>.</p> <code>n_feat2save</code> <code>n_estimators</code> <code>int</code> <p>The number of trees in the random forest. Default: <code>const.default.n_estimators</code>.</p> <code>n_estimators</code> <code>random_states</code> <code>List[int]</code> <p>The random states for random forest selection. Default: <code>const.default.random_states</code>.</p> <code>random_states</code> <code>n_jobs_rf</code> <code>int</code> <p>The number of jobs for parallelized random forest fitting. Default: <code>const.default.n_jobs_rf</code>.</p> <code>n_jobs_rf</code> <p>Usage:</p> <pre><code>&gt;&gt;&gt; from biodem.utils.data_ncv import DEMDataset\n&gt;&gt;&gt; _dataset = DEMDataset(...)\n&gt;&gt;&gt; _dataset._setup()\n</code></pre> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def __init__(\n        self,\n        reproduction_mode: bool,\n        paths_omics: Dict[str, str],\n        path_label: Optional[str] = None,\n        col2use_in_label: Optional[Union[List[str], List[int]]] = None,\n        sample_ind_for_preproc: Optional[List[int]] = None,\n        dir_preprocessors: Optional[str] = None,\n        target_n_samples: Optional[int] = None,\n        seed_resample: int = const.default.seed_1,\n        n_fragments: int = 1,\n        prepr_labels: bool = True,\n        prepr_omics: bool = True,\n        snp_onehot_bits: int = const.default.snp_onehot_bits,\n        which_outer_test: Optional[int] = None,\n        which_inner_val: Optional[int] = None,\n        variance_threshold: float = const.default.variance_threshold,\n        save_n_feat: int = const.default.n_feat2save,\n        n_estimators: int = const.default.n_estimators,\n        random_states: List[int] = const.default.random_states,\n        n_jobs_rf: int = const.default.n_jobs_rf,\n    ) -&gt; None:\n    r\"\"\"Read data for litdata optimization. Preprocessing is optional.\n\n    If you have labels (phenotypes) and want to use them,\n    the input labels (phenotypes) are expected as follows:\n\n    + For **REGRESSION** task\n        + Please keep original values that are not preprocessed.\n        + If you have MULTIPLE traits, please set different columns names in CSV file.\n        + The pipeline ***standardize/normalize data AFTER splitting*** to ***avoid data leakage.***\n        + The method and parameters of standardization/normalization are kept the same as those in training data.\n    + For **CLASSIFICATION** task\n        + Please transform labels by one-hot encoder ***MANUALLY BEFORE input***.\n        + The length of one-hot vectors is recommended to be **n_categories + 1** for **UNPRECEDENTED labels**.\n        + If you have MULTIPLE traits, please **concatenate** one-hot encoded matrix along the horizontal axis before input.\n\n    Args:\n        reproduction_mode: Whether to use existing processors.\n            Please provide ``dir_preprocessors`` if ``reproduction_mode`` is ``True``.\n\n        paths_omics: The ``Dict`` ``{name: path}`` of paths to multiple ``.csv`` or ``.parquet`` files or directories. For genotypes, it is a path to a ``.pkl.gz`` file.\n            If some paths are directories, the files inside will be read as existing data, and ``target_n_samples`` will be ignored.\n\n        path_label: The path to label data (a ``.csv`` or ``.parquet`` file).\n            If it is `None`, no labels(phenotypes) are provided.\n\n        col2use_in_label: The columns to use in label data.\n            If it is `List[int]`, its numbers are the indices **(1-based)** of the columns to be used.\n            If it is `None`, all columns are used.\n\n        sample_ind_for_preproc: The indices for selecting samples for preprocessors fitting.\n            If it is `None`, all samples are used.\n            It is ignored when existing data are used.\n\n        dir_preprocessors: The directory used to save data processors that fitted on training data.\n            If it is `None`, preprocessing is not performed.\n\n        target_n_samples: The target sample size for expanding the dataset through random sampling.\n            Resampling is not performed when existing data are used.\n\n        seed_resample: The random seed for sampling new samples.\n            Default: ``const.default.seed_1``.\n\n        n_fragments: The number of fragments (= k_outer * k_inner).\n            Default: ``1``.\n\n        prepr_labels: Whether to preprocess labels or not.\n            Default: ``True``.\n\n        prepr_omics: Whether to preprocess omics data or not.\n            Default: ``True``.\n\n        snp_onehot_bits: The number of bits for one-hot encoding SNPs.\n            Default: ``const.default.snp_onehot_bits``.\n\n        which_outer_test: The index of outer test set. It is used for reading existing data.\n\n        which_inner_val: The index of inner validation set. It is used for reading existing data.\n\n        variance_threshold: The threshold for variance selection.\n\n        save_n_feat: The number of features to save when random forest selection is performed.\n            Default: ``const.default.n_feat2save``.\n\n        n_estimators: The number of trees in the random forest.\n            Default: ``const.default.n_estimators``.\n\n        random_states: The random states for random forest selection.\n            Default: ``const.default.random_states``.\n\n        n_jobs_rf: The number of jobs for parallelized random forest fitting.\n            Default: ``const.default.n_jobs_rf``.\n\n    Usage:\n\n        &gt;&gt;&gt; from biodem.utils.data_ncv import DEMDataset\n        &gt;&gt;&gt; _dataset = DEMDataset(...)\n        &gt;&gt;&gt; _dataset._setup()\n\n    \"\"\"\n    super().__init__()\n    self.reproduction_mode = reproduction_mode\n    self.paths_omics = paths_omics\n    self.path_label = path_label\n    self.col2use_in_label = col2use_in_label\n\n    self.sample_ind_for_preproc = sample_ind_for_preproc\n    self.dir_preprocessors = dir_preprocessors\n\n    self.prepr_labels = prepr_labels\n    self.prepr_omics = prepr_omics\n    self.variance_threshold = variance_threshold\n    self.save_n_feat = save_n_feat\n    self.n_estimators = n_estimators\n    self.random_states = random_states\n    self.n_jobs_rf = n_jobs_rf\n\n    self.snp_onehot_bits = snp_onehot_bits\n\n    self.target_n_samples = target_n_samples\n    self.seed_resample = seed_resample\n    self.n_fragments = n_fragments\n\n    self.which_outer_test = which_outer_test\n    self.which_inner_val = which_inner_val\n\n    self.existing_omics_sample_id: Dict[str, Dict[str, List[str]]] = {}\n    self.existing_omics: Dict[str, Dict[str, pl.DataFrame]] = {}\n\n    self.omics_name = sorted(list(paths_omics.keys()))\n    self.omics_name_new: List[str] = []\n    self.omics_name_existing: List[str] = []\n    self.indices_trn: List[int] | None = None\n    self.indices_val: List[int] | None = None\n    self.indices_tst: List[int] | None = None\n\n    self.key_gv = None\n    self.n_omics = len(self.omics_name)\n    self.omics_dfs: Dict[str, pl.DataFrame] = {}\n\n    self.sample_ids: List[str] = []\n    self.labels_df = None\n    self.dim_model_output = None\n\n    self.omics_data: List[np.ndarray] = []\n    self.omics_features: List[List[str]] = []\n    self.omics_dims: List[int] = []\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.DEMDataset.recommend_index_by_existing_omics","title":"<code>recommend_index_by_existing_omics()</code>","text":"<p>Recommend indices for the samples that are shared in all omics if existing data is used.</p> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def recommend_index_by_existing_omics(self):\n    r\"\"\"Recommend indices for the samples that are shared in all omics if existing data is used.\n    \"\"\"\n    if len(self.omics_name_existing) &lt; 1:\n        return None\n    inters_trn, indices_trn = intersect_lists([self.sample_ids, *[self.existing_omics_sample_id[ikey][const.abbr_train] for ikey in self.omics_name_existing]])\n    inters_val, indices_val = intersect_lists([self.sample_ids, *[self.existing_omics_sample_id[ikey][const.abbr_val] for ikey in self.omics_name_existing]])\n    inters_tst, indices_tst = intersect_lists([self.sample_ids, *[self.existing_omics_sample_id[ikey][const.abbr_test] for ikey in self.omics_name_existing]])\n\n    for i in range(len(self.omics_name_existing)):\n        _tmp_part_trn = self.existing_omics[self.omics_name_existing[i]][const.abbr_train][indices_trn[i+1], :]\n        _tmp_part_val = self.existing_omics[self.omics_name_existing[i]][const.abbr_val][indices_val[i+1], :]\n        _tmp_part_tst = self.existing_omics[self.omics_name_existing[i]][const.abbr_test][indices_tst[i+1], :]\n        self.omics_dfs[self.omics_name_existing[i]] = _tmp_part_trn.vstack(_tmp_part_val).vstack(_tmp_part_tst)\n\n    for i in range(len(self.omics_name_new)):\n        _tmp_part_trn = self.omics_dfs[self.omics_name_new[i]][indices_trn[0],:]\n        _tmp_part_val = self.omics_dfs[self.omics_name_new[i]][indices_val[0],:]\n        _tmp_part_tst = self.omics_dfs[self.omics_name_new[i]][indices_tst[0],:]\n        self.omics_dfs[self.omics_name_new[i]] = _tmp_part_trn.vstack(_tmp_part_val).vstack(_tmp_part_tst)\n\n    if self.labels_df is not None:\n        self.labels_df = self.labels_df[indices_trn[0]+indices_val[0]+indices_tst[0],:]\n\n    n_id_trn = len(inters_trn)\n    n_id_val = len(inters_val)\n    n_id_tst = len(inters_tst)\n    self.indices_trn = [i for i in range(n_id_trn)]\n    self.indices_val = [i+n_id_trn for i in range(n_id_val)]\n    self.indices_tst = [i+n_id_trn+n_id_val for i in range(n_id_tst)]\n\n    self.sample_ids = inters_trn + inters_val + inters_tst\n    self.sample_ids_trn = inters_trn\n    self.sample_ids_val = inters_val\n    self.sample_ids_tst = inters_tst\n\n    self.n_samples = len(self.sample_ids)\n    self.sample_ind_for_preproc = self.indices_trn\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.Dict2Dataset","title":"<code>Dict2Dataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>class Dict2Dataset(Dataset):\n    def __init__(self, data_dict: Dict[str, Any]):\n        r\"\"\"Transform a dict of data to a Dataset for shuffled omics feature values.\n\n        Args:\n            data_dict: dict of data\n\n        \"\"\"\n        super().__init__()\n        self.data_dict = data_dict\n\n    def __len__(self):\n        return len(self.data_dict[const.dkey.litdata_index])\n\n    def __getitem__(self, idx: int):\n        _omics = [om_x[idx, :] for om_x in self.data_dict[const.dkey.litdata_omics]]\n        _id = self.data_dict[const.dkey.litdata_id][idx]\n        _idx = idx\n        if const.dkey.litdata_label in self.data_dict:\n            _labels = self.data_dict[const.dkey.litdata_label][idx]\n            return {const.dkey.litdata_index: _idx, const.dkey.litdata_omics: _omics, const.dkey.litdata_id: _id, const.dkey.litdata_label: _labels}\n        else:\n            return {const.dkey.litdata_index: _idx, const.dkey.litdata_omics: _omics, const.dkey.litdata_id: _id}\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.Dict2Dataset.__init__","title":"<code>__init__(data_dict)</code>","text":"<p>Transform a dict of data to a Dataset for shuffled omics feature values.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Any]</code> <p>dict of data</p> required Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def __init__(self, data_dict: Dict[str, Any]):\n    r\"\"\"Transform a dict of data to a Dataset for shuffled omics feature values.\n\n    Args:\n        data_dict: dict of data\n\n    \"\"\"\n    super().__init__()\n    self.data_dict = data_dict\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.OptimizeLitdataNCV","title":"<code>OptimizeLitdataNCV</code>","text":"Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>class OptimizeLitdataNCV:\n    def __init__(\n            self,\n            paths_omics: Dict[str, str],\n            path_label: Optional[str],\n            output_dir: str,\n            k_outer: int,\n            k_inner: int,\n            fragment_elem_ids: Optional[List[List[int]]] = None,\n            which_outer_inner: Optional[List[int]] = None,\n            col2use_in_labels: Optional[Union[List[str], List[int]]] = None,\n            prepr_labels: bool = True,\n            prepr_omics: bool = True,\n            seed_permut: int = const.default.seed_1,\n            seed_resample: int = const.default.seed_2,\n            compression: Optional[str] = const.default.compression_alg,\n            n_workers: int = const.default.n_workers_litdata,\n            variance_threshold: float = const.default.variance_threshold,\n            save_n_feat: int = const.default.n_feat2save,\n            n_estimators: int = const.default.n_estimators,\n            random_states: List[int] = const.default.random_states,\n            n_jobs_rf: int = const.default.n_jobs_rf,\n        ):\n        r\"\"\"Optimize the data for nested cross validation.\n\n        Args:\n            paths_omics: The ``Dict`` ``{name: path}`` of paths to multiple ``.csv`` or ``.parquet`` files or directories. For genotypes, it is a path to a ``.pkl.gz`` file.\n                If some paths are directories, the files inside will be read as existing data.\n\n            path_label: The path to label data (a ``.csv`` or ``.parquet`` file).\n                If it is `None`, no labels(phenotypes) are provided.\n\n            output_dir: The directory to save optimized data.\n\n            k_outer: Number of outer folds.\n\n            k_inner: Number of inner folds.\n\n            fragment_elem_ids: List of list of indices of elements in each fragment.\n                It is optional when the data is already split into fragments.\n                It overrides ``seed_permut` and disables random permutation.\n                For nested cross validation with 10 outer folds and 5 inner folds, it is a list of 50 lists of indices.\n\n            which_outer_inner: If specified, only the specified outer-inner fold will be optimized.\n\n            col2use_in_labels: The columns to use in label data.\n                If it is `List[int]`, its numbers are the indices **(1-based)** of the columns to be used.\n                If it is `None`, all columns are used.\n\n            prepr_labels: Whether to preprocess labels.\n                Default: ``True``.\n\n            prepr_omics: Whether to preprocess omics.\n                Default: ``True``.\n\n            seed_permut: Seed for permutation.\n                Default: ``const.default.seed_1``.\n\n            seed_resample: The random seed for sampling new samples for the target number of samples.\n                Default: ``const.default.seed_2``.\n\n            compression: Compression method.\n                Default: ``const.default.compression_alg``.\n\n            n_workers: Number of workers.\n                Default: ``const.default.n_workers_litdata``.\n\n            variance_threshold: The threshold for variance selection.\n                Default: ``const.default.variance_threshold``.\n\n            save_n_feat: The number of features to save when random forest selection is performed.\n                Default: ``const.default.n_feat2save``.\n\n            n_estimators: The number of trees in the random forest.\n                Default: ``const.default.n_estimators``.\n\n            random_states: The random states for random forest selection.\n                Default: ``const.default.random_states``.\n\n            n_jobs_rf: The number of jobs for parallelized random forest fitting.\n                Default: ``const.default.n_jobs_rf``.\n\n        Usage:\n\n            &gt;&gt;&gt; from biodem.utils.data_ncv import OptimizeLitdataNCV\n            &gt;&gt;&gt; prep_data_ncv = OptimizeLitdataNCV(...)\n            &gt;&gt;&gt; prep_data_ncv.run_optimization()\n\n        \"\"\"\n        self.paths_omics = paths_omics\n        self.path_label = path_label\n        self.output_dir = output_dir\n        self.k_outer = k_outer\n        self.k_inner = k_inner\n        self.col2use_in_labels = col2use_in_labels\n        self.prepr_labels = prepr_labels\n        self.prepr_omics = prepr_omics\n\n        self.variance_threshold = variance_threshold\n        self.n_feat2save = save_n_feat\n        self.n_estimators = n_estimators\n        self.random_states = random_states\n        self.n_jobs_rf = n_jobs_rf\n\n        self.seed_resample = seed_resample\n        self.compression = compression\n        self.n_workers = n_workers\n        self.which_outer_inner = which_outer_inner\n        if which_outer_inner is not None:\n            if len(which_outer_inner) != 2:\n                raise ValueError(\"which_outer_inner must be a list of two elements\")\n            self.which_outer = which_outer_inner[0]\n            self.which_inner = which_outer_inner[1]\n        else:\n            self.which_outer = None\n            self.which_inner = None\n        self.fragments = self._check(seed_permut, fragment_elem_ids)\n        self.n_fragments = len(self.fragments)\n\n        self.litdata_cache_dir = os.path.join(output_dir, f\".cache_{random_string(9)}\")\n        os.environ[\"DATA_OPTIMIZER_CACHE_FOLDER\"] = self.litdata_cache_dir\n        # self.run_optimization()\n\n    def run_optimization(self):\n        if self.which_outer_inner is None:\n            combn_outer_inner = list(itertools.product(range(self.k_outer), range(self.k_inner)))\n            for xo, xi in combn_outer_inner:\n                self.optimize_xoxi(xo, xi)\n        else:\n            self.optimize_xoxi(*self.which_outer_inner)\n\n        if self.path_label is not None:\n            _, dim_model_output, _ = read_labels(self.path_label, self.col2use_in_labels)\n            df_output_dim = pl.DataFrame(data={const.dkey.model_output_dim: [dim_model_output]})\n            path_output_dim = os.path.join(self.output_dir, const.fname.output_dim)\n            if not os.path.exists(path_output_dim):\n                df_output_dim.write_csv(path_output_dim)\n\n        # Check if Genomic Variants are available in paths_omics\n        for px in self.paths_omics.values():\n            if px.endswith(\".pkl.gz\"):\n                path_cp_pklgz = os.path.join(self.output_dir, const.fname.genotypes)\n                if not os.path.exists(path_cp_pklgz):\n                    shutil.copy(px, path_cp_pklgz)\n\n        # Remove cache dir\n        shutil.rmtree(self.litdata_cache_dir)\n        return None\n\n    def _check(self, seed_permut: int, fragment_elem_ids: Optional[List[List[int]]]):\n        \"\"\"\n        Check if fragment_elem_ids is provided or not. If not, generate random fragments.\n        \"\"\"\n        if fragment_elem_ids is None:\n            n_fragments = int(self.k_outer * self.k_inner)\n            tmp_init = DEMDataset(\n                reproduction_mode=False,\n                paths_omics=self.paths_omics,\n                path_label=self.path_label,\n                col2use_in_label=self.col2use_in_labels,\n                sample_ind_for_preproc=None,\n                dir_preprocessors=None,\n                target_n_samples=None,\n                seed_resample=self.seed_resample,\n                n_fragments=n_fragments,\n                prepr_labels=False,\n                prepr_omics=False,\n                which_outer_test=self.which_outer,\n                which_inner_val=self.which_inner,\n                variance_threshold=self.variance_threshold,\n                save_n_feat=self.n_feat2save,\n                n_estimators=self.n_estimators,\n                random_states=self.random_states,\n                n_jobs_rf=self.n_jobs_rf,\n            )\n            tmp_init._setup()\n\n            np.random.seed(seed_permut)\n            _indices = np.random.permutation(len(tmp_init))\n            fragments = np.array_split(_indices, n_fragments)\n            fragments = [i.tolist() for i in fragments]\n        else:\n            fragments = fragment_elem_ids\n            n_fragments = len(fragments)\n            assert n_fragments == self.k_outer * self.k_inner\n\n        return fragments\n\n    def optimize_xoxi(self, which_outer_test: int, which_inner_val: int):\n        dir_xoxi = os.path.join(self.output_dir, f\"ncv_test_{which_outer_test}_val_{which_inner_val}\")\n        os.makedirs(dir_xoxi, exist_ok=True)\n        fr_indices_trn, fr_indices_val, fr_indices_test = get_indices_ncv(self.k_outer, self.k_inner, which_outer_test, which_inner_val)\n        ind_trn = np.concatenate([self.fragments[i] for i in fr_indices_trn]).tolist()\n        ind_val = np.concatenate([self.fragments[i] for i in fr_indices_val]).tolist()\n        ind_tst = np.concatenate([self.fragments[i] for i in fr_indices_test]).tolist()\n\n        dataset_xoxi = DEMDataset(\n            reproduction_mode=False,\n            paths_omics=self.paths_omics,\n            path_label=self.path_label,\n            col2use_in_label=self.col2use_in_labels,\n            sample_ind_for_preproc=ind_trn,\n            dir_preprocessors=dir_xoxi,\n            target_n_samples=None,\n            seed_resample=self.seed_resample,\n            n_fragments=self.n_fragments,\n            prepr_labels=self.prepr_labels,\n            prepr_omics=self.prepr_omics,\n            which_outer_test=which_outer_test,\n            which_inner_val=which_inner_val,\n            variance_threshold=self.variance_threshold,\n            save_n_feat=self.n_feat2save,\n            n_estimators=self.n_estimators,\n            random_states=self.random_states,\n            n_jobs_rf=self.n_jobs_rf,\n        )\n        dataset_xoxi._setup()\n        sample_ids = dataset_xoxi.sample_ids\n        if dataset_xoxi.indices_trn is not None:\n            ind_trn = dataset_xoxi.indices_trn\n        if dataset_xoxi.indices_val is not None:\n            ind_val = dataset_xoxi.indices_val\n        if dataset_xoxi.indices_tst is not None:\n            ind_tst = dataset_xoxi.indices_tst\n\n        # Write sample IDs\n        _df_ids = pl.DataFrame(sample_ids, schema=[const.dkey.id])\n        _df_ids.write_csv(os.path.join(dir_xoxi, const.fname.predata_ids))\n        _df_ids_trn = pl.DataFrame([sample_ids[i] for i in ind_trn], schema=[const.dkey.id])\n        _df_ids_trn.write_csv(os.path.join(dir_xoxi, const.fname.predata_ids_trn))\n        _df_ids_val = pl.DataFrame([sample_ids[i] for i in ind_val], schema=[const.dkey.id])\n        _df_ids_val.write_csv(os.path.join(dir_xoxi, const.fname.predata_ids_val))\n        _df_ids_tst = pl.DataFrame([sample_ids[i] for i in ind_tst], schema=[const.dkey.id])\n        _df_ids_tst.write_csv(os.path.join(dir_xoxi, const.fname.predata_ids_tst))\n\n        # Start optimizing\n        optimize(\n            fn = dataset_xoxi.__getitem__,\n            inputs = ind_trn,\n            output_dir = os.path.join(dir_xoxi, const.title_train),\n            chunk_bytes = const.default.chunk_bytes,\n            compression = self.compression,\n            num_workers = self.n_workers,\n        )\n        optimize(\n            fn = dataset_xoxi.__getitem__,\n            inputs = ind_val,\n            output_dir = os.path.join(dir_xoxi, const.title_val),\n            chunk_bytes = const.default.chunk_bytes,\n            compression = self.compression,\n            num_workers = self.n_workers,\n        )\n        optimize(\n            fn = dataset_xoxi.__getitem__,\n            inputs = ind_tst,\n            output_dir = os.path.join(dir_xoxi, const.title_test),\n            chunk_bytes = const.default.chunk_bytes,\n            compression = self.compression,\n            num_workers = self.n_workers,\n        )\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.OptimizeLitdataNCV.__init__","title":"<code>__init__(paths_omics, path_label, output_dir, k_outer, k_inner, fragment_elem_ids=None, which_outer_inner=None, col2use_in_labels=None, prepr_labels=True, prepr_omics=True, seed_permut=const.default.seed_1, seed_resample=const.default.seed_2, compression=const.default.compression_alg, n_workers=const.default.n_workers_litdata, variance_threshold=const.default.variance_threshold, save_n_feat=const.default.n_feat2save, n_estimators=const.default.n_estimators, random_states=const.default.random_states, n_jobs_rf=const.default.n_jobs_rf)</code>","text":"<p>Optimize the data for nested cross validation.</p> <p>Parameters:</p> Name Type Description Default <code>paths_omics</code> <code>Dict[str, str]</code> <p>The <code>Dict</code> <code>{name: path}</code> of paths to multiple <code>.csv</code> or <code>.parquet</code> files or directories. For genotypes, it is a path to a <code>.pkl.gz</code> file. If some paths are directories, the files inside will be read as existing data.</p> required <code>path_label</code> <code>Optional[str]</code> <p>The path to label data (a <code>.csv</code> or <code>.parquet</code> file). If it is <code>None</code>, no labels(phenotypes) are provided.</p> required <code>output_dir</code> <code>str</code> <p>The directory to save optimized data.</p> required <code>k_outer</code> <code>int</code> <p>Number of outer folds.</p> required <code>k_inner</code> <code>int</code> <p>Number of inner folds.</p> required <code>fragment_elem_ids</code> <code>Optional[List[List[int]]]</code> <p>List of list of indices of elements in each fragment. It is optional when the data is already split into fragments. It overrides <code>`seed_permut</code> and disables random permutation. For nested cross validation with 10 outer folds and 5 inner folds, it is a list of 50 lists of indices.</p> <code>None</code> <code>which_outer_inner</code> <code>Optional[List[int]]</code> <p>If specified, only the specified outer-inner fold will be optimized.</p> <code>None</code> <code>col2use_in_labels</code> <code>Optional[Union[List[str], List[int]]]</code> <p>The columns to use in label data. If it is <code>List[int]</code>, its numbers are the indices (1-based) of the columns to be used. If it is <code>None</code>, all columns are used.</p> <code>None</code> <code>prepr_labels</code> <code>bool</code> <p>Whether to preprocess labels. Default: <code>True</code>.</p> <code>True</code> <code>prepr_omics</code> <code>bool</code> <p>Whether to preprocess omics. Default: <code>True</code>.</p> <code>True</code> <code>seed_permut</code> <code>int</code> <p>Seed for permutation. Default: <code>const.default.seed_1</code>.</p> <code>seed_1</code> <code>seed_resample</code> <code>int</code> <p>The random seed for sampling new samples for the target number of samples. Default: <code>const.default.seed_2</code>.</p> <code>seed_2</code> <code>compression</code> <code>Optional[str]</code> <p>Compression method. Default: <code>const.default.compression_alg</code>.</p> <code>compression_alg</code> <code>n_workers</code> <code>int</code> <p>Number of workers. Default: <code>const.default.n_workers_litdata</code>.</p> <code>n_workers_litdata</code> <code>variance_threshold</code> <code>float</code> <p>The threshold for variance selection. Default: <code>const.default.variance_threshold</code>.</p> <code>variance_threshold</code> <code>save_n_feat</code> <code>int</code> <p>The number of features to save when random forest selection is performed. Default: <code>const.default.n_feat2save</code>.</p> <code>n_feat2save</code> <code>n_estimators</code> <code>int</code> <p>The number of trees in the random forest. Default: <code>const.default.n_estimators</code>.</p> <code>n_estimators</code> <code>random_states</code> <code>List[int]</code> <p>The random states for random forest selection. Default: <code>const.default.random_states</code>.</p> <code>random_states</code> <code>n_jobs_rf</code> <code>int</code> <p>The number of jobs for parallelized random forest fitting. Default: <code>const.default.n_jobs_rf</code>.</p> <code>n_jobs_rf</code> <p>Usage:</p> <pre><code>&gt;&gt;&gt; from biodem.utils.data_ncv import OptimizeLitdataNCV\n&gt;&gt;&gt; prep_data_ncv = OptimizeLitdataNCV(...)\n&gt;&gt;&gt; prep_data_ncv.run_optimization()\n</code></pre> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def __init__(\n        self,\n        paths_omics: Dict[str, str],\n        path_label: Optional[str],\n        output_dir: str,\n        k_outer: int,\n        k_inner: int,\n        fragment_elem_ids: Optional[List[List[int]]] = None,\n        which_outer_inner: Optional[List[int]] = None,\n        col2use_in_labels: Optional[Union[List[str], List[int]]] = None,\n        prepr_labels: bool = True,\n        prepr_omics: bool = True,\n        seed_permut: int = const.default.seed_1,\n        seed_resample: int = const.default.seed_2,\n        compression: Optional[str] = const.default.compression_alg,\n        n_workers: int = const.default.n_workers_litdata,\n        variance_threshold: float = const.default.variance_threshold,\n        save_n_feat: int = const.default.n_feat2save,\n        n_estimators: int = const.default.n_estimators,\n        random_states: List[int] = const.default.random_states,\n        n_jobs_rf: int = const.default.n_jobs_rf,\n    ):\n    r\"\"\"Optimize the data for nested cross validation.\n\n    Args:\n        paths_omics: The ``Dict`` ``{name: path}`` of paths to multiple ``.csv`` or ``.parquet`` files or directories. For genotypes, it is a path to a ``.pkl.gz`` file.\n            If some paths are directories, the files inside will be read as existing data.\n\n        path_label: The path to label data (a ``.csv`` or ``.parquet`` file).\n            If it is `None`, no labels(phenotypes) are provided.\n\n        output_dir: The directory to save optimized data.\n\n        k_outer: Number of outer folds.\n\n        k_inner: Number of inner folds.\n\n        fragment_elem_ids: List of list of indices of elements in each fragment.\n            It is optional when the data is already split into fragments.\n            It overrides ``seed_permut` and disables random permutation.\n            For nested cross validation with 10 outer folds and 5 inner folds, it is a list of 50 lists of indices.\n\n        which_outer_inner: If specified, only the specified outer-inner fold will be optimized.\n\n        col2use_in_labels: The columns to use in label data.\n            If it is `List[int]`, its numbers are the indices **(1-based)** of the columns to be used.\n            If it is `None`, all columns are used.\n\n        prepr_labels: Whether to preprocess labels.\n            Default: ``True``.\n\n        prepr_omics: Whether to preprocess omics.\n            Default: ``True``.\n\n        seed_permut: Seed for permutation.\n            Default: ``const.default.seed_1``.\n\n        seed_resample: The random seed for sampling new samples for the target number of samples.\n            Default: ``const.default.seed_2``.\n\n        compression: Compression method.\n            Default: ``const.default.compression_alg``.\n\n        n_workers: Number of workers.\n            Default: ``const.default.n_workers_litdata``.\n\n        variance_threshold: The threshold for variance selection.\n            Default: ``const.default.variance_threshold``.\n\n        save_n_feat: The number of features to save when random forest selection is performed.\n            Default: ``const.default.n_feat2save``.\n\n        n_estimators: The number of trees in the random forest.\n            Default: ``const.default.n_estimators``.\n\n        random_states: The random states for random forest selection.\n            Default: ``const.default.random_states``.\n\n        n_jobs_rf: The number of jobs for parallelized random forest fitting.\n            Default: ``const.default.n_jobs_rf``.\n\n    Usage:\n\n        &gt;&gt;&gt; from biodem.utils.data_ncv import OptimizeLitdataNCV\n        &gt;&gt;&gt; prep_data_ncv = OptimizeLitdataNCV(...)\n        &gt;&gt;&gt; prep_data_ncv.run_optimization()\n\n    \"\"\"\n    self.paths_omics = paths_omics\n    self.path_label = path_label\n    self.output_dir = output_dir\n    self.k_outer = k_outer\n    self.k_inner = k_inner\n    self.col2use_in_labels = col2use_in_labels\n    self.prepr_labels = prepr_labels\n    self.prepr_omics = prepr_omics\n\n    self.variance_threshold = variance_threshold\n    self.n_feat2save = save_n_feat\n    self.n_estimators = n_estimators\n    self.random_states = random_states\n    self.n_jobs_rf = n_jobs_rf\n\n    self.seed_resample = seed_resample\n    self.compression = compression\n    self.n_workers = n_workers\n    self.which_outer_inner = which_outer_inner\n    if which_outer_inner is not None:\n        if len(which_outer_inner) != 2:\n            raise ValueError(\"which_outer_inner must be a list of two elements\")\n        self.which_outer = which_outer_inner[0]\n        self.which_inner = which_outer_inner[1]\n    else:\n        self.which_outer = None\n        self.which_inner = None\n    self.fragments = self._check(seed_permut, fragment_elem_ids)\n    self.n_fragments = len(self.fragments)\n\n    self.litdata_cache_dir = os.path.join(output_dir, f\".cache_{random_string(9)}\")\n    os.environ[\"DATA_OPTIMIZER_CACHE_FOLDER\"] = self.litdata_cache_dir\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.optimize_data_external","title":"<code>optimize_data_external(output_dir, paths_omics, path_label=None, col2use_in_labels=None, prepr_labels=True, prepr_omics=True, reproduction_mode=False, dir_preprocessors=None, compression=const.default.compression_alg, n_workers=const.default.n_workers_litdata, chunk_bytes=const.default.chunk_bytes)</code>","text":"<p>Optimize data for external data.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>The directory to save optimized data.</p> required <code>paths_omics</code> <code>Dict[str, str]</code> <p>The <code>Dict</code> <code>{name: path}</code> of paths to multiple <code>.csv</code> or <code>.parquet</code> files or directories. For genotypes, it is a path to a <code>.pkl.gz</code> file. If some paths are directories, the files inside will be read as existing data.</p> required <code>path_label</code> <code>Optional[str]</code> <p>The path to label data (a <code>.csv</code> or <code>.parquet</code> file). If it is <code>None</code>, no labels(phenotypes) are provided.</p> <code>None</code> <code>col2use_in_labels</code> <code>Optional[Union[List[str], List[int]]]</code> <p>The columns to use in label data. If it is <code>List[int]</code>, its numbers are the indices (1-based) of the columns to be used. If it is <code>None</code>, all columns are used.</p> <code>None</code> <code>prepr_labels</code> <code>bool</code> <p>Whether to preprocess labels. Default: <code>True</code>.</p> <code>True</code> <code>prepr_omics</code> <code>bool</code> <p>Whether to preprocess omics. Default: <code>True</code>.</p> <code>True</code> <code>reproduction_mode</code> <code>bool</code> <p>Whether to use existing processors. Please provide <code>dir_preprocessors</code> if <code>reproduction_mode</code> is <code>True</code>.</p> <code>False</code> <code>dir_preprocessors</code> <code>Optional[str]</code> <p>The directory used to save data processors that fitted on training data. If it is <code>None</code>, preprocessing is not performed.</p> <code>None</code> <code>compression</code> <code>Optional[str]</code> <p>Compression method. Default: <code>const.default.compression_alg</code>.</p> <code>compression_alg</code> <code>n_workers</code> <code>int</code> <p>Number of workers. Default: <code>const.default.n_workers_litdata</code>.</p> <code>n_workers_litdata</code> <code>chunk_bytes</code> <code>str</code> <p>Chunk size. Default: <code>const.default.chunk_bytes</code>.</p> <code>chunk_bytes</code> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def optimize_data_external(\n        output_dir: str,\n        paths_omics: Dict[str, str],\n        path_label: Optional[str] = None,\n        col2use_in_labels: Optional[Union[List[str], List[int]]] = None,\n        prepr_labels: bool = True,\n        prepr_omics: bool = True,\n        reproduction_mode: bool = False,\n        dir_preprocessors: Optional[str] = None,\n        compression: Optional[str] = const.default.compression_alg,\n        n_workers: int = const.default.n_workers_litdata,\n        chunk_bytes: str = const.default.chunk_bytes,\n    ):\n    r\"\"\"Optimize data for external data.\n\n    Args:\n        output_dir: The directory to save optimized data.\n\n        paths_omics: The ``Dict`` ``{name: path}`` of paths to multiple ``.csv`` or ``.parquet`` files or directories. For genotypes, it is a path to a ``.pkl.gz`` file.\n            If some paths are directories, the files inside will be read as existing data.\n\n        path_label: The path to label data (a ``.csv`` or ``.parquet`` file).\n            If it is `None`, no labels(phenotypes) are provided.\n\n        col2use_in_labels: The columns to use in label data.\n            If it is `List[int]`, its numbers are the indices **(1-based)** of the columns to be used.\n            If it is `None`, all columns are used.\n\n        prepr_labels: Whether to preprocess labels.\n            Default: ``True``.\n\n        prepr_omics: Whether to preprocess omics.\n            Default: ``True``.\n\n        reproduction_mode: Whether to use existing processors.\n            Please provide ``dir_preprocessors`` if ``reproduction_mode`` is ``True``.\n\n        dir_preprocessors: The directory used to save data processors that fitted on training data.\n            If it is `None`, preprocessing is not performed.\n\n        compression: Compression method.\n            Default: ``const.default.compression_alg``.\n\n        n_workers: Number of workers.\n            Default: ``const.default.n_workers_litdata``.\n\n        chunk_bytes: Chunk size.\n            Default: ``const.default.chunk_bytes``.\n\n    \"\"\"\n    dataset_ext = DEMDataset(\n        reproduction_mode=reproduction_mode,\n        paths_omics=paths_omics,\n        path_label=path_label,\n        col2use_in_label=col2use_in_labels,\n        dir_preprocessors=dir_preprocessors,\n        prepr_labels=prepr_labels,\n        prepr_omics=prepr_omics,\n    )\n    dataset_ext._setup()\n    optimize(\n        fn = dataset_ext.__getitem__,\n        inputs = range(len(dataset_ext)),\n        output_dir = output_dir,\n        chunk_bytes = chunk_bytes,\n        compression = compression,\n        num_workers = n_workers,\n    )\n</code></pre>"},{"location":"reference/biodem.utils.data_ncv/#biodem.utils.data_ncv.read_omics_names","title":"<code>read_omics_names(litdata_dir, get_path=False)</code>","text":"<p>Read omics names from the parent directory of the litdata.</p> <p>Parameters:</p> Name Type Description Default <code>litdata_dir</code> <code>str</code> <p>Path to the litdata directory.</p> required Output <p>A sorted list of omics names.</p> Source code in <code>src\\biodem\\utils\\data_ncv.py</code> <pre><code>def read_omics_names(litdata_dir: str, get_path: bool = False):\n    r\"\"\"Read omics names from the parent directory of the litdata.\n\n    Args:\n        litdata_dir: Path to the litdata directory.\n\n    Output:\n        A sorted list of omics names.\n\n    \"\"\"\n    _dir_trnvaltst = os.path.dirname(litdata_dir)\n    omics_names: List[str] = []\n    omics_paths: List[str] = []\n    for _fname in os.listdir(_dir_trnvaltst):\n        if _fname.startswith(const.fname.predata_omics_features_prefix):\n            _tmp = os.path.splitext(_fname)[0]\n            _tmp = _tmp.removeprefix(const.fname.predata_omics_features_prefix)\n            _tmp = _tmp.removeprefix(\"_\")\n            omics_names.append(_tmp)\n            omics_paths.append(os.path.join(_dir_trnvaltst, _fname))\n    if len(omics_names) == 0:\n        raise ValueError(\"No omics data found in the specified directory.\")\n\n    sortperm = np.argsort(omics_names)\n    omics_names = np.take(omics_names, sortperm).tolist()\n    omics_paths = np.take(omics_paths, sortperm).tolist()\n\n    if get_path:\n        return omics_names, omics_paths\n    else:\n        return omics_names\n</code></pre>"},{"location":"reference/biodem.utils.uni/","title":"biodem.utils.uni","text":""},{"location":"reference/biodem.utils.uni/#biodem.utils.uni","title":"<code>biodem.utils.uni</code>","text":"<p>Some universal functions.</p>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog","title":"<code>CollectFitLog</code>","text":"Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>class CollectFitLog:\n    def __init__(self, dir_log: str):\n        r\"\"\"Collect training logs from optuna db files and ckpt files.\n\n        Args:\n            dir_log: Directory containing the model fitting logs.\n\n        \"\"\"\n        self.dir_log = dir_log\n        if not os.path.exists(self.dir_log):\n            raise ValueError(f'Directory {self.dir_log} does not exist.')\n\n    def get_df_csv(self, dir_output: str, overwrite_collected_log: bool = False):\n        r\"\"\"Collect trained models for each fold in nested cross-validation.\n\n        Args:\n            dir_output: Directory to save the collected logs.\n\n            overwrite_collected_log: Whether to overwrite existing collected logs.\n\n        \"\"\"\n        collected_logs = self.collect()\n\n        models_bv = collected_logs[const.dkey.best_trials]\n        path_log_best_trials = os.path.join(dir_output, const.fname.log_best_trials)\n        if os.path.exists(path_log_best_trials) and not overwrite_collected_log:\n            models_bv = pl.read_csv(path_log_best_trials)\n        else:\n            models_bv.write_csv(path_log_best_trials)\n\n        models_bi = collected_logs[const.dkey.best_inner_folds]\n        path_log_best_inners = os.path.join(dir_output, const.fname.log_best_inners)\n        if os.path.exists(path_log_best_inners) and not overwrite_collected_log:\n            models_bi = pl.read_csv(path_log_best_inners)\n        else:\n            models_bi.write_csv(path_log_best_inners)\n\n        return models_bv, models_bi\n\n    def collect(self) -&gt; Dict[str, pl.DataFrame]:\n        r\"\"\"Collect training logs from optuna db files and ckpt files.\n        \"\"\"\n        best_trials_df, all_ckpt = self.collect_ckpt()\n        optuna_best_inners_df = self.collect_optuna_db()\n\n        # Merge the two dataframes on the const.dkey.which_outer and const.dkey.which_inner columns\n        logs_df = optuna_best_inners_df.join(best_trials_df, on=[const.dkey.which_outer, const.dkey.which_inner], how='left')\n        # Remove the const.title_val_loss column from the merged dataframe\n        logs_df = logs_df.drop(const.title_val_loss)\n        # Rename 'min_loss' column to const.title_val_loss\n        logs_df = logs_df.rename({const.dkey.min_loss: const.title_val_loss})\n        # Sort the dataframe by const.dkey.which_outer and const.dkey.which_inner\n        logs_df = logs_df.sort([const.dkey.which_outer, const.dkey.which_inner])\n\n        best_inners_df = logs_df.group_by(const.dkey.which_outer).agg(pl.col(const.title_tst_loss).min()).join(logs_df, on=[const.dkey.which_outer, const.title_tst_loss], how='left')\n        best_inners_df = best_inners_df.sort([const.dkey.which_outer, const.title_tst_loss])\n\n        print(\"\\nFound model logs:\")\n        print(logs_df)\n        print(\"\\nBest inner folds:\")\n        print(best_inners_df)\n\n        return {const.dkey.best_trials: logs_df, const.dkey.best_inner_folds: best_inners_df}\n\n    def collect_ckpt(self):\n        r\"\"\"Collect info from ckpt files and tensorboard events.\n        \"\"\"\n        paths_ckpt = self.search_ckpt()\n\n        # Pick ids of outer and inner folds, val_loss and version from ckpt file paths\n        test_loss_values = [self.read_tensorboard_events(os.path.join(os.path.dirname(path_x), \"version_0\")) for path_x in paths_ckpt]\n        val_loss_values = [float(os.path.basename(path_x).split('-')[3].split('=')[1].split('.ckpt')[0]) for path_x in paths_ckpt]\n        trial_tags = [path_x.split(os.path.sep)[-2] for path_x in paths_ckpt]\n        study_tags = [path_x.split(os.path.sep)[-4].split('_')[-1] for path_x in paths_ckpt]\n        ncv_inner_x = [int(path_x.split(os.path.sep)[-3].split('_')[-1]) for path_x in paths_ckpt]\n        ncv_outer_x = [int(path_x.split(os.path.sep)[-3].split('_')[-2]) for path_x in paths_ckpt]\n\n        # Create a dataframe with the above values\n        ckpt_df = pl.DataFrame({const.dkey.which_outer: ncv_outer_x, const.dkey.which_inner: ncv_inner_x, const.title_tst_loss: test_loss_values, const.title_val_loss: val_loss_values, const.dkey.trial_tag: trial_tags, const.dkey.study_tag: study_tags, const.dkey.ckpt_path: paths_ckpt})\n\n        # Pick the best model based on val_loss between the trials of the same outer and inner fold\n        best_trials_df = ckpt_df.group_by([const.dkey.which_outer, const.dkey.which_inner]).agg([pl.col(const.title_val_loss).min()]).join(ckpt_df, on=[const.dkey.which_outer, const.dkey.which_inner, const.title_val_loss], how='left')\n\n        return best_trials_df, ckpt_df\n\n    def collect_optuna_db(self):\n        r\"\"\"Collect info of optuna db files.\n        \"\"\"\n        # Find all optuna db files in the directory `dir_log` and its subdirectories\n        paths_optuna_db = [os.path.join(dirpath, f)\n                    for dirpath, dirnames, files in os.walk(self.dir_log)\n                    for f in files if f.endswith('.db')\n        ]\n        if len(paths_optuna_db) == 0:\n            raise FileNotFoundError('No optuna db files found.')\n        paths_optuna_db.sort()\n        print(f'Found {len(paths_optuna_db)} optuna db files\\n')\n\n        # Read optuna db files and store the results in a dataframe\n        studies_dicts = [self.read_optuna_db(path_optuna_db) for path_optuna_db in paths_optuna_db]\n        studies_df = pl.DataFrame(studies_dicts)\n\n        return studies_df\n\n    def read_optuna_db(self, path_optuna_db: str) -&gt; Dict[str, Any]:\n        loaded_study = optuna.load_study(study_name=None, storage=f\"sqlite:///{path_optuna_db}\")\n        study_name = loaded_study.study_name\n        min_loss = loaded_study.best_value\n        # trials_df = loaded_study.trials_dataframe()\n\n        frag_name = study_name.split('_')\n        assert len(frag_name) &gt; 4, 'Study name is not in the expected format.'\n        x_outer = int(frag_name[2])\n        x_inner = int(frag_name[3])\n        x_time = frag_name[4]\n        return {const.dkey.study_name: study_name, const.dkey.which_outer: x_outer, const.dkey.which_inner: x_inner, const.dkey.min_loss: min_loss, const.dkey.time_str: x_time}\n\n    def read_tensorboard_events(self, dir_events: str, get_test_loss: bool = True):\n        r\"\"\"Read tensorboard events from the directory.\n        \"\"\"\n        event_acc = EventAccumulator(dir_events)\n        event_acc.Reload()\n        scalar_tags = event_acc.Tags()[\"scalars\"]\n        scalar_data = {tag: [] for tag in scalar_tags}\n\n        for tag in scalar_tags:\n            _events = event_acc.Scalars(tag)\n            for _event in _events:\n                scalar_data[tag].append((_event.step, _event.value))\n\n        test_loss: float = scalar_data[const.title_tst_loss][0][1]\n\n        if get_test_loss:\n            return test_loss\n        else:\n            return scalar_data\n\n    def search_ckpt(self):\n        r\"\"\"Search checkpoints in the directory and its subdirectories.\n        \"\"\"\n        paths_ckpt = [os.path.join(dirpath, f)\n                    for dirpath, dirnames, files in os.walk(self.dir_log)\n                    for f in files if f.endswith('.ckpt')]\n        if len(paths_ckpt) == 0:\n            raise FileNotFoundError(\"No checkpoint files found.\")\n        paths_ckpt.sort()\n        print(f'Found {len(paths_ckpt)} checkpoints.\\n')\n        return paths_ckpt\n\n    def remove_inferior_models(self):\n        r\"\"\"Remove inferior models based on the collected result table.\n        \"\"\"\n        best_trials, all_trials = self.collect_ckpt()\n        n_all_ckpt = len(all_trials)\n        n_removed_models = 0\n        for _x in range(n_all_ckpt):\n            # Check if all_trials[const.dkey.trial_tag][_x] is in best_trials[const.dkey.trial_tag]\n            if best_trials[const.dkey.trial_tag].str.contains(all_trials[const.dkey.trial_tag][_x]).any():\n                continue\n            else:\n                os.remove(all_trials[const.dkey.ckpt_path][_x])\n                print(f\"Removed {all_trials[const.dkey.ckpt_path][_x]}\")\n                n_removed_models += 1\n        print(f\"Removed {n_removed_models} inferior models.\")\n        return None\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.__init__","title":"<code>__init__(dir_log)</code>","text":"<p>Collect training logs from optuna db files and ckpt files.</p> <p>Parameters:</p> Name Type Description Default <code>dir_log</code> <code>str</code> <p>Directory containing the model fitting logs.</p> required Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def __init__(self, dir_log: str):\n    r\"\"\"Collect training logs from optuna db files and ckpt files.\n\n    Args:\n        dir_log: Directory containing the model fitting logs.\n\n    \"\"\"\n    self.dir_log = dir_log\n    if not os.path.exists(self.dir_log):\n        raise ValueError(f'Directory {self.dir_log} does not exist.')\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.collect","title":"<code>collect()</code>","text":"<p>Collect training logs from optuna db files and ckpt files.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def collect(self) -&gt; Dict[str, pl.DataFrame]:\n    r\"\"\"Collect training logs from optuna db files and ckpt files.\n    \"\"\"\n    best_trials_df, all_ckpt = self.collect_ckpt()\n    optuna_best_inners_df = self.collect_optuna_db()\n\n    # Merge the two dataframes on the const.dkey.which_outer and const.dkey.which_inner columns\n    logs_df = optuna_best_inners_df.join(best_trials_df, on=[const.dkey.which_outer, const.dkey.which_inner], how='left')\n    # Remove the const.title_val_loss column from the merged dataframe\n    logs_df = logs_df.drop(const.title_val_loss)\n    # Rename 'min_loss' column to const.title_val_loss\n    logs_df = logs_df.rename({const.dkey.min_loss: const.title_val_loss})\n    # Sort the dataframe by const.dkey.which_outer and const.dkey.which_inner\n    logs_df = logs_df.sort([const.dkey.which_outer, const.dkey.which_inner])\n\n    best_inners_df = logs_df.group_by(const.dkey.which_outer).agg(pl.col(const.title_tst_loss).min()).join(logs_df, on=[const.dkey.which_outer, const.title_tst_loss], how='left')\n    best_inners_df = best_inners_df.sort([const.dkey.which_outer, const.title_tst_loss])\n\n    print(\"\\nFound model logs:\")\n    print(logs_df)\n    print(\"\\nBest inner folds:\")\n    print(best_inners_df)\n\n    return {const.dkey.best_trials: logs_df, const.dkey.best_inner_folds: best_inners_df}\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.collect_ckpt","title":"<code>collect_ckpt()</code>","text":"<p>Collect info from ckpt files and tensorboard events.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def collect_ckpt(self):\n    r\"\"\"Collect info from ckpt files and tensorboard events.\n    \"\"\"\n    paths_ckpt = self.search_ckpt()\n\n    # Pick ids of outer and inner folds, val_loss and version from ckpt file paths\n    test_loss_values = [self.read_tensorboard_events(os.path.join(os.path.dirname(path_x), \"version_0\")) for path_x in paths_ckpt]\n    val_loss_values = [float(os.path.basename(path_x).split('-')[3].split('=')[1].split('.ckpt')[0]) for path_x in paths_ckpt]\n    trial_tags = [path_x.split(os.path.sep)[-2] for path_x in paths_ckpt]\n    study_tags = [path_x.split(os.path.sep)[-4].split('_')[-1] for path_x in paths_ckpt]\n    ncv_inner_x = [int(path_x.split(os.path.sep)[-3].split('_')[-1]) for path_x in paths_ckpt]\n    ncv_outer_x = [int(path_x.split(os.path.sep)[-3].split('_')[-2]) for path_x in paths_ckpt]\n\n    # Create a dataframe with the above values\n    ckpt_df = pl.DataFrame({const.dkey.which_outer: ncv_outer_x, const.dkey.which_inner: ncv_inner_x, const.title_tst_loss: test_loss_values, const.title_val_loss: val_loss_values, const.dkey.trial_tag: trial_tags, const.dkey.study_tag: study_tags, const.dkey.ckpt_path: paths_ckpt})\n\n    # Pick the best model based on val_loss between the trials of the same outer and inner fold\n    best_trials_df = ckpt_df.group_by([const.dkey.which_outer, const.dkey.which_inner]).agg([pl.col(const.title_val_loss).min()]).join(ckpt_df, on=[const.dkey.which_outer, const.dkey.which_inner, const.title_val_loss], how='left')\n\n    return best_trials_df, ckpt_df\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.collect_optuna_db","title":"<code>collect_optuna_db()</code>","text":"<p>Collect info of optuna db files.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def collect_optuna_db(self):\n    r\"\"\"Collect info of optuna db files.\n    \"\"\"\n    # Find all optuna db files in the directory `dir_log` and its subdirectories\n    paths_optuna_db = [os.path.join(dirpath, f)\n                for dirpath, dirnames, files in os.walk(self.dir_log)\n                for f in files if f.endswith('.db')\n    ]\n    if len(paths_optuna_db) == 0:\n        raise FileNotFoundError('No optuna db files found.')\n    paths_optuna_db.sort()\n    print(f'Found {len(paths_optuna_db)} optuna db files\\n')\n\n    # Read optuna db files and store the results in a dataframe\n    studies_dicts = [self.read_optuna_db(path_optuna_db) for path_optuna_db in paths_optuna_db]\n    studies_df = pl.DataFrame(studies_dicts)\n\n    return studies_df\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.get_df_csv","title":"<code>get_df_csv(dir_output, overwrite_collected_log=False)</code>","text":"<p>Collect trained models for each fold in nested cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>dir_output</code> <code>str</code> <p>Directory to save the collected logs.</p> required <code>overwrite_collected_log</code> <code>bool</code> <p>Whether to overwrite existing collected logs.</p> <code>False</code> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def get_df_csv(self, dir_output: str, overwrite_collected_log: bool = False):\n    r\"\"\"Collect trained models for each fold in nested cross-validation.\n\n    Args:\n        dir_output: Directory to save the collected logs.\n\n        overwrite_collected_log: Whether to overwrite existing collected logs.\n\n    \"\"\"\n    collected_logs = self.collect()\n\n    models_bv = collected_logs[const.dkey.best_trials]\n    path_log_best_trials = os.path.join(dir_output, const.fname.log_best_trials)\n    if os.path.exists(path_log_best_trials) and not overwrite_collected_log:\n        models_bv = pl.read_csv(path_log_best_trials)\n    else:\n        models_bv.write_csv(path_log_best_trials)\n\n    models_bi = collected_logs[const.dkey.best_inner_folds]\n    path_log_best_inners = os.path.join(dir_output, const.fname.log_best_inners)\n    if os.path.exists(path_log_best_inners) and not overwrite_collected_log:\n        models_bi = pl.read_csv(path_log_best_inners)\n    else:\n        models_bi.write_csv(path_log_best_inners)\n\n    return models_bv, models_bi\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.read_tensorboard_events","title":"<code>read_tensorboard_events(dir_events, get_test_loss=True)</code>","text":"<p>Read tensorboard events from the directory.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def read_tensorboard_events(self, dir_events: str, get_test_loss: bool = True):\n    r\"\"\"Read tensorboard events from the directory.\n    \"\"\"\n    event_acc = EventAccumulator(dir_events)\n    event_acc.Reload()\n    scalar_tags = event_acc.Tags()[\"scalars\"]\n    scalar_data = {tag: [] for tag in scalar_tags}\n\n    for tag in scalar_tags:\n        _events = event_acc.Scalars(tag)\n        for _event in _events:\n            scalar_data[tag].append((_event.step, _event.value))\n\n    test_loss: float = scalar_data[const.title_tst_loss][0][1]\n\n    if get_test_loss:\n        return test_loss\n    else:\n        return scalar_data\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.remove_inferior_models","title":"<code>remove_inferior_models()</code>","text":"<p>Remove inferior models based on the collected result table.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def remove_inferior_models(self):\n    r\"\"\"Remove inferior models based on the collected result table.\n    \"\"\"\n    best_trials, all_trials = self.collect_ckpt()\n    n_all_ckpt = len(all_trials)\n    n_removed_models = 0\n    for _x in range(n_all_ckpt):\n        # Check if all_trials[const.dkey.trial_tag][_x] is in best_trials[const.dkey.trial_tag]\n        if best_trials[const.dkey.trial_tag].str.contains(all_trials[const.dkey.trial_tag][_x]).any():\n            continue\n        else:\n            os.remove(all_trials[const.dkey.ckpt_path][_x])\n            print(f\"Removed {all_trials[const.dkey.ckpt_path][_x]}\")\n            n_removed_models += 1\n    print(f\"Removed {n_removed_models} inferior models.\")\n    return None\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.CollectFitLog.search_ckpt","title":"<code>search_ckpt()</code>","text":"<p>Search checkpoints in the directory and its subdirectories.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def search_ckpt(self):\n    r\"\"\"Search checkpoints in the directory and its subdirectories.\n    \"\"\"\n    paths_ckpt = [os.path.join(dirpath, f)\n                for dirpath, dirnames, files in os.walk(self.dir_log)\n                for f in files if f.endswith('.ckpt')]\n    if len(paths_ckpt) == 0:\n        raise FileNotFoundError(\"No checkpoint files found.\")\n    paths_ckpt.sort()\n    print(f'Found {len(paths_ckpt)} checkpoints.\\n')\n    return paths_ckpt\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.ProcOnTrainSet","title":"<code>ProcOnTrainSet</code>","text":"Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>class ProcOnTrainSet:\n    def __init__(self, df_in: pl.DataFrame, ind_for_fit: Optional[List[Any]], n_feat2save: Optional[int] = None, df_labels: Optional[pl.DataFrame] = None):\n        r\"\"\"Process all data points based on the training set.\n\n        Args:\n            df_in: Input dataframe.\n\n            ind_for_fit: Sample indices for fitting the preprocessors.\n\n            n_feat2save: Number of features to save.\n\n            df_labels: Labels dataframe.\n\n        How to use:\n            - Initialize the class.\n            - Call the method `pr_xxxxx` to process the data.\n            - Call the method `save_processors` to save the processors (as a dict) to a pickle file.\n\n        \"\"\"\n        self.ind_for_fit = ind_for_fit\n        self.n_feat2save = n_feat2save\n        self._df = df_in\n        if df_labels is not None:\n            self._labels = df_labels\n\n        if ind_for_fit is not None:\n            self._df_part = self._df[ind_for_fit,:]\n            if df_labels is not None:\n                self._labels_part = self._labels[ind_for_fit,:]\n        else:\n            self._df_part = self._df\n            if df_labels is not None:\n                self._labels_part = df_labels\n\n        self.preprocessors = {}\n\n    def keep_preprocessors(self, x_value: Any):\n        r\"\"\"The key (int, ***0-based***) is automatically generated by the order of the data processor,\n        for the reproduction of data processing steps.\n        \"\"\"\n        x_order = len(self.preprocessors)\n        self.preprocessors[x_order] = x_value\n\n    def save_preprocessors(self, dir_save_processors: str, file_name: Optional[str] = None):\n        if file_name is None:\n            fname_preprocessors = const.fname.preprocessors\n        else:\n            fname_preprocessors = file_name\n        os.makedirs(dir_save_processors, exist_ok=True)\n        path_save_processors = os.path.join(dir_save_processors, fname_preprocessors)\n        if os.path.exists(path_save_processors):\n            raise FileExistsError(f\"The file {path_save_processors} already exists.\")\n\n        if len(self.preprocessors) &lt; 1:\n            raise Warning(\"No data processor is saved.\")\n        else:\n            with open(path_save_processors, \"wb\") as f:\n                pickle.dump(self.preprocessors, f)\n            print(f\"The data processors have been saved to: {path_save_processors}\")\n\n    def load_run_preprocessors(self, dir_save_processors: str, file_name: str):\n        path_processors = os.path.join(dir_save_processors, file_name)\n        with open(path_processors, \"rb\") as f:\n            processors_dict = pickle.load(f)\n        # Run the processors\n        _tmp_df = self._df.drop(const.dkey.id).to_numpy()\n        for i in range(len(processors_dict.keys())):\n            _tmp_df = processors_dict[i].transform(_tmp_df)\n        _tmp_df = pl.DataFrame(_tmp_df, schema=self._df.columns[1:])\n        _tmp_df = pl.DataFrame({const.dkey.id: self._df[const.dkey.id]}).hstack(_tmp_df)\n        self._df = _tmp_df\n\n    def general_preprocessor(self, _processor: Any):\n        try:\n            _processor.fit(self._df_part.drop(const.dkey.id).to_numpy())\n        except:\n            try:\n                _processor.fit(self._df_part)\n            except:\n                _processor.fit(self._df_part, self._labels_part)\n\n        try:\n            df_o = _processor.transform(self._df.drop(const.dkey.id).to_numpy())\n        except:\n            df_o = _processor.transform(self._df)\n\n        if type(df_o) != pl.DataFrame:\n            df_o = pl.DataFrame(df_o, schema=self._df.columns[1:])\n            df_o = pl.DataFrame({const.dkey.id: self._df[const.dkey.id]}).hstack(df_o)\n\n        self._df = df_o\n        self.keep_preprocessors(_processor)\n\n        # Refresh the part data for next processing steps.\n        if self.ind_for_fit is not None:\n            self._df_part = self._df[self.ind_for_fit,:]\n        else:\n            self._df_part = self._df\n\n    def pr_var(self, threshold: float = const.default.variance_threshold):\n        _selector = VarThreSelector(threshold=threshold)\n        self.general_preprocessor(_selector)\n\n    def pr_minmax(self):\n        _processor = MinMaxScaler()\n        self.general_preprocessor(_processor)\n\n    def pr_zscore(self):\n        _processor = StandardScaler()\n        self.general_preprocessor(_processor)\n\n    def pr_impute(self, strategy: str = \"mean\"):\n        _imputer = SimpleImputer(strategy=strategy)\n        self.general_preprocessor(_imputer)\n\n    def pr_rf(self, random_states: List[int], n_estimators: int = const.default.n_estimators, n_jobs_rf=const.default.n_jobs_rf):\n        if not hasattr(self, \"_labels\") or self.n_feat2save is None:\n            raise ValueError(\"The labels are not provided.\")\n\n        _selector = RFSelector(self.n_feat2save, random_states, n_estimators, n_jobs=n_jobs_rf)\n        self.general_preprocessor(_selector)\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.ProcOnTrainSet.__init__","title":"<code>__init__(df_in, ind_for_fit, n_feat2save=None, df_labels=None)</code>","text":"<p>Process all data points based on the training set.</p> <p>Parameters:</p> Name Type Description Default <code>df_in</code> <code>DataFrame</code> <p>Input dataframe.</p> required <code>ind_for_fit</code> <code>Optional[List[Any]]</code> <p>Sample indices for fitting the preprocessors.</p> required <code>n_feat2save</code> <code>Optional[int]</code> <p>Number of features to save.</p> <code>None</code> <code>df_labels</code> <code>Optional[DataFrame]</code> <p>Labels dataframe.</p> <code>None</code> How to use <ul> <li>Initialize the class.</li> <li>Call the method <code>pr_xxxxx</code> to process the data.</li> <li>Call the method <code>save_processors</code> to save the processors (as a dict) to a pickle file.</li> </ul> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def __init__(self, df_in: pl.DataFrame, ind_for_fit: Optional[List[Any]], n_feat2save: Optional[int] = None, df_labels: Optional[pl.DataFrame] = None):\n    r\"\"\"Process all data points based on the training set.\n\n    Args:\n        df_in: Input dataframe.\n\n        ind_for_fit: Sample indices for fitting the preprocessors.\n\n        n_feat2save: Number of features to save.\n\n        df_labels: Labels dataframe.\n\n    How to use:\n        - Initialize the class.\n        - Call the method `pr_xxxxx` to process the data.\n        - Call the method `save_processors` to save the processors (as a dict) to a pickle file.\n\n    \"\"\"\n    self.ind_for_fit = ind_for_fit\n    self.n_feat2save = n_feat2save\n    self._df = df_in\n    if df_labels is not None:\n        self._labels = df_labels\n\n    if ind_for_fit is not None:\n        self._df_part = self._df[ind_for_fit,:]\n        if df_labels is not None:\n            self._labels_part = self._labels[ind_for_fit,:]\n    else:\n        self._df_part = self._df\n        if df_labels is not None:\n            self._labels_part = df_labels\n\n    self.preprocessors = {}\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.ProcOnTrainSet.keep_preprocessors","title":"<code>keep_preprocessors(x_value)</code>","text":"<p>The key (int, 0-based) is automatically generated by the order of the data processor, for the reproduction of data processing steps.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def keep_preprocessors(self, x_value: Any):\n    r\"\"\"The key (int, ***0-based***) is automatically generated by the order of the data processor,\n    for the reproduction of data processing steps.\n    \"\"\"\n    x_order = len(self.preprocessors)\n    self.preprocessors[x_order] = x_value\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.RFSelector","title":"<code>RFSelector</code>","text":"Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>class RFSelector:\n    def __init__(self, n_feat2save: int, random_states: List[int], n_estimators: int, n_jobs: int, save_processors: bool = False):\n        r\"\"\"Select features based on random forest.\n        After `fit`, the selector recgonizes the colnames of the input dataframe.\n\n        Args:\n            n_feat2save: The number of features to save.\n\n            random_states: The random states for the random forest.\n\n            n_estimators: The number of trees in the random forest.\n\n            n_jobs: The number of jobs to run in parallel.\n\n            save_processors: Whether to save the random forest models.\n\n        \"\"\"\n        self.n_feat2save = n_feat2save\n        self.random_states = random_states\n        self.n_estimators = n_estimators\n        self.n_jobs = n_jobs\n        self.processors = {}\n        self.save_processors = save_processors\n\n    def fit(self, omics_df: pl.DataFrame, labels_df: pl.DataFrame):\n        _omics_np = omics_df.drop(const.dkey.id).to_numpy()\n        _labels_np = labels_df.drop(const.dkey.id).to_numpy()\n        if _labels_np.shape[1] == 1:\n            _labels_np = _labels_np.ravel()\n        _feat_imp = np.zeros(shape=(len(self.random_states), _omics_np.shape[1]))\n        print(f\"Starting to fit RF for {len(self.random_states)} random states...\")\n        for i in range(len(self.random_states)):\n            _feat_imp[i,:] = self.fit_1(_omics_np, _labels_np, self.random_states[i])\n            print(f\"Finished {i+1}/{len(self.random_states)}\")\n        _feat_imp_mean = np.mean(_feat_imp, axis=0)\n        _feat_imp_mean_sorted = np.argsort(_feat_imp_mean)[::-1]\n        if self.n_feat2save &lt;= _omics_np.shape[1]:\n            _feat_to_save = _feat_imp_mean_sorted[:self.n_feat2save]\n        else:\n            _feat_to_save = _feat_imp_mean_sorted\n        _colnames = omics_df.drop(const.dkey.id).columns\n        self.colname_to_save = [_colnames[i] for i in _feat_to_save]\n\n    def transform(self, X_df: pl.DataFrame):\n        _selected = X_df.select(self.colname_to_save)\n        df_o = pl.DataFrame({const.dkey.id: X_df[const.dkey.id]}).hstack(_selected)\n        return df_o\n\n    def keep_preprocessor(self, x_processor: Any):\n        r\"\"\"The key (int, ***0-based***) is automatically generated by the order of the data processor,\n        for the reproduction of data processing steps.\n\n        Args:\n            x_processor: the processor to be kept.\n\n        \"\"\"\n        x_order = len(self.processors)\n        self.processors[x_order] = x_processor\n\n    def fit_1(self, X: np.ndarray, y: np.ndarray, random_state: int):\n        _processor = RandomForestRegressor(n_estimators=self.n_estimators, n_jobs=self.n_jobs, random_state=random_state)\n        _processor.fit(X, y)\n        if self.save_processors:\n            self.keep_preprocessor(_processor)\n        return _processor.feature_importances_\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.RFSelector.__init__","title":"<code>__init__(n_feat2save, random_states, n_estimators, n_jobs, save_processors=False)</code>","text":"<p>Select features based on random forest. After <code>fit</code>, the selector recgonizes the colnames of the input dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>n_feat2save</code> <code>int</code> <p>The number of features to save.</p> required <code>random_states</code> <code>List[int]</code> <p>The random states for the random forest.</p> required <code>n_estimators</code> <code>int</code> <p>The number of trees in the random forest.</p> required <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> required <code>save_processors</code> <code>bool</code> <p>Whether to save the random forest models.</p> <code>False</code> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def __init__(self, n_feat2save: int, random_states: List[int], n_estimators: int, n_jobs: int, save_processors: bool = False):\n    r\"\"\"Select features based on random forest.\n    After `fit`, the selector recgonizes the colnames of the input dataframe.\n\n    Args:\n        n_feat2save: The number of features to save.\n\n        random_states: The random states for the random forest.\n\n        n_estimators: The number of trees in the random forest.\n\n        n_jobs: The number of jobs to run in parallel.\n\n        save_processors: Whether to save the random forest models.\n\n    \"\"\"\n    self.n_feat2save = n_feat2save\n    self.random_states = random_states\n    self.n_estimators = n_estimators\n    self.n_jobs = n_jobs\n    self.processors = {}\n    self.save_processors = save_processors\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.RFSelector.keep_preprocessor","title":"<code>keep_preprocessor(x_processor)</code>","text":"<p>The key (int, 0-based) is automatically generated by the order of the data processor, for the reproduction of data processing steps.</p> <p>Parameters:</p> Name Type Description Default <code>x_processor</code> <code>Any</code> <p>the processor to be kept.</p> required Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def keep_preprocessor(self, x_processor: Any):\n    r\"\"\"The key (int, ***0-based***) is automatically generated by the order of the data processor,\n    for the reproduction of data processing steps.\n\n    Args:\n        x_processor: the processor to be kept.\n\n    \"\"\"\n    x_order = len(self.processors)\n    self.processors[x_order] = x_processor\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.VarThreSelector","title":"<code>VarThreSelector</code>","text":"Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>class VarThreSelector:\n    def __init__(self, threshold: float):\n        r\"\"\"Select features based on variance.\n        After `fit`, the selector recgonizes the colnames of the input dataframe.\n\n        Args:\n            threshold: The threshold for variance.\n\n        \"\"\"\n        self.threshold = threshold\n        self.processors = {}\n\n    def fit(self, omics_df: pl.DataFrame):\n        _omics_np = omics_df.drop(const.dkey.id).to_numpy()\n        _selector = VarianceThreshold(threshold=self.threshold)\n        _selector.fit(_omics_np)\n        _feat_to_save = _selector.get_support()\n        _colnames = omics_df.drop(const.dkey.id).columns\n        self.colname_to_save = [_colnames[i] for i in range(len(_colnames)) if _feat_to_save[i]]\n\n    def transform(self, X_df: pl.DataFrame):\n        _selected = X_df.select(self.colname_to_save)\n        df_o = pl.DataFrame({const.dkey.id: X_df[const.dkey.id]}).hstack(_selected)\n        return df_o\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.VarThreSelector.__init__","title":"<code>__init__(threshold)</code>","text":"<p>Select features based on variance. After <code>fit</code>, the selector recgonizes the colnames of the input dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold for variance.</p> required Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def __init__(self, threshold: float):\n    r\"\"\"Select features based on variance.\n    After `fit`, the selector recgonizes the colnames of the input dataframe.\n\n    Args:\n        threshold: The threshold for variance.\n\n    \"\"\"\n    self.threshold = threshold\n    self.processors = {}\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.get_indices_ncv","title":"<code>get_indices_ncv(k_outer, k_inner, which_outer_test, which_inner_val)</code>","text":"<p>Get indices of fragments for NCV.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def get_indices_ncv(\n        k_outer: int,\n        k_inner: int,\n        which_outer_test: int,\n        which_inner_val: int,\n    ):\n    r\"\"\"Get indices of fragments for NCV.\n    \"\"\"\n    # Init fragment indices for test dataset\n    n_fragments = int(k_outer * k_inner)\n    n_f_test = int(n_fragments / k_outer)\n    indices_test_dataset = [i for i in range(int(which_outer_test * n_f_test), int((which_outer_test + 1) * n_f_test))]\n    # Indices excluding test dataset\n    indices_train_dataset = [i for i in range(n_fragments) if i not in indices_test_dataset]\n    # Indices for validation dataset\n    parts = np.array_split(indices_train_dataset, k_inner)\n    indices_val_dataset = parts[which_inner_val].tolist()\n    indices_trn_dataset = [i for i in indices_train_dataset if i not in indices_val_dataset]\n\n    return indices_trn_dataset, indices_val_dataset, indices_test_dataset\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.idx_convert","title":"<code>idx_convert(indices, onehot_bits=const.default.snp_onehot_bits)</code>","text":"<p>Convert the indices to the corresponding indices in the one-hot vector.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def idx_convert(indices: List[int], onehot_bits: int = const.default.snp_onehot_bits) -&gt; List[int]:\n    r\"\"\"Convert the indices to the corresponding indices in the one-hot vector.\n    \"\"\"\n    converted_indices = np.array(indices)[:, np.newaxis] * onehot_bits + np.arange(onehot_bits)\n    converted_indices = np.sort(converted_indices.flatten()).tolist()\n    return converted_indices\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.intersect_lists","title":"<code>intersect_lists(lists, get_indices=True, to_sorted=True)</code>","text":"<p>Find the shared elements between multiple lists.</p> <p>Parameters:</p> Name Type Description Default <code>lists</code> <code>List[List[Any]]</code> <p>A list of lists.</p> required <code>get_indices</code> <code>bool</code> <p>Whether to return the indices of the shared elements in each list.</p> <code>True</code> <code>to_sorted</code> <code>bool</code> <p>Whether to sort the shared elements.</p> <code>True</code> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def intersect_lists(lists: List[List[Any]], get_indices: bool = True, to_sorted: bool = True):\n    r\"\"\"Find the shared elements between multiple lists.\n\n    Args:\n        lists: A list of lists.\n\n        get_indices: Whether to return the indices of the shared elements in each list.\n\n        to_sorted: Whether to sort the shared elements.\n\n    \"\"\"\n    if len(lists) == 0:\n        raise ValueError(\"The list of lists is empty.\")\n    elif len(lists) == 1:\n        shared = lists[0]\n    else:\n        shared = list(set.intersection(*map(set, lists)))\n\n    assert len(shared) &gt; 0, \"No intersecting elements!\"\n    if to_sorted:\n        shared = sorted(shared)\n    if get_indices:\n        indices = []\n        # for xl in range(len(lists)):\n        #     # Accelerate the search by NumPy\n        #     indices.append(np.where(np.isin(lists[xl], shared))[0].tolist())\n        # !!!!!!!!!!! The following can keep the order of `shared` !!!!!!!!!!!!!!\n        for xl in range(len(lists)):\n            indices.append([])\n        for i in shared:\n            for xl in range(len(lists)):\n                indices[xl].append(np.where(np.isin(lists[xl], i))[0].tolist()[0])\n        return shared, indices\n    else:\n        return shared\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.onehot_encode_snp_mat","title":"<code>onehot_encode_snp_mat(snp_matrix, onehot_bits=None, genes_snps=None)</code>","text":"<p>One-hot encode the SNP matrix.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def onehot_encode_snp_mat(\n        snp_matrix: np.ndarray,\n        onehot_bits: Optional[int] = None,\n        genes_snps: Optional[List[List[int]]] = None,\n    ):\n    r\"\"\"One-hot encode the SNP matrix.\n    \"\"\"\n    if onehot_bits is None:\n        len_onehot = const.default.snp_onehot_bits\n    else:\n        len_onehot = onehot_bits\n\n    if genes_snps is not None:\n        num_genes = len(genes_snps)\n        indices_snp = []\n        for i_gene in range(num_genes):\n            indices_snp.append(idx_convert(genes_snps[i_gene], len_onehot))\n        snp_data = []\n        for i_sample in range(snp_matrix.shape[0]):\n            snp_vec = snp_matrix[i_sample].astype(int)\n            snp_vec = np.eye(len_onehot + 1)[snp_vec][:, 1:].reshape(-1)\n            snp_vec_genes = [snp_vec[indices_snp[i_gene]].astype(np.float32) for i_gene in range(num_genes)]\n            snp_data.append(snp_vec_genes)\n    else:\n        snp_data = []\n        for i_sample in range(snp_matrix.shape[0]):\n            snp_vec = snp_matrix[i_sample].astype(int)\n            snp_vec = np.eye(len_onehot + 1)[snp_vec][:, 1:].reshape(-1).astype(np.float32)\n            snp_data.append(snp_vec)\n    snp_data_np = np.array(snp_data)\n    return snp_data_np\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.read_labels","title":"<code>read_labels(path_label, col2use=None)</code>","text":"<p>Read labels from a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>path_label</code> <code>str</code> <p>Path to the labels file.</p> required <code>col2use</code> <code>Optional[List[Any]]</code> <p>A list of column names or indices. If <code>col2use</code> is <code>List[int]</code>, its numbers are the indices (1-based) of the columns to be used.</p> <code>None</code> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def read_labels(path_label: str, col2use: Optional[List[Any]] = None):\n    r\"\"\"Read labels from a csv file.\n\n    Args:\n        path_label: Path to the labels file.\n\n        col2use: A list of column names or indices.\n            If `col2use` is `List[int]`, its numbers are the indices **(1-based)** of the columns to be used.\n\n    \"\"\"\n    label_df = read_omics(path_label)\n    if label_df.columns[0] != const.dkey.id:\n        # Check the element data type of the first column\n        if label_df.schema.dtypes()[0] == pl.String():\n            label_df = label_df.rename({label_df.columns[0]: const.dkey.id})\n        else:\n            raise ValueError(\"The first column of the label file must be the sample IDs.\")\n\n    sample_ids = label_df.select(const.dkey.id).to_series().to_list()\n\n    if col2use is not None:\n        if type(col2use[0]) == str:\n            label_df = label_df.select([const.dkey.id] + col2use)\n        elif type(col2use[0]) == int:\n            label_df = label_df[[0]+col2use,:]\n        else:\n            raise ValueError(\"col2use must be either a list of strings or a list of integers.\")\n    dim_model_output = len(label_df.columns) - 1\n    return label_df, dim_model_output, sample_ids\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.read_omics","title":"<code>read_omics(data_path)</code>","text":"<p>Read omics data from various formats.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def read_omics(data_path: str):\n    r\"\"\"Read omics data from various formats.\n    \"\"\"\n    # Check if the path is a file or a folder\n    if os.path.isdir(data_path):\n        raise ValueError(f\"The path {data_path} is a folder. Please provide a path to a single file.\")\n\n    # Check the file extension\n    file_ext = os.path.splitext(data_path)[-1]\n    if len(file_ext) &lt; 2:\n        raise ValueError(\"The file extension is empty.\")\n    else:\n        file_ext = file_ext.lower()\n\n    match file_ext:\n        case '.csv':\n            _data = pl.read_csv(data_path, schema_overrides={const.dkey.id: pl.Utf8})\n            if _data.columns[0] == '':\n                # Rename the first column to const.dkey.id\n                _data = _data.rename({_data.columns[0]: const.dkey.id})\n        case '.parquet':\n            _data = pl.read_parquet(data_path)\n            if _data.columns[0] == '':\n                # Rename the first column to const.dkey.id\n                _data = _data.rename({_data.columns[0]: const.dkey.id})\n        case '.gz':\n            if data_path.endswith('.pkl.gz'):\n                snp_data_dict = read_pkl_gv(data_path)\n                snp_matrix = snp_data_dict[const.dkey.genotype_matrix]\n                snp_sample_ids = snp_data_dict[const.dkey.sample_ids]\n                snp_ids = snp_data_dict[const.dkey.snp_ids]\n                # snp_block_ids = snp_data_dict['block_ids']\n                _tmp_snp_df = pl.DataFrame(data=snp_matrix, schema=snp_ids)\n                _tmp_id = pl.DataFrame({const.dkey.id: snp_sample_ids})\n                _data = _tmp_id.hstack(_tmp_snp_df)\n            else:\n                raise ValueError(\"The file extension is not supported.\")\n        case _:\n            raise ValueError(\"The file extension is not supported.\")\n\n    return _data\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.read_omics_xoxi","title":"<code>read_omics_xoxi(data_path, which_outer_test, which_inner_val, trnvaltst=const.abbr_train, file_ext=None, prefix=None)</code>","text":"<p>Read processed data from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the directory containing the data.</p> required <code>which_outer_test</code> <code>int</code> <p>Which outer test set to read.</p> required <code>which_inner_val</code> <code>int</code> <p>Which inner validation set to read.</p> required <code>trnvaltst</code> <code>str</code> <p>The abbreviation of the training/validation/test set.</p> <code>abbr_train</code> <code>file_ext</code> <code>Optional[str]</code> <p>The file extension of the data files. If None, the default extension will be used.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix of the file name. (Optional)</p> <code>None</code> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def read_omics_xoxi(\n        data_path: str,\n        which_outer_test: int,\n        which_inner_val: int,\n        trnvaltst: str = const.abbr_train,\n        file_ext: Optional[str] = None,\n        prefix: Optional[str] = None,\n    ):\n    r\"\"\"Read processed data from a directory.\n\n    Args:\n        data_path: Path to the directory containing the data.\n\n        which_outer_test: Which outer test set to read.\n\n        which_inner_val: Which inner validation set to read.\n\n        trnvaltst: The abbreviation of the training/validation/test set.\n\n        file_ext: The file extension of the data files. If None, the default extension will be used.\n\n        prefix: The prefix of the file name. (Optional)\n\n    \"\"\"\n    if not os.path.isdir(data_path):\n        raise ValueError(f\"The path {data_path} is not a directory.\")\n    if file_ext is None:\n        fname_ext = const.fname.data_ext\n    else:\n        fname_ext = file_ext\n\n    # Walk through the directory and find all files with the specified pattern\n    if prefix is None:\n        files_found = [os.path.join(dir_path, f) for dir_path, _, files in os.walk(data_path) for f in files if f.endswith(fname_ext)]\n    else:\n        files_found = [os.path.join(dir_path, f) for dir_path, _, files in os.walk(data_path) for f in files if f.startswith(prefix) and f.endswith(fname_ext)]\n\n    if len(files_found) == 0:\n        raise ValueError(f\"No files found with the specified pattern in {data_path}.\")\n\n    # Search for the specific file name\n    for file_path in files_found:\n        _tmp_name = os.path.basename(file_path)\n        _tmp_name = os.path.splitext(_tmp_name)[0]\n        _tmp_name_parts = _tmp_name.split('_')\n        if _tmp_name_parts[-1] == trnvaltst:\n            if _tmp_name_parts[-2] == str(which_inner_val) and _tmp_name_parts[-3] == str(which_outer_test):\n                return read_omics(file_path)\n\n    raise ValueError(f\"No file found with the specified pattern in {data_path}.\")\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.read_pkl_gv","title":"<code>read_pkl_gv(path_pkl)</code>","text":"<p>Read processed VCF data from a pickle file.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def read_pkl_gv(path_pkl: str) -&gt; Dict[str, Any]:\n    r\"\"\"Read processed VCF data from a pickle file.\n    \"\"\"\n    with gzip.open(path_pkl, 'rb') as file:\n        # Initialize an empty list to hold all the deserialized vectors\n        _vectors = []\n\n        # While there is data in the file, load it\n        while True:\n            try:\n                # Load the next pickled object from the file\n                _data = pickle.load(file)\n                # Append the loaded data to the list\n                _vectors.append(_data)\n            except EOFError:\n                # An EOFError is raised when there is no more data to read\n                break\n\n    _sample_ids = _vectors[0]\n    _snp_ids = _vectors[1]\n    _block_ids = _vectors[2]\n    _block2gtype = _vectors[3]\n    _mat_vec = _vectors[4]\n    _mat_shape = (len(_snp_ids), len(_sample_ids))\n\n    # Reshape the matrix to the correct shape\n    vcf_mat = np.reshape(_mat_vec, _mat_shape).transpose()\n\n    return {\n        const.dkey.genotype_matrix: vcf_mat,\n        const.dkey.gblock2gtype: _block2gtype,\n        const.dkey.sample_ids: _sample_ids,\n        const.dkey.snp_ids: _snp_ids,\n        const.dkey.gblock_ids: _block_ids,\n    }\n</code></pre>"},{"location":"reference/biodem.utils.uni/#biodem.utils.uni.train_model","title":"<code>train_model(model, datamodule, es_patience, max_epochs, min_epochs, log_dir, devices=const.default.devices, accelerator=const.default.accelerator, in_dev=False)</code>","text":"<p>Fit the model.</p> Source code in <code>src\\biodem\\utils\\uni.py</code> <pre><code>def train_model(\n        model: Any,\n        datamodule: LightningDataModule,\n        es_patience: int,\n        max_epochs: int,\n        min_epochs: int,\n        log_dir: str,\n        devices: Union[list[int], str, int] = const.default.devices,\n        accelerator: str = const.default.accelerator,\n        in_dev: bool = False,\n    ):\n    r\"\"\"Fit the model.\n    \"\"\"\n    avail_dev = get_avail_nvgpu(devices)\n\n    callback_es = EarlyStopping(\n        monitor=const.title_val_loss,\n        patience=es_patience,\n        mode='min',\n        verbose=True,\n    )\n    callback_ckpt = ModelCheckpoint(\n        dirpath=log_dir,\n        filename=const.default.ckpt_fname_format,\n        monitor=const.title_val_loss,\n    )\n\n    logger_tr = TensorBoardLogger(\n        save_dir=log_dir,\n        name='',\n    )\n\n    trainer = Trainer(\n        fast_dev_run=in_dev,\n        logger=logger_tr,\n        log_every_n_steps=1,\n        precision=\"16-mixed\",\n        devices=avail_dev,\n        accelerator=accelerator,\n        max_epochs=max_epochs,\n        min_epochs=min_epochs,\n        callbacks=[callback_es, callback_ckpt],\n        num_sanity_val_steps=0,\n        default_root_dir=log_dir,\n    )\n\n    trainer.fit(model=model, datamodule=datamodule)\n\n    if callback_ckpt.best_model_score is not None:\n        best_score = callback_ckpt.best_model_score.item()\n    else:\n        best_score = None\n\n    trainer.test(ckpt_path=callback_ckpt.best_model_path, dataloaders=datamodule)\n\n    print(f\"\\nBest validation score: {best_score}\")\n    print(f\"Best model path: {callback_ckpt.best_model_path}\\n\")\n\n    return best_score\n</code></pre>"}]}